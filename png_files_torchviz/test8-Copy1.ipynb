{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ba9b76-3c4a-4dba-a295-b930ea39f73c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes',\n",
       " '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/cityscapes_dataset')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,Subset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from sklearn.metrics import jaccard_score\n",
    "from torchvision.models.mobilenetv3 import MobileNet_V3_Small_Weights\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from labels import labels\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "curr_dir=os.getcwd()\n",
    "root= os.path.join(curr_dir,\"cityscapes_dataset\")\n",
    "curr_dir,root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a94aa7-dba7-4018-9134-f063b811cd98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "# from albumentations.augmentations.transforms import RandomShadow\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\" Normalizes RGB image to  0-mean 1-std_dev \"\"\" \n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], depth_norm=5, max_depth=250):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.depth_norm = depth_norm\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            \n",
    "        return {'left': TF.normalize(left, self.mean, self.std), \n",
    "                'mask': mask, \n",
    "                'depth' : torch.clip( # saftey clip :)\n",
    "                            torch.log(torch.clip(depth, 0, self.max_depth))/self.depth_norm, \n",
    "                            0, \n",
    "                            self.max_depth)}\n",
    "\n",
    "\n",
    "class AddColorJitter(object):\n",
    "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\" \n",
    "    def __init__(self, brightness, contrast, saturation, hue):\n",
    "        ''' Applies brightness, constrast, saturation, and hue jitter to image ''' \n",
    "        self.color_jitter = transforms.ColorJitter(brightness, contrast, saturation, hue)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        return {'left': self.color_jitter(left), \n",
    "                'mask': mask, \n",
    "                'depth' : depth}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\" Rescales images with bilinear interpolation and masks with nearest interpolation \"\"\"\n",
    "\n",
    "    def __init__(self, h, w):\n",
    "        self.h, self.w = h, w\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "# mask interpolation Nearest is import to have smoothness\n",
    "        return {'left': TF.resize(left, (self.h, self.w)), \n",
    "                'mask': TF.resize(mask.unsqueeze(0), (self.h, self.w), transforms.InterpolationMode.NEAREST), \n",
    "                'depth' : TF.resize(depth.unsqueeze(0), (self.h, self.w))}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, h, w, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.scale = scale\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "        i, j, h, w = transforms.RandomResizedCrop.get_params(left, scale=self.scale, ratio=self.ratio)\n",
    "\n",
    "        return {'left': TF.resized_crop(left, i, j, h, w, (self.h, self.w)), \n",
    "                'mask': TF.resized_crop(mask.unsqueeze(0), i, j, h, w, (self.h, self.w), interpolation=TF.InterpolationMode.NEAREST),\n",
    "                'depth' : TF.resized_crop(depth.unsqueeze(0), i, j, h, w, (self.h, self.w))}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "         \n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        return {'left': transforms.ToTensor()(left), \n",
    "                'mask': torch.as_tensor(mask, dtype=torch.int64),\n",
    "                'depth' : transforms.ToTensor()(depth).type(torch.float32)}\n",
    "    \n",
    "\n",
    "class ElasticTransform(object):\n",
    "    def __init__(self, alpha=25.0, sigma=5.0, prob=0.5):\n",
    "        self.alpha = [1.0, alpha]\n",
    "        self.sigma = [1, sigma]\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            _, H, W = mask.shape\n",
    "            displacement = transforms.ElasticTransform.get_params(self.alpha, self.sigma, [H, W])\n",
    "\n",
    "            # # TEMP\n",
    "            # print(TF.elastic_transform(left, displacement).shape)\n",
    "            # print(TF.elastic_transform(mask.unsqueeze(0), displacement, interpolation=TF.InterpolationMode.NEAREST).shape)\n",
    "            # print(torch.clip(TF.elastic_transform(depth, displacement), 0, depth.max()).shape)\n",
    "\n",
    "            return {'left': TF.elastic_transform(left, displacement), \n",
    "                    'mask': TF.elastic_transform(mask.unsqueeze(0), displacement, interpolation=TF.InterpolationMode.NEAREST), \n",
    "                    'depth' : torch.clip(TF.elastic_transform(depth, displacement), 0, depth.max())} \n",
    "        \n",
    "        else:\n",
    "            return sample\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "# new transform to rotate the images\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, angle):\n",
    "        if not isinstance(angle, (list, tuple)):\n",
    "            self.angle = (-abs(angle), abs(angle))\n",
    "        else:\n",
    "            self.angle = angle\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        angle = transforms.RandomRotation.get_params(self.angle)\n",
    "\n",
    "        return {'left': TF.rotate(left, angle), \n",
    "                'mask': TF.rotate(mask.unsqueeze(0), angle), \n",
    "                'depth' : TF.rotate(depth, angle)}\n",
    "    \n",
    "    \n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            return {'left': TF.hflip(left), \n",
    "                    'mask': TF.hflip(mask), \n",
    "                    'depth' : TF.hflip(depth)}\n",
    "        else:\n",
    "            return sample\n",
    "        \n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            return {'left': TF.vflip(left), \n",
    "                    'mask': TF.vflip(mask), \n",
    "                    'depth' : TF.vflip(depth)}\n",
    "        else:\n",
    "            return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f22ee78a-1a67-4c7b-a0ef-2ec953859137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_numpy(image):\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        if len(image.shape) == 2:\n",
    "            image = image.detach().cpu().numpy()\n",
    "        else:\n",
    "            image = image.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_color_mask(mask, labels, id_type='id'):\n",
    "    try:\n",
    "        h, w = mask.shape\n",
    "    except ValueError:\n",
    "        mask = mask.squeeze(-1)\n",
    "        h, w = mask.shape\n",
    "\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    if id_type == 'id':\n",
    "        for lbl in labels:\n",
    "            color_mask[mask == lbl.id] = lbl.color\n",
    "    elif id_type == 'trainId':\n",
    "        for lbl in labels:\n",
    "            if (lbl.trainId != 255) | (lbl.trainId != -1):\n",
    "                color_mask[mask == lbl.trainId] = lbl.color\n",
    "\n",
    "    return color_mask\n",
    "\n",
    "\n",
    "def plot_items(left, mask, depth, labels=None, num_seg_labels=34, id_type='id'):\n",
    "    left = convert_to_numpy(left)\n",
    "    mask = convert_to_numpy(mask)\n",
    "    depth = convert_to_numpy(depth)\n",
    "\n",
    "    # unnormalize left image\n",
    "    left = (left*np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
    "\n",
    "    # cmaps: 'prism', 'terrain', 'turbo', 'gist_rainbow_r', 'nipy_spectral_r'\n",
    "    \n",
    "    \n",
    "    _, ax = plt.subplots(1, 3, figsize=(15,10))\n",
    "    ax[0].imshow(left)\n",
    "    ax[0].set_title(\"Left Image\")\n",
    "\n",
    "    if labels:\n",
    "        color_mask = get_color_mask(mask, labels, id_type)\n",
    "        ax[1].imshow(color_mask)\n",
    "    else:\n",
    "        cmap = mpl.colormaps.get_cmap('nipy_spectral_r').resampled(num_seg_labels)\n",
    "        ax[1].imshow(mask, cmap=cmap)\n",
    "\n",
    "    ax[1].set_title(\"Seg Mask\")\n",
    "    ax[2].imshow(depth, cmap='plasma')\n",
    "    ax[2].set_title(\"Depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e366997-a6f1-47cd-8190-1fb12596df2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_invariant_depth_loss(pred, target, lambda_weight=0.1):\n",
    "    if pred.shape != target.shape:\n",
    "        pred = F.interpolate(pred, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "    \n",
    "    diff = pred - target\n",
    "    n = diff.numel()\n",
    "    mse = torch.sum(diff**2) / n\n",
    "    scale_invariant = mse - (lambda_weight / (n**2)) * (torch.sum(diff))**2\n",
    "    return scale_invariant\n",
    "\n",
    "def depth_smoothness_loss(pred, img, alpha=1.0):\n",
    "    depth_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])\n",
    "    depth_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])\n",
    "    img_grad_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), dim=1, keepdim=True)\n",
    "    img_grad_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), dim=1, keepdim=True)\n",
    "    smoothness_x = depth_grad_x * torch.exp(-alpha * img_grad_x)\n",
    "    smoothness_y = depth_grad_y * torch.exp(-alpha * img_grad_y)\n",
    "    return smoothness_x.mean() + smoothness_y.mean()\n",
    "\n",
    "\n",
    "def inv_huber_loss(pred, target, delta=0.1):\n",
    "    \"\"\"\n",
    "    Inverse Huber loss for depth prediction.\n",
    "    Args:\n",
    "        pred (Tensor): Predicted depth map.\n",
    "        target (Tensor): Ground truth depth map.\n",
    "        delta (float): Threshold for switching between quadratic and linear terms.\n",
    "    Returns:\n",
    "        Tensor: Inverse Huber loss.\n",
    "    \"\"\"\n",
    "    abs_diff = torch.abs(pred - target)\n",
    "    delta_tensor = torch.tensor(delta, dtype=abs_diff.dtype, device=abs_diff.device)  # Convert delta to tensor\n",
    "    quadratic = torch.minimum(abs_diff, delta_tensor)\n",
    "    linear = abs_diff - quadratic\n",
    "    return (0.5 * quadratic**2 + delta_tensor * linear).mean()\n",
    "\n",
    "\n",
    "def mean_iou(pred, target, num_classes):\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    intersection = torch.logical_and(pred == target, target != 255).float()  # Ignore class 255\n",
    "    union = torch.logical_or(pred == target, target != 255).float()\n",
    "    iou = torch.sum(intersection) / torch.sum(union)\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def contrastive_loss(pred, target, margin=1.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss to ensure the depth map predictions are closer to the target.\n",
    "    \"\"\"\n",
    "    # Flatten the tensors for element-wise operations\n",
    "    pred_flat = pred.view(pred.size(0), -1)  # Flatten except for the batch dimension\n",
    "    target_flat = target.view(target.size(0), -1)  # Flatten except for the batch dimension\n",
    "\n",
    "    # Compute the pairwise distances\n",
    "    distances = torch.sqrt(torch.sum((pred_flat - target_flat) ** 2, dim=1))  # Batch-wise distances\n",
    "\n",
    "    # Create labels for contrastive loss\n",
    "    labels = (torch.abs(pred_flat - target_flat).mean(dim=1) < margin).float()  # Batch-wise labels\n",
    "\n",
    "    # Calculate contrastive loss\n",
    "    similar_loss = labels * distances**2\n",
    "    dissimilar_loss = (1 - labels) * torch.clamp(margin - distances, min=0)**2\n",
    "    loss = (similar_loss + dissimilar_loss).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def dice_loss(predictions, targets, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Dice Loss for segmentation.\n",
    "    Args:\n",
    "        predictions (torch.Tensor): The predicted segmentation map (logits or probabilities).\n",
    "                                    Shape: [batch_size, num_classes, height, width]\n",
    "        targets (torch.Tensor): The ground truth segmentation map (one-hot encoded or integer labels).\n",
    "                                Shape: [batch_size, height, width]\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    Returns:\n",
    "        torch.Tensor: Dice Loss (scalar).\n",
    "    \"\"\"\n",
    "    # Convert integer labels to one-hot if needed\n",
    "    if predictions.shape != targets.shape:\n",
    "        targets = F.one_hot(targets, num_classes=predictions.shape[1]).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    # Apply softmax to predictions for multi-class segmentation\n",
    "    predictions = torch.softmax(predictions, dim=1)\n",
    "    \n",
    "    # Flatten tensors to calculate intersection and union\n",
    "    predictions_flat = predictions.view(predictions.shape[0], predictions.shape[1], -1)\n",
    "    targets_flat = targets.view(targets.shape[0], targets.shape[1], -1)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = (predictions_flat * targets_flat).sum(dim=-1)\n",
    "    union = predictions_flat.sum(dim=-1) + targets_flat.sum(dim=-1)\n",
    "    \n",
    "    # Calculate Dice Coefficient\n",
    "    dice_coeff = (2 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # Dice Loss\n",
    "    return 1 - dice_coeff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ec23d0-1fad-4dfe-8b94-6de4c2579d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, valid_losses, save_dir):\n",
    "    epochs = range(1, len(train_losses[\"seg\"]) + 1)\n",
    "\n",
    "    # Plot Segmentation Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses[\"seg\"], label=\"Train Seg Loss\")\n",
    "    plt.plot(epochs, valid_losses[\"seg\"], label=\"Valid Seg Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Segmentation Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Segmentation Loss Over Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir, \"segmentation_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Depth Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses[\"depth\"], label=\"Train Depth Loss\")\n",
    "    plt.plot(epochs, valid_losses[\"depth\"], label=\"Valid Depth Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Depth Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Depth Loss Over Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir, \"depth_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Combined Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses[\"combined\"], label=\"Train Combined Loss\")\n",
    "    plt.plot(epochs, valid_losses[\"combined\"], label=\"Valid Combined Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Combined Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Combined Loss Over Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir, \"combined_loss.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8fe187-44fc-45aa-bf13-87b9b28936af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels):\n",
    "    inputs = inputs.detach().cpu()\n",
    "    seg_output = torch.argmax(seg_output, dim=1).detach().cpu()\n",
    "    depth_output = depth_output.detach().cpu()\n",
    "    seg_labels = seg_labels.detach().cpu()\n",
    "    depth_labels = depth_labels.detach().cpu()\n",
    "    \n",
    "#     inputs_rgb = (inputs - inputs.min()) / (inputs.max() - inputs.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "    \n",
    "#     # Normalize depth maps for visualization\n",
    "#     depth_labels_vis = (depth_labels - depth_labels.min()) / (depth_labels.max() - depth_labels.min() + 1e-5)\n",
    "#     depth_preds_vis = (depth_output - depth_output.min()) / (depth_output.max() - depth_output.min() + 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = min(4, inputs.size(0))  # Limit to 4 samples for visualization\n",
    "    fig, axes = plt.subplots(batch_size, 5, figsize=(15, 4 * batch_size))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        inputs_temp = inputs[i]\n",
    "        # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "        inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "        \n",
    "        depth_labels_vis = (depth_labels[i] - depth_labels[i].min()) / (depth_labels[i].max() - depth_labels[i].min() + 1e-5)\n",
    "        depth_preds = depth_output[i]\n",
    "        depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5)\n",
    "        # print(f\"depth_labels_vis: {depth_labels_vis.shape}\")\n",
    "        # print(f\"depth_preds_vis: {depth_preds_vis.shape}\")\n",
    "\n",
    "    \n",
    "        \n",
    "        # Row 1: Ground truth\n",
    "        axes[i, 0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "        axes[i, 0].set_title(\"RGB Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(seg_labels[i], cmap=\"tab20\")\n",
    "        axes[i, 1].set_title(\"GT Segmentation\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(depth_labels_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 2].set_title(\"GT Depth\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "        # Row 2: Predictions\n",
    "        axes[i, 3].imshow(seg_output[i], cmap=\"tab20\")\n",
    "        axes[i, 3].set_title(\"Generated Segmentation\")\n",
    "        axes[i, 3].axis(\"off\")\n",
    "\n",
    "        axes[i, 4].imshow(depth_preds_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 4].set_title(\"Generated Depth\")\n",
    "        axes[i, 4].axis(\"off\")\n",
    "        \n",
    "    # Remove axes for cleaner visualization\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    fig.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # # Save current epoch as an image for GIF\n",
    "    # epoch_img_path = os.path.join(gif_path, f\"epoch_{epoch}.png\")\n",
    "    # os.makedirs(gif_path, exist_ok=True)\n",
    "    # plt.savefig(epoch_img_path)\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    # return epoch_img_path\n",
    "    frame = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)  # Updated to buffer_rgba\n",
    "    frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (4,))  # RGBA has 4 channels\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Convert to PIL.Image for GIF\n",
    "    frame_rgb = frame[:, :, :3] \n",
    "\n",
    "    # Return as PIL.Image for GIF creation\n",
    "    # return Image.fromarray(frame)\n",
    "    return Image.fromarray(frame_rgb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55d241-89e1-47d7-9552-60deb4df0251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d01be9a0-180e-44bf-a549-ba003cbf2bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Define the ResBlock\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "# Define the CRPBlock\n",
    "class CRPBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, n_stages=4, groups=False):\n",
    "        super(CRPBlock, self).__init__()\n",
    "        self.n_stages = n_stages\n",
    "        groups = in_chans if groups else 1\n",
    "        self.mini_blocks = nn.ModuleList()\n",
    "        for _ in range(n_stages):\n",
    "            self.mini_blocks.append(nn.MaxPool2d(kernel_size=5, stride=1, padding=2))\n",
    "            self.mini_blocks.append(nn.Conv2d(in_chans, out_chans, kernel_size=1, bias=False, groups=groups))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for block in self.mini_blocks:\n",
    "            out = block(out)\n",
    "            x = x + out\n",
    "        return x\n",
    "\n",
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, feature_dim=256):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        base_model = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Freeze pre-trained layers\n",
    "        for param in base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Extract ResNet layers and modify strides/pooling to preserve spatial dimensions\n",
    "        layers = list(base_model.children())[:-2]  # Remove FC and AvgPool layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                layer.stride = (1, 1)  # Set stride to 1\n",
    "            elif isinstance(layer, nn.MaxPool2d) or isinstance(layer, nn.AvgPool2d):\n",
    "                layer.stride = (1, 1)  # Avoid reducing dimensions with pooling layers\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # Adjust final feature dimension using a 1x1 convolution\n",
    "        self.feature_dim = feature_dim\n",
    "        self.adjust_channels = nn.Conv2d(base_model.fc.in_features, feature_dim, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Extract features without changing spatial dimensions\n",
    "        x = self.adjust_channels(x)  # Adjust feature channels\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70ae93ff-1a2e-46c9-92cf-57767c645dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CityScapesDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, split='train', label_map='id', crop=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "        self.crop = crop\n",
    "\n",
    "        self.left_paths = glob(os.path.join(root, 'leftImg8bit', split, '**/*.png'))\n",
    "        self.mask_paths = glob(os.path.join(root, 'gtFine', split, '**/*gtFine_labelIds.png'))\n",
    "        self.depth_paths = glob(os.path.join(root, 'crestereo_depth2', split, '**/*.npy'))\n",
    "\n",
    "        sorted(self.left_paths)\n",
    "        sorted(self.mask_paths)\n",
    "        sorted(self.depth_paths)\n",
    "\n",
    "        # get label mappings\n",
    "        self.id_2_train = {}\n",
    "        self.id_2_cat = {}\n",
    "        self.train_2_id = {}\n",
    "        self.id_2_name = {-1 : 'unlabeled'}\n",
    "        self.trainid_2_name = {19 : 'unlabeled'} # {255 : 'unlabeled', -1 : 'unlabeled'}\n",
    "\n",
    "        for lbl in labels:\n",
    "            self.id_2_train.update({lbl.id : lbl.trainId})\n",
    "            self.id_2_cat.update({lbl.id : lbl.categoryId})\n",
    "            if lbl.trainId != 19: # (lbl.trainId > 0) and (lbl.trainId != 255):\n",
    "                self.trainid_2_name.update({lbl.trainId : lbl.name})\n",
    "                self.train_2_id.update({lbl.trainId : lbl.id})\n",
    "            if lbl.id > 0:\n",
    "                self.id_2_name.update({lbl.id : lbl.name})\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left = cv2.cvtColor(cv2.imread(self.left_paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_UNCHANGED).astype(np.uint8)\n",
    "        depth = np.load(self.depth_paths[idx]) # data is type float16\n",
    "\n",
    "        if self.crop:\n",
    "            left = left[:800, :, :]\n",
    "            mask = mask[:800, :]\n",
    "            depth = depth[:800, :]\n",
    "\n",
    "        # get label id\n",
    "        if self.label_map == 'id':\n",
    "            mask[mask==-1] == 0\n",
    "        elif self.label_map == 'trainId':\n",
    "            for _id, train_id in self.id_2_train.items():\n",
    "                mask[mask==_id] = train_id\n",
    "        elif self.label_map == 'categoryId':\n",
    "            for _id, train_id in self.id_2_cat.items():\n",
    "                mask[mask==_id] = train_id\n",
    "\n",
    "        sample = {'left' : left, 'mask' : mask, 'depth' : depth}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # ensure that no depth values are less than 0\n",
    "        depth[depth < 0] = 0\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        print(f\"Number of RGB images: {len(self.left_paths)}\")\n",
    "        print(f\"Number of Mask images: {len(self.mask_paths)}\")\n",
    "        print(f\"Number of depth images: {len(self.depth_paths)}\")\n",
    "        return len(self.left_paths)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c92a35c2-3d5c-4f87-87ac-4b189c154bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OG_W, OG_H = 2048, 800 # OG width and height after crop\n",
    "W, H = OG_W//4, OG_H//4 # resize w,h for training\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    RandomCrop(H, W),\n",
    "    # ElasticTransform(alpha=100.0, sigma=25.0, prob=0.5),\n",
    "    AddColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    RandomHorizontalFlip(0.5),\n",
    "    RandomVerticalFlip(0.2),\n",
    "    # RandomRotate((-30, 30)),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Rescale(H, W),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CityScapesDataset(root, transform=transform, split='train', label_map='trainId') # 'trainId')\n",
    "train_subset = Subset(train_dataset, indices=range(2968)) #2968\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n",
    "\n",
    "\n",
    "valid_dataset = CityScapesDataset(root, transform=valid_transform, split='val', label_map='trainId')\n",
    "val_subset = Subset(valid_dataset, indices=range(496)) #496 \n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=False)\n",
    "valid_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4612e4a-638e-4b58-9cd1-7c6f59edf9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b96a8e6-8ceb-4ce9-b268-73df64cc0c12",
   "metadata": {},
   "source": [
    "# shared Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1dc8478-39f9-41f3-bb56-02ffccc724bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SharedGenerator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         Shared Generator for both tasks.\n",
    "#         Contains shared layers for skip connection processing and refinement.\n",
    "#         \"\"\"\n",
    "#         super(SharedGenerator, self).__init__()\n",
    "#         # Shared convolution layers to process each skip connection\n",
    "#         self.shared_conv1 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l11_out (1/32)\n",
    "#         self.shared_conv2 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l7_out (1/16)\n",
    "#         self.shared_conv3 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l3_out (1/8)\n",
    "#         self.shared_conv4 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l1_out (1/4)\n",
    "\n",
    "#         # Shared CRP blocks for refinement\n",
    "#         self.shared_crp1 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/32\n",
    "#         self.shared_crp2 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/16\n",
    "#         self.shared_crp3 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/8\n",
    "#         self.shared_crp4 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/4\n",
    "\n",
    "#     def forward(self, skips):\n",
    "#         \"\"\"\n",
    "#         Process skips with shared layers for task-specific generation.\n",
    "#         Args:\n",
    "#             skips (dict): Skip connections from the encoder.\n",
    "#         Returns:\n",
    "#             dict: Processed skip connections.\n",
    "#         \"\"\"\n",
    "#         x1 = self.shared_crp1(self.shared_conv1(skips[\"l11_out\"]))\n",
    "#         x2 = self.shared_crp2(self.shared_conv2(skips[\"l7_out\"]))\n",
    "#         x3 = self.shared_crp3(self.shared_conv3(skips[\"l3_out\"]))\n",
    "#         x4 = self.shared_crp4(self.shared_conv4(skips[\"l1_out\"]))\n",
    "\n",
    "#         return {\"x1\": x1, \"x2\": x2, \"x3\": x3, \"x4\": x4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e4d36-98bd-4506-ad2f-3220cd3661f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb487e-4641-481a-9b15-56d1ce9a38ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e74154-b3ec-4e4c-aa36-aa2e9f5545fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa913d77-fd04-4079-a039-6caf557329a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SharedPix2PixGenerator(nn.Module):\n",
    "#     def __init__(self, seg_output_channels=20, depth_output_channels=1):\n",
    "#         \"\"\"\n",
    "#         Shared Pix2Pix Generator for Segmentation and Depth tasks.\n",
    "#         Args:\n",
    "#             seg_output_channels (int): Number of output channels for segmentation.\n",
    "#             depth_output_channels (int): Number of output channels for depth estimation.\n",
    "#         \"\"\"\n",
    "#         super(SharedPix2PixGenerator, self).__init__()\n",
    "#         self.shared_generator = SharedGenerator()\n",
    "#         self.seg_output_layer = TaskOutputLayer(output_channels=seg_output_channels)\n",
    "#         self.depth_output_layer = TaskOutputLayer(output_channels=depth_output_channels)\n",
    "\n",
    "#     def forward(self, skips, input_size):\n",
    "#         \"\"\"\n",
    "#         Forward pass for both tasks.\n",
    "#         Args:\n",
    "#             skips (dict): Skip connections from the encoder.\n",
    "#             input_size (tuple): Original input size (H, W).\n",
    "#         Returns:\n",
    "#             dict: Outputs for segmentation and depth tasks.\n",
    "#         \"\"\"\n",
    "#         shared_features = self.shared_generator(skips)\n",
    "\n",
    "#         # Task-specific outputs\n",
    "#         seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "#         depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "#         return {\n",
    "#             \"seg_output\": seg_output,\n",
    "#             \"depth_output\": depth_output\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80b7b6-15b9-47dd-bdea-af6fa22b055b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59b8dc73-e69d-4f80-b1e4-09507649a504",
   "metadata": {},
   "source": [
    "# Saving batch gif code And function to plot al losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36faf655-f78f-4cc8-99af-8de9a1ecabcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels):\n",
    "    inputs = inputs.detach().cpu()\n",
    "    seg_output = torch.argmax(seg_output, dim=1).detach().cpu()\n",
    "    depth_output = depth_output.detach().cpu()\n",
    "    seg_labels = seg_labels.detach().cpu()\n",
    "    depth_labels = depth_labels.detach().cpu()\n",
    "    \n",
    "#     inputs_rgb = (inputs - inputs.min()) / (inputs.max() - inputs.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "    \n",
    "#     # Normalize depth maps for visualization\n",
    "#     depth_labels_vis = (depth_labels - depth_labels.min()) / (depth_labels.max() - depth_labels.min() + 1e-5)\n",
    "#     depth_preds_vis = (depth_output - depth_output.min()) / (depth_output.max() - depth_output.min() + 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = min(4, inputs.size(0))  # Limit to 4 samples for visualization\n",
    "    fig, axes = plt.subplots(batch_size, 5, figsize=(15, 4 * batch_size))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        inputs_temp = inputs[i]\n",
    "        # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "        inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "        \n",
    "        depth_labels_vis = (depth_labels[i] - depth_labels[i].min()) / (depth_labels[i].max() - depth_labels[i].min() + 1e-5)\n",
    "        depth_preds = depth_output[i]\n",
    "        depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5)\n",
    "        # print(f\"depth_labels_vis: {depth_labels_vis.shape}\")\n",
    "        # print(f\"depth_preds_vis: {depth_preds_vis.shape}\")\n",
    "\n",
    "    \n",
    "        \n",
    "        # Row 1: Ground truth\n",
    "        axes[i, 0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "        axes[i, 0].set_title(\"RGB Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(seg_labels[i], cmap=\"tab20\")\n",
    "        axes[i, 1].set_title(\"GT Segmentation\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(depth_labels_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 2].set_title(\"GT Depth\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "        # Row 2: Predictions\n",
    "        axes[i, 3].imshow(seg_output[i], cmap=\"tab20\")\n",
    "        axes[i, 3].set_title(\"Generated Segmentation\")\n",
    "        axes[i, 3].axis(\"off\")\n",
    "\n",
    "        axes[i, 4].imshow(depth_preds_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 4].set_title(\"Generated Depth\")\n",
    "        axes[i, 4].axis(\"off\")\n",
    "        \n",
    "    # Remove axes for cleaner visualization\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    fig.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # # Save current epoch as an image for GIF\n",
    "    # epoch_img_path = os.path.join(gif_path, f\"epoch_{epoch}.png\")\n",
    "    # os.makedirs(gif_path, exist_ok=True)\n",
    "    # plt.savefig(epoch_img_path)\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    # return epoch_img_path\n",
    "    frame = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)  # Updated to buffer_rgba\n",
    "    frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (4,))  # RGBA has 4 channels\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Convert to PIL.Image for GIF\n",
    "    frame_rgb = frame[:, :, :3] \n",
    "\n",
    "    # Return as PIL.Image for GIF creation\n",
    "    # return Image.fromarray(frame)\n",
    "    return Image.fromarray(frame_rgb)\n",
    "\n",
    "def plot_all_losses(train_losses,valid_losses,save_dir):\n",
    "    # Plot training and validation losses\n",
    "    for key in train_losses.keys():\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses[key], label=f\"Train {key}\")\n",
    "        plt.plot(valid_losses[key], label=f\"Valid {key}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(key.replace(\"_\", \" \").title())\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, f\"{key}_loss.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f670c-bfdb-49d7-90f9-b1f42c9a3152",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e338594d-fd9d-4121-aeef-288680c916cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"vgg16\", layers=[\"relu3_3\"], device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Perceptual loss class.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model (str): Pretrained model to use (e.g., \"vgg16\").\n",
    "            layers (list of str): Layers to extract features from.\n",
    "            device (str): Device to load the pretrained model on (\"cuda\" or \"cpu\").\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained model\n",
    "        if pretrained_model == \"vgg16\":\n",
    "            vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pretrained model: {pretrained_model}\")\n",
    "\n",
    "        # Freeze the parameters\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Select layers\n",
    "        self.layers = layers\n",
    "        self.feature_extractor = nn.ModuleDict({\n",
    "            layer: vgg[:i] for i, layer in enumerate(vgg._modules.keys()) if layer in self.layers\n",
    "        })\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        \"\"\"\n",
    "        Compute perceptual loss between generated and target images.\n",
    "\n",
    "        Args:\n",
    "            generated (torch.Tensor): Generated image batch.\n",
    "            target (torch.Tensor): Target image batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: MSE loss between extracted features.\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        for layer_name, extractor in self.feature_extractor.items():\n",
    "            gen_features = extractor(generated)\n",
    "            target_features = extractor(target)\n",
    "            loss += F.mse_loss(gen_features, target_features)\n",
    "        return loss\n",
    "\n",
    "def scale_invariant_depth_loss(pred, target, lambda_weight=0.1):\n",
    "    if pred.shape != target.shape:\n",
    "        pred = F.interpolate(pred, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "    \n",
    "    diff = pred - target\n",
    "    n = diff.numel()\n",
    "    mse = torch.sum(diff**2) / n\n",
    "    scale_invariant = mse - (lambda_weight / (n**2)) * (torch.sum(diff))**2\n",
    "    return scale_invariant\n",
    "\n",
    "def depth_smoothness_loss(pred, img, alpha=1.0):\n",
    "    depth_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])\n",
    "    depth_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])\n",
    "    img_grad_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), dim=1, keepdim=True)\n",
    "    img_grad_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), dim=1, keepdim=True)\n",
    "    smoothness_x = depth_grad_x * torch.exp(-alpha * img_grad_x)\n",
    "    smoothness_y = depth_grad_y * torch.exp(-alpha * img_grad_y)\n",
    "    return smoothness_x.mean() + smoothness_y.mean()\n",
    "\n",
    "\n",
    "def inv_huber_loss(pred, target, delta=0.1):\n",
    "    \"\"\"\n",
    "    Inverse Huber loss for depth prediction.\n",
    "    Args:\n",
    "        pred (Tensor): Predicted depth map.\n",
    "        target (Tensor): Ground truth depth map.\n",
    "        delta (float): Threshold for switching between quadratic and linear terms.\n",
    "    Returns:\n",
    "        Tensor: Inverse Huber loss.\n",
    "    \"\"\"\n",
    "    abs_diff = torch.abs(pred - target)\n",
    "    delta_tensor = torch.tensor(delta, dtype=abs_diff.dtype, device=abs_diff.device)  # Convert delta to tensor\n",
    "    quadratic = torch.minimum(abs_diff, delta_tensor)\n",
    "    linear = abs_diff - quadratic\n",
    "    return (0.5 * quadratic**2 + delta_tensor * linear).mean()\n",
    "\n",
    "\n",
    "def mean_iou(pred, target, num_classes):\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    intersection = torch.logical_and(pred == target, target != 255).float()  # Ignore class 255\n",
    "    union = torch.logical_or(pred == target, target != 255).float()\n",
    "    iou = torch.sum(intersection) / torch.sum(union)\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def contrastive_loss(pred, target, margin=1.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss to ensure the depth map predictions are closer to the target.\n",
    "    \"\"\"\n",
    "    # Flatten the tensors for element-wise operations\n",
    "    pred_flat = pred.view(pred.size(0), -1)  # Flatten except for the batch dimension\n",
    "    target_flat = target.view(target.size(0), -1)  # Flatten except for the batch dimension\n",
    "\n",
    "    # Compute the pairwise distances\n",
    "    distances = torch.sqrt(torch.sum((pred_flat - target_flat) ** 2, dim=1))  # Batch-wise distances\n",
    "\n",
    "    # Create labels for contrastive loss\n",
    "    labels = (torch.abs(pred_flat - target_flat).mean(dim=1) < margin).float()  # Batch-wise labels\n",
    "\n",
    "    # Calculate contrastive loss\n",
    "    similar_loss = labels * distances**2\n",
    "    dissimilar_loss = (1 - labels) * torch.clamp(margin - distances, min=0)**2\n",
    "    loss = (similar_loss + dissimilar_loss).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def dice_loss(predictions, targets, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Dice Loss for segmentation.\n",
    "    Args:\n",
    "        predictions (torch.Tensor): The predicted segmentation map (logits or probabilities).\n",
    "                                    Shape: [batch_size, num_classes, height, width]\n",
    "        targets (torch.Tensor): The ground truth segmentation map (one-hot encoded or integer labels).\n",
    "                                Shape: [batch_size, height, width]\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    Returns:\n",
    "        torch.Tensor: Dice Loss (scalar).\n",
    "    \"\"\"\n",
    "    # Convert integer labels to one-hot if needed\n",
    "    if predictions.shape != targets.shape:\n",
    "        targets = F.one_hot(targets, num_classes=predictions.shape[1]).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    # Apply softmax to predictions for multi-class segmentation\n",
    "    predictions = torch.softmax(predictions, dim=1)\n",
    "    \n",
    "    # Flatten tensors to calculate intersection and union\n",
    "    predictions_flat = predictions.view(predictions.shape[0], predictions.shape[1], -1)\n",
    "    targets_flat = targets.view(targets.shape[0], targets.shape[1], -1)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = (predictions_flat * targets_flat).sum(dim=-1)\n",
    "    union = predictions_flat.sum(dim=-1) + targets_flat.sum(dim=-1)\n",
    "    \n",
    "    # Calculate Dice Coefficient\n",
    "    dice_coeff = (2 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # Dice Loss\n",
    "    return 1 - dice_coeff.mean()\n",
    "\n",
    "\n",
    "def initialize_optimizers_and_schedulers(model, lr_gen=1e-4, lr_disc=1e-4, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Initialize optimizers and schedulers for all generators and discriminators.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): MultiTaskModel instance.\n",
    "        lr_gen (float): Learning rate for generators.\n",
    "        lr_disc (float): Learning rate for discriminators.\n",
    "        weight_decay (float): Weight decay for optimizers.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Optimizers and schedulers for generators and discriminators.\n",
    "    \"\"\"\n",
    "    # Optimizers for shared generator\n",
    "    optimizer_shared_gen = torch.optim.AdamW(\n",
    "        model.feature_generator.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_shared_gen = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_shared_gen, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # Optimizer and scheduler for the shared generator's refinement layer\n",
    "    optimizer_shared_refine = torch.optim.AdamW(\n",
    "        model.shared_generator.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_shared_refine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_shared_refine, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # Optimizers and schedulers for task-specific generators\n",
    "    optimizer_seg_gen = torch.optim.AdamW(\n",
    "        model.seg_output_layer.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_seg_gen = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_seg_gen, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    optimizer_depth_gen = torch.optim.AdamW(\n",
    "        model.depth_output_layer.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_depth_gen = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_depth_gen, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    # Optimizers and schedulers for task-specific discriminators\n",
    "    optimizer_seg_disc = torch.optim.AdamW(\n",
    "        model.seg_discriminator.parameters(),\n",
    "        lr=lr_disc,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_seg_disc = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_seg_disc, step_size=20, gamma=0.1\n",
    "    )\n",
    "\n",
    "    optimizer_depth_disc = torch.optim.AdamW(\n",
    "        model.depth_discriminator.parameters(),\n",
    "        lr=lr_disc,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_depth_disc = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_depth_disc, step_size=20, gamma=0.1\n",
    "    )\n",
    "\n",
    "    # Optimizer and scheduler for the multi-task discriminator\n",
    "    optimizer_multi_task_disc = torch.optim.AdamW(\n",
    "        model.multi_task_discriminator.parameters(),\n",
    "        lr=lr_disc,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_multi_task_disc = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_multi_task_disc, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"optimizers\": {\n",
    "            \"shared_gen\": optimizer_shared_gen,\n",
    "            \"shared_refine\": optimizer_shared_refine,\n",
    "            \"seg_gen\": optimizer_seg_gen,\n",
    "            \"depth_gen\": optimizer_depth_gen,\n",
    "            \"seg_disc\": optimizer_seg_disc,\n",
    "            \"depth_disc\": optimizer_depth_disc,\n",
    "            \"multi_task_disc\": optimizer_multi_task_disc\n",
    "        },\n",
    "        \"schedulers\": {\n",
    "            \"shared_gen\": scheduler_shared_gen,\n",
    "            \"shared_refine\": scheduler_shared_refine,\n",
    "            \"seg_gen\": scheduler_seg_gen,\n",
    "            \"depth_gen\": scheduler_depth_gen,\n",
    "            \"seg_disc\": scheduler_seg_disc,\n",
    "            \"depth_disc\": scheduler_depth_disc,\n",
    "            \"multi_task_disc\": scheduler_multi_task_disc\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b91ad4-6f1c-4f07-b1ee-098fb0b0b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13bfe288-9700-45ec-8835-9795a3d51277",
   "metadata": {},
   "source": [
    "## MultiTaskModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb51d0-879f-40b1-9092-731e2d247b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ad8610-be7e-460c-93f2-cb1f579d6b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MobileNetV3Backbone(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.proj_l1 = nn.Conv2d(16, 576, kernel_size=1, bias=False)   # For l1_out (1/4 resolution)\n",
    "        self.proj_l3 = nn.Conv2d(24, 576, kernel_size=1, bias=False)  # For l3_out (1/8 resolution)\n",
    "        self.proj_l7 = nn.Conv2d(48, 576, kernel_size=1, bias=False)  # For l7_out (1/16 resolution)\n",
    "        self.proj_l11 = nn.Conv2d(96, 576, kernel_size=1, bias=False) # For l11_out (1/32 resolution)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Passes input theough MobileNetV3 backbone feature extraction layers\n",
    "            layers to add connections to (0 indexed)\n",
    "                - 1:  1/4 res\n",
    "                - 3:  1/8 res\n",
    "                - 7, 8:  1/16 res\n",
    "                - 10, 11: 1/32 res\n",
    "           \"\"\"\n",
    "        # skips = nn.ParameterDict()\n",
    "        # for i in range(len(self.backbone) - 1):\n",
    "        #     x = self.backbone[i](x)\n",
    "        #     # add skip connection outputs\n",
    "        #     if i in [1, 3, 7, 11]:\n",
    "        #         skips.update({f\"l{i}_out\" : x})\n",
    "\n",
    "        # return skips\n",
    "        skips = {}  # Dictionary to store skip connections\n",
    "\n",
    "        for i, layer in enumerate(self.backbone):\n",
    "            x = layer(x)\n",
    "            # Add skip connections for specific layers\n",
    "            if i == 1:\n",
    "                skips[\"l1_out\"] = self.proj_l1(x)  # Project l1_out\n",
    "            elif i == 3:\n",
    "                skips[\"l3_out\"] = self.proj_l3(x)  # Project l3_out\n",
    "            elif i == 7:\n",
    "                skips[\"l7_out\"] = self.proj_l7(x)  # Project l7_out\n",
    "            elif i == 11:\n",
    "                skips[\"l11_out\"] = self.proj_l11(x)  # Project l11_out\n",
    "\n",
    "        return skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9bf6050-8f9c-439c-b125-bc1b9955a4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnhancedSharedGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Enhanced Shared Generator for Segmentation and Depth tasks.\n",
    "        Includes additional refinement layers for better generalization.\n",
    "        \"\"\"\n",
    "        super(EnhancedSharedGenerator, self).__init__()\n",
    "        # Shared convolution layers to process each skip connection\n",
    "        self.shared_conv1 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l11_out (1/32)\n",
    "        self.shared_conv2 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l7_out (1/16)\n",
    "        self.shared_conv3 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l3_out (1/8)\n",
    "        self.shared_conv4 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l1_out (1/4)\n",
    "\n",
    "        # CRP blocks for refinement\n",
    "        self.shared_crp1 = CRPBlock(256, 256, n_stages=4)\n",
    "        self.shared_crp2 = CRPBlock(256, 256, n_stages=4)\n",
    "        self.shared_crp3 = CRPBlock(256, 256, n_stages=4)\n",
    "        self.shared_crp4 = CRPBlock(256, 256, n_stages=4)\n",
    "\n",
    "        # Additional refinement layers\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, skips):\n",
    "        \"\"\"\n",
    "        Process skips with shared layers for task-specific generation.\n",
    "        Args:\n",
    "            skips (dict): Skip connections from the encoder.\n",
    "        Returns:\n",
    "            dict: Processed skip connections.\n",
    "        \"\"\"\n",
    "        x1 = self.shared_crp1(self.shared_conv1(skips[\"l11_out\"]))\n",
    "        x1 = self.refine(x1)  # Extra refinement\n",
    "\n",
    "        x2 = self.shared_crp2(self.shared_conv2(skips[\"l7_out\"]))\n",
    "        x2 = self.refine(x2)\n",
    "\n",
    "        x3 = self.shared_crp3(self.shared_conv3(skips[\"l3_out\"]))\n",
    "        x3 = self.refine(x3)\n",
    "\n",
    "        x4 = self.shared_crp4(self.shared_conv4(skips[\"l1_out\"]))\n",
    "        x4 = self.refine(x4)\n",
    "\n",
    "        return {\"x1\": x1, \"x2\": x2, \"x3\": x3, \"x4\": x4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2783c167-438d-4093-9b90-256011e88b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaskOutputLayer(nn.Module):\n",
    "    def __init__(self, output_channels):\n",
    "        \"\"\"\n",
    "        Task-specific output layers for generating final predictions.\n",
    "        Args:\n",
    "            output_channels (int): Number of output channels (e.g., 20 for segmentation, 1 for depth).\n",
    "        \"\"\"\n",
    "        super(TaskOutputLayer, self).__init__()\n",
    "        self.final_conv = nn.Conv2d(256, output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, input_size):\n",
    "        \"\"\"\n",
    "        Generate task-specific output.\n",
    "        Args:\n",
    "            x (Tensor): Input feature map.\n",
    "            input_size (tuple): Original input size (H, W).\n",
    "        Returns:\n",
    "            Tensor: Task-specific output.\n",
    "        \"\"\"\n",
    "        x = self.final_conv(x)\n",
    "        return nn.functional.interpolate(x, size=input_size, mode=\"bilinear\", align_corners=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac107db7-0257-45ec-b7af-e09ff552760d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaskSpecificDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(TaskSpecificDiscriminator, self).__init__()\n",
    "        self.adapt_conv = nn.Conv2d(input_channels+input_channels, input_channels, kernel_size=1, bias=False)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, task_output, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the discriminator.\n",
    "\n",
    "        Args:\n",
    "            task_output (Tensor): Output from the generator (e.g., seg_output or depth_output).\n",
    "            labels (Tensor, optional): Ground truth labels. If provided, aligns channels with task_output.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Discriminator's prediction.\n",
    "        \"\"\"\n",
    "        if labels is not None:\n",
    "            # Ensure labels match the shape of task_output\n",
    "            if labels.dim() < task_output.dim():\n",
    "                labels = labels.unsqueeze(1)  # Add channel dimension if needed\n",
    "            if labels.size(1) != task_output.size(1):\n",
    "                labels = torch.nn.functional.one_hot(labels.squeeze(1), num_classes=task_output.size(1))\n",
    "                labels = labels.permute(0, 3, 1, 2).float().to(task_output.device)\n",
    "            combined = torch.cat([task_output, labels], dim=1)\n",
    "            combined = self.adapt_conv(combined)\n",
    "        else:\n",
    "            combined = task_output\n",
    "\n",
    "        return self.model(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc64fce4-dfa5-4884-addd-2d46b50dba0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiTaskDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        \"\"\"\n",
    "        Multi-Task Discriminator for evaluating all task-specific outputs.\n",
    "        Args:\n",
    "            input_channels (int): Number of input channels for concatenated features and outputs.\n",
    "        \"\"\"\n",
    "        super(MultiTaskDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Evaluate input image and task-specific outputs.\n",
    "        Args:\n",
    "            inputs (Tensor): Input image.\n",
    "            outputs (list[Tensor]): List of task-specific outputs.\n",
    "        Returns:\n",
    "            Tensor: Discriminator output.\n",
    "        \"\"\"\n",
    "        # combined = torch.cat([inputs] + outputs, dim=1)\n",
    "        return self.model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd983afc-6f6f-4d21-ab3e-9d45f90e78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone, num_seg_classes=20, depth_channels=1):\n",
    "        \"\"\"\n",
    "        Multi-task model with shared Pix2Pix Generator, task-specific discriminators,\n",
    "        and a multi-task discriminator.\n",
    "        Args:\n",
    "            backbone (nn.Module): Encoder backbone for feature extraction.\n",
    "            num_seg_classes (int): Number of segmentation classes.\n",
    "            depth_channels (int): Number of output channels for depth.\n",
    "        \"\"\"\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.feature_generator = MobileNetV3Backbone(backbone)\n",
    "        self.shared_generator = EnhancedSharedGenerator()\n",
    "        self.seg_output_layer = TaskOutputLayer(output_channels=num_seg_classes)\n",
    "        self.depth_output_layer = TaskOutputLayer(output_channels=depth_channels)\n",
    "\n",
    "        # Task-specific discriminators\n",
    "        self.seg_discriminator = TaskSpecificDiscriminator(input_channels=num_seg_classes)\n",
    "        self.depth_discriminator = TaskSpecificDiscriminator(input_channels=depth_channels)\n",
    "\n",
    "        # Multi-task discriminator\n",
    "        self.multi_task_discriminator = MultiTaskDiscriminator(input_channels=3 + num_seg_classes + depth_channels)\n",
    "        \n",
    "    def forward(self, inputs, input_size, seg_labels=None, depth_labels=None, return_discriminator_outputs=False):\n",
    "        # Extract features from the encoder\n",
    "        skips = self.feature_generator(inputs)\n",
    "        shared_features = self.shared_generator(skips)\n",
    "\n",
    "        # Task-specific outputs\n",
    "        seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "        depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "        output_dict = {\n",
    "            \"seg_output\": seg_output,\n",
    "            \"depth_output\": depth_output,\n",
    "        }\n",
    "\n",
    "        if return_discriminator_outputs:\n",
    "            \n",
    "            # Detach outputs to prevent discriminator backward from interfering with the generator\n",
    "            seg_output_detached = seg_output.detach()\n",
    "            depth_output_detached = depth_output.detach()\n",
    "            \n",
    "            # Adversarial feedback from task-specific discriminators\n",
    "            seg_real_disc = self.seg_discriminator(seg_output_detached, seg_labels) if seg_labels is not None else None\n",
    "            seg_fake_disc = self.seg_discriminator(seg_output_detached, None)\n",
    "\n",
    "            depth_real_disc = self.depth_discriminator(depth_output_detached, depth_labels) if depth_labels is not None else None\n",
    "            depth_fake_disc = self.depth_discriminator(depth_output_detached, None)\n",
    "\n",
    "            # Multi-task discriminator feedback\n",
    "            combined_real_input = torch.cat([inputs, seg_labels, depth_labels], dim=1) if seg_labels is not None and depth_labels is not None else None\n",
    "            combined_fake_input = torch.cat([inputs, seg_output, depth_output], dim=1)\n",
    "\n",
    "            combined_real_disc = self.multi_task_discriminator(combined_real_input) if combined_real_input is not None else None\n",
    "            combined_fake_disc = self.multi_task_discriminator(combined_fake_input.detach())\n",
    "\n",
    "            output_dict.update({\n",
    "                \"seg_real_disc\": seg_real_disc,\n",
    "                \"seg_fake_disc\": seg_fake_disc,\n",
    "                \"depth_real_disc\": depth_real_disc,\n",
    "                \"depth_fake_disc\": depth_fake_disc,\n",
    "                \"combined_real_disc\": combined_real_disc,\n",
    "                \"combined_fake_disc\": combined_fake_disc,\n",
    "            })\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "#     def forward(self, inputs, input_size, seg_labels=None, depth_labels=None, return_discriminator_outputs=False):\n",
    "#         \"\"\"\n",
    "#         Forward pass for multi-task model.\n",
    "#         Args:\n",
    "#             inputs (Tensor): Input images.\n",
    "#             input_size (tuple): Original input size.\n",
    "#             seg_labels (Tensor, optional): Ground truth segmentation labels. Required for discriminator feedback.\n",
    "#             depth_labels (Tensor, optional): Ground truth depth labels. Required for discriminator feedback.\n",
    "#             return_discriminator_outputs (bool): If True, returns discriminator outputs for adversarial loss.\n",
    "#         Returns:\n",
    "#             dict: Outputs for segmentation and depth tasks, and optionally discriminator outputs.\n",
    "#         \"\"\"\n",
    "#         skips = self.feature_generator(inputs)\n",
    "#         shared_features = self.shared_generator(skips)\n",
    "\n",
    "#         # Task-specific outputs\n",
    "#         seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "#         depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "#         if return_discriminator_outputs:\n",
    "#             # Adversarial feedback from task-specific discriminators\n",
    "#             seg_real_disc = self.seg_discriminator(seg_output, seg_labels) if seg_labels is not None else None\n",
    "#             depth_real_disc = self.depth_discriminator(depth_output, depth_labels) if depth_labels is not None else None\n",
    "\n",
    "#             # Multi-task discriminator feedback\n",
    "#             combined_real_disc = self.multi_task_discriminator(inputs, [seg_output, depth_output])\n",
    "\n",
    "#             return {\n",
    "#                 \"seg_output\": seg_output,\n",
    "#                 \"depth_output\": depth_output,\n",
    "#                 \"seg_real_disc\": seg_real_disc,\n",
    "#                 \"depth_real_disc\": depth_real_disc,\n",
    "#                 \"combined_real_disc\": combined_real_disc\n",
    "#             }\n",
    "\n",
    "#         return {\n",
    "#             \"seg_output\": seg_output,\n",
    "#             \"depth_output\": depth_output\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adc322fb-dbb7-4cd2-a934-33118521696b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageSequence\n",
    "import os\n",
    "\n",
    "def combine_training_gifs(model_dir, save_dir2, output_path):\n",
    "    \"\"\"\n",
    "    Combine two training visualization GIFs into one.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Directory containing the first training GIF.\n",
    "        save_dir2: Directory containing the second training GIF.\n",
    "        output_path: Path to save the combined GIF.\n",
    "    \"\"\"\n",
    "    # Find the GIF files\n",
    "    model_dir_gif = [file for file in os.listdir(model_dir) if file.startswith(\"training_visualization\") and file.endswith(\".gif\")]\n",
    "    save_dir2_gif = [file for file in os.listdir(save_dir2) if file.startswith(\"training_visualization\") and file.endswith(\".gif\")]\n",
    "    \n",
    "    if not model_dir_gif or not save_dir2_gif:\n",
    "        raise FileNotFoundError(\"Could not find training_visualization_*.gif in one of the directories.\")\n",
    "    \n",
    "    model_dir_gif_path = os.path.join(model_dir, model_dir_gif[0])\n",
    "    save_dir2_gif_path = os.path.join(save_dir2, save_dir2_gif[0])\n",
    "\n",
    "    # Open the GIFs\n",
    "    gif1 = Image.open(model_dir_gif_path)\n",
    "    gif2 = Image.open(save_dir2_gif_path)\n",
    "\n",
    "    # Collect all frames from both GIFs\n",
    "    combined_frames = []\n",
    "    for frame in ImageSequence.Iterator(gif1):\n",
    "        combined_frames.append(frame.copy())\n",
    "    for frame in ImageSequence.Iterator(gif2):\n",
    "        combined_frames.append(frame.copy())\n",
    "\n",
    "    # Save the combined GIF\n",
    "    combined_frames[0].save(\n",
    "        output_path,\n",
    "        save_all=True,\n",
    "        append_images=combined_frames[1:],\n",
    "        duration=gif1.info.get(\"duration\", 500),  # Use duration from the first GIF\n",
    "        loop=0\n",
    "    )\n",
    "\n",
    "    print(f\"Combined GIF saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b6e25-1715-412c-b906-714e85c0e266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dce8dd65-e71e-4d5b-86b0-a28e02fdaa23",
   "metadata": {},
   "source": [
    "# Saving loss charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce716570-bcca-40b6-91be-72d6713ff523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_all_losses(epoch, train_losses,valid_losses,save_dir):\n",
    "    # Plot training and validation losses\n",
    "    for key in train_losses.keys():\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses[key], label=f\"Train {key}\")\n",
    "        plt.plot(valid_losses[key], label=f\"Valid {key}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(key.replace(\"_\", \" \").title())\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, f\"{key}_loss_after_epoch_{epoch}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11165c9f-97c8-4240-9985-956b229c2656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d7f380-f308-45f2-a57e-7cfaa4dd9b28",
   "metadata": {},
   "source": [
    "# saving checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf7ada56-870b-4926-b793-a9e3468afcaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save checkpoint including model, optimizer, and scheduler states\n",
    "def save_checkpoint(model, opt_sched, save_path, epoch, best_loss):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_states\": {name: opt.state_dict() for name, opt in opt_sched[\"optimizers\"].items()},\n",
    "        \"scheduler_states\": {name: sched.state_dict() for name, sched in opt_sched[\"schedulers\"].items()},\n",
    "        \"epoch\": epoch,\n",
    "        \"best_loss\": best_loss\n",
    "    }\n",
    "    torch.save(checkpoint, save_path, _use_new_zipfile_serialization=True)\n",
    "    \n",
    "def load_checkpoint(model, opt_sched, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "    for name, opt in opt_sched[\"optimizers\"].items():\n",
    "        if name in checkpoint[\"optimizer_states\"]:\n",
    "            opt.load_state_dict(checkpoint[\"optimizer_states\"][name])\n",
    "            \n",
    "    for name, sched in opt_sched[\"schedulers\"].items():\n",
    "        if name in checkpoint[\"scheduler_states\"]:\n",
    "            sched.load_state_dict(checkpoint[\"scheduler_states\"][name])\n",
    "            \n",
    "    # return checkpoint[\"epoch\"], checkpoint[\"best_loss\"]\n",
    "    return checkpoint.get(\"epoch\", 0), checkpoint.get(\"best_loss\", float(\"inf\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43d19b7e-72cb-4674-9d20-b86afb22c7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "def combine_and_plot_loss_data(model_dir, save_dir2, combined_save_dir=\"all_data_from_prev_curr_epoch\"):\n",
    "    \"\"\"\n",
    "    Combines loss data from previous and current training sessions and plots combined graphs.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Path to the directory containing the previous loss-tracking CSV.\n",
    "        save_dir2: Path to the directory containing the current loss-tracking CSV.\n",
    "        combined_save_dir: Path to save the combined data and plots.\n",
    "\n",
    "    Returns:\n",
    "        combined_df: A pandas DataFrame containing the combined loss data.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    combined_save_dir = os.path.join(save_dir2, combined_save_dir)\n",
    "    os.makedirs(combined_save_dir, exist_ok=True)\n",
    "\n",
    "    # Locate CSV files\n",
    "    previous_csv = os.path.join(model_dir, [file for file in os.listdir(model_dir) if file.endswith(\".csv\")][0])\n",
    "    current_csv = os.path.join(save_dir2, [file for file in os.listdir(save_dir2) if file.endswith(\".csv\")][0])\n",
    "\n",
    "    # Load data into pandas DataFrames\n",
    "    previous_df = pd.read_csv(previous_csv)\n",
    "    current_df = pd.read_csv(current_csv)\n",
    "\n",
    "    # Update epoch numbers in the current DataFrame\n",
    "    max_prev_epoch = previous_df[\"epoch\"].max()\n",
    "    current_df[\"epoch\"] += max_prev_epoch\n",
    "\n",
    "    # Combine the DataFrames\n",
    "    combined_df = pd.concat([previous_df, current_df], ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame\n",
    "    combined_csv_path = os.path.join(combined_save_dir, \"combined_loss_tracking.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Combined loss data saved to {combined_csv_path}\")\n",
    "\n",
    "    # # Generate plots for each loss type\n",
    "    # loss_columns = [\"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\", \"train_adv_loss\",\n",
    "    #                 \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\", \"valid_adv_loss\"]\n",
    "    # for col in loss_columns:\n",
    "    #     plt.figure()\n",
    "    #     plt.plot(combined_df[\"epoch\"], combined_df[col], label=col)\n",
    "    #     plt.xlabel(\"Epoch\")\n",
    "    #     plt.ylabel(\"Loss\")\n",
    "    #     plt.title(f\"{col.replace('_', ' ').title()} Over Epochs\")\n",
    "    #     plt.legend()\n",
    "    #     plot_path = os.path.join(combined_save_dir, f\"{col}_plot.png\")\n",
    "    #     plt.savefig(plot_path)\n",
    "    #     plt.close()\n",
    "    #     print(f\"Plot saved to {plot_path}\")\n",
    "    # Generate combined plots for train and valid losses\n",
    "    \n",
    "    loss_types = [\"seg_loss\", \"depth_loss\", \"combined_loss\", \"adv_loss\"]\n",
    "    for loss_type in loss_types:\n",
    "        train_loss_col = f\"train_{loss_type}\"\n",
    "        valid_loss_col = f\"valid_{loss_type}\"\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(combined_df[\"epoch\"], combined_df[train_loss_col], label=f\"Train {loss_type.capitalize()}\")\n",
    "        plt.plot(combined_df[\"epoch\"], combined_df[valid_loss_col], label=f\"Valid {loss_type.capitalize()}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"{loss_type.replace('_', ' ').capitalize()} Over Epochs\")\n",
    "        plt.legend()\n",
    "        plot_path = os.path.join(combined_save_dir, f\"{loss_type}_train_valid_plot.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Combined plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0a88c39-c135-4bcc-9995-732df53562bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf9c5647-e263-48ba-a1c5-f2d1898da89c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes',\n",
       " 'results_test8')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd(),'results_test8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6cf32-b092-4ee0-b628-f4b9ab3cf990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb691baa-d9fc-4d44-9abb-305a026d5847",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "238ab35f-3458-488c-b170-a1def0a1b714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_with_adversarial_loss_tracking(\n",
    "    model, train_loader, valid_loader, num_epochs, device, opt_sched, save_dir=\"results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a multi-task model with adversarial feedback and tracks losses.\n",
    "    \n",
    "    Args:\n",
    "        model: Multi-task model with integrated generators and discriminators.\n",
    "        train_loader: DataLoader for training data.\n",
    "        valid_loader: DataLoader for validation data.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        device: Device for training (\"cuda\" or \"cpu\").\n",
    "        opt_sched: Dictionary of optimizers and schedulers for generators and discriminators.\n",
    "        save_dir: Directory to save results.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, valid_losses: Lists of losses for training and validation.\n",
    "    \"\"\"\n",
    "    # Create directories for saving results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = os.path.join(save_dir, timestamp)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare CSV file for loss tracking\n",
    "    csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "    gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "    \n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "            \"train_adv_loss\", \n",
    "            # \"train_seg_iou\",\n",
    "            \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "            \"valid_adv_loss\", \n",
    "            # \"valid_seg_iou\"\n",
    "        ])\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    best_combined_loss = float(\"inf\")\n",
    "    gif_frames =[]\n",
    "    perceptual_loss_fn = PerceptualLoss(pretrained_model=\"vgg16\").to(device)\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "        num_batches = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", unit=\"batch\") as pbar:\n",
    "            for batch in train_loader:\n",
    "                inputs, seg_labels, depth_labels = (\n",
    "                    batch[\"left\"].to(device),\n",
    "                    batch[\"mask\"].to(device),\n",
    "                    batch[\"depth\"].to(device),\n",
    "                )\n",
    "                input_size = inputs.size()[-2:]\n",
    "\n",
    "                # Preprocess seg_labels to one-hot encoding\n",
    "                if seg_labels.size(1) == 1:  # If class indices are given\n",
    "                    seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "                    seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)  # Convert to [B, C, H, W]\n",
    "\n",
    "                # Ensure depth_labels has correct dimensions\n",
    "                if depth_labels.dim() == 5:  # If depth_labels has extra dimensions\n",
    "                    depth_labels = depth_labels.squeeze(2)\n",
    "\n",
    "                # Zero gradients\n",
    "                for optimizer in opt_sched[\"optimizers\"].values():\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass with discriminator outputs\n",
    "                outputs = model(\n",
    "                    inputs,\n",
    "                    input_size=input_size,\n",
    "                    seg_labels=seg_labels,\n",
    "                    depth_labels=depth_labels,\n",
    "                    return_discriminator_outputs=True,\n",
    "                )\n",
    "\n",
    "                # Generator losses\n",
    "                seg_loss = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + \\\n",
    "                           dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "                depth_loss = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                             inv_huber_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                             depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "                \n",
    "                seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "                depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "                \n",
    "                seg_loss = seg_loss + 0.1 * seg_perceptual_loss\n",
    "                depth_loss = depth_loss + 0.1 * depth_perceptual_loss\n",
    "                \n",
    "\n",
    "                adv_loss = -(\n",
    "                    torch.mean(outputs[\"seg_real_disc\"]) +\n",
    "                    torch.mean(outputs[\"depth_real_disc\"]) +\n",
    "                    torch.mean(outputs[\"combined_real_disc\"])\n",
    "                )\n",
    "\n",
    "                combined_loss = seg_loss + depth_loss + 0.01 * adv_loss\n",
    "\n",
    "                # Backpropagation for generators\n",
    "                combined_loss.backward(retain_graph=True)\n",
    "                # opt_sched[\"optimizers\"][\"generator\"].step()\n",
    "                opt_sched[\"optimizers\"][\"shared_gen\"].step()\n",
    "                opt_sched[\"optimizers\"][\"shared_refine\"].step()\n",
    "                opt_sched[\"optimizers\"][\"seg_gen\"].step()\n",
    "                opt_sched[\"optimizers\"][\"depth_gen\"].step()\n",
    "\n",
    "\n",
    "                # Update task-specific discriminators\n",
    "                for task, disc_optimizer in [\n",
    "                    (\"seg\", \"seg_disc\"),\n",
    "                    (\"depth\", \"depth_disc\"),\n",
    "                ]:\n",
    "                    opt_sched[\"optimizers\"][disc_optimizer].zero_grad()\n",
    "                    real_disc_loss = torch.mean(\n",
    "                        (outputs[f\"{task}_real_disc\"] - 1) ** 2\n",
    "                    )\n",
    "                    fake_disc_loss = torch.mean(\n",
    "                        (outputs[f\"{task}_fake_disc\"].detach()) ** 2\n",
    "                    )\n",
    "                    disc_loss = (real_disc_loss + fake_disc_loss) / 2\n",
    "                    disc_loss.backward()\n",
    "                    opt_sched[\"optimizers\"][disc_optimizer].step()\n",
    "\n",
    "                # Update multi-task discriminator\n",
    "                opt_sched[\"optimizers\"][\"multi_task_disc\"].zero_grad()\n",
    "                real_combined_loss = torch.mean(\n",
    "                    (outputs[\"combined_real_disc\"] - 1) ** 2\n",
    "                )\n",
    "                fake_combined_loss = torch.mean(\n",
    "                    (outputs[\"combined_fake_disc\"].detach()) ** 2\n",
    "                )\n",
    "                combined_disc_loss = (real_combined_loss + fake_combined_loss) / 2\n",
    "                combined_disc_loss.backward()\n",
    "                opt_sched[\"optimizers\"][\"multi_task_disc\"].step()\n",
    "\n",
    "                # Update training metrics\n",
    "                epoch_train[\"seg\"] += seg_loss.item()\n",
    "                epoch_train[\"depth\"] += depth_loss.item()\n",
    "                epoch_train[\"combined\"] += combined_loss.item()\n",
    "                epoch_train[\"adv\"] += adv_loss.item()\n",
    "                # epoch_train[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "                num_batches += 1\n",
    "\n",
    "            # Average training metrics\n",
    "            for key in epoch_train.keys():\n",
    "                train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "            num_valid_batches = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    inputs, seg_labels, depth_labels = (\n",
    "                        batch[\"left\"].to(device),\n",
    "                        batch[\"mask\"].to(device),\n",
    "                        batch[\"depth\"].to(device),\n",
    "                    )\n",
    "                    input_size = inputs.size()[-2:]\n",
    "\n",
    "                    # Preprocess seg_labels to one-hot encoding\n",
    "                    if seg_labels.size(1) == 1:\n",
    "                        seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "                        seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "                    # Ensure depth_labels has correct dimensions\n",
    "                    if depth_labels.dim() == 5:\n",
    "                        depth_labels = depth_labels.squeeze(2)\n",
    "\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(\n",
    "                        inputs,\n",
    "                        input_size=input_size,\n",
    "                        seg_labels=seg_labels,\n",
    "                        depth_labels=depth_labels,\n",
    "                        return_discriminator_outputs=True,\n",
    "                    )\n",
    "\n",
    "                    # Validation loss calculations\n",
    "                    seg_loss = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + \\\n",
    "                               dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "                    depth_loss = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                                 inv_huber_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                                 depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "                    \n",
    "                    seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "                    depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "\n",
    "                    seg_loss = seg_loss + 0.1 * seg_perceptual_loss\n",
    "                    depth_loss = depth_loss + 0.1 * depth_perceptual_loss\n",
    "\n",
    "\n",
    "                    adv_loss = -(\n",
    "                        torch.mean(outputs[\"seg_real_disc\"]) +\n",
    "                        torch.mean(outputs[\"depth_real_disc\"]) +\n",
    "                        torch.mean(outputs[\"combined_real_disc\"])\n",
    "                    )\n",
    "\n",
    "                    combined_loss = seg_loss + depth_loss + 0.01 * adv_loss\n",
    "\n",
    "                    # Update validation metrics\n",
    "                    epoch_valid[\"seg\"] += seg_loss.item()\n",
    "                    epoch_valid[\"depth\"] += depth_loss.item()\n",
    "                    epoch_valid[\"combined\"] += combined_loss.item()\n",
    "                    epoch_valid[\"adv\"] += adv_loss.item()\n",
    "                    # epoch_valid[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "                    num_valid_batches += 1\n",
    "\n",
    "        # Average validation metrics\n",
    "        for key in epoch_valid.keys():\n",
    "            valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "        # Save best model\n",
    "        valid_combined_loss = epoch_valid[\"combined\"] / num_valid_batches\n",
    "        if valid_combined_loss < best_combined_loss:\n",
    "            best_combined_loss = valid_combined_loss\n",
    "            # torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "            checkpoint_path = os.path.join(save_dir, \"best_model_checkpoint.pth\")\n",
    "            save_checkpoint(model, opt_sched, checkpoint_path, epoch + 1, best_combined_loss)\n",
    "            print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "        frame = save_training_visualization_as_gif2(epoch, inputs, outputs[\"seg_output\"], outputs[\"depth_output\"], torch.argmax(seg_labels, dim=1), depth_labels)\n",
    "        gif_frames.append(frame)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Append metrics to CSV\n",
    "        with open(csv_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                epoch + 1,\n",
    "                epoch_train[\"seg\"] / num_batches,\n",
    "                epoch_train[\"depth\"] / num_batches,\n",
    "                epoch_train[\"combined\"] / num_batches,\n",
    "                epoch_train[\"adv\"] / num_batches,\n",
    "                # epoch_train[\"iou\"] / num_batches,\n",
    "                epoch_valid[\"seg\"] / num_valid_batches,\n",
    "                epoch_valid[\"depth\"] / num_valid_batches,\n",
    "                epoch_valid[\"combined\"] / num_valid_batches,\n",
    "                epoch_valid[\"adv\"] / num_valid_batches,\n",
    "                # epoch_valid[\"iou\"] / num_valid_batches,\n",
    "            ])\n",
    "            \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Results:\")\n",
    "\n",
    "        # Print training losses\n",
    "        print(f\"  Train Losses - Segmentation: {epoch_train['seg']/num_batches:.4f}, Depth: {epoch_train['depth']/num_batches:.4f}, \"\n",
    "              f\"Combined: {epoch_train['combined']/num_batches:.4f}, Adversarial: {epoch_train['adv']/num_batches:.4f}\")\n",
    "\n",
    "        # Print validation losses\n",
    "        print(f\"  Valid Losses - Segmentation: {epoch_valid['seg']/ num_valid_batches:.4f}, Depth: {epoch_valid['depth']/ num_valid_batches:.4f}, \"\n",
    "              f\"Combined: {epoch_valid['combined']/ num_valid_batches:.4f}, Adversarial: {epoch_valid['adv']/ num_valid_batches:.4f}\")\n",
    "\n",
    "\n",
    "        # Update schedulers\n",
    "        for name, scheduler in opt_sched[\"schedulers\"].items():\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                # Pass the appropriate metric to ReduceLROnPlateau\n",
    "                scheduler.step(valid_losses[\"combined\"][-1])  # Use the most recent validation combined loss\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "        if epoch %10 == 0:\n",
    "            gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "            gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "            plot_all_losses(epoch, train_losses,valid_losses,save_dir)\n",
    "            \n",
    "    \n",
    "    gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "    print(f\"Training visualization saved as GIF at {gif_path}\")\n",
    "    plot_all_losses(epoch, train_losses,valid_losses,save_dir)\n",
    "\n",
    "    \n",
    "    return train_losses, valid_losses, save_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc558fd7-917a-4782-8edf-6f0ec696f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resume_training_with_loss_tracking(\n",
    "    model_class,\n",
    "    model_dir,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_additional_epochs,\n",
    "    device,\n",
    "    opt_sched,\n",
    "    save_dir,\n",
    "):\n",
    "    \"\"\"\n",
    "    Resumes training a multi-task model, appends loss data to the existing CSV file,\n",
    "    and generates graphs for the combined training history.\n",
    "\n",
    "    Args:\n",
    "        model_class: The model class to instantiate.\n",
    "        model_dir: Path to the directory containing the saved model and loss CSV file.\n",
    "        train_loader: DataLoader for training data.\n",
    "        valid_loader: DataLoader for validation data.\n",
    "        num_additional_epochs: Number of additional epochs to train.\n",
    "        device: Device for training (\"cuda\" or \"cpu\").\n",
    "        opt_sched: Dictionary of optimizers and schedulers.\n",
    "        save_dir: Directory to save the updated results.\n",
    "\n",
    "    Returns:\n",
    "        Updated train and validation losses.\n",
    "    \"\"\"\n",
    "    # # Load the best model\n",
    "    # best_model_path = os.path.join(model_dir, \"best_model.pth\")\n",
    "    # if not os.path.exists(best_model_path):\n",
    "    #     raise FileNotFoundError(f\"Best model not found at {best_model_path}\")\n",
    "        \n",
    "    # Load the checkpoint\n",
    "    checkpoint_path = os.path.join(model_dir, \"best_model_checkpoint.pth\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "    \n",
    "        \n",
    "    model = model_class().to(device)\n",
    "    \n",
    "    start_epoch, best_loss = load_checkpoint(model, opt_sched, checkpoint_path, device)\n",
    "\n",
    "    # model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "    # Locate the existing loss CSV\n",
    "    csv_path = os.path.join(model_dir, [file for file in os.listdir(model_dir) if file.endswith(\".csv\")][0])\n",
    "\n",
    "    # Parse existing CSV data\n",
    "    existing_train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    existing_valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    current_epoch = 0\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            current_epoch = int(row[0])\n",
    "            existing_train_losses[\"seg\"].append(float(row[1]))\n",
    "            existing_train_losses[\"depth\"].append(float(row[2]))\n",
    "            existing_train_losses[\"combined\"].append(float(row[3]))\n",
    "            existing_train_losses[\"adv\"].append(float(row[4]))\n",
    "            existing_valid_losses[\"seg\"].append(float(row[5]))\n",
    "            existing_valid_losses[\"depth\"].append(float(row[6]))\n",
    "            existing_valid_losses[\"combined\"].append(float(row[7]))\n",
    "            existing_valid_losses[\"adv\"].append(float(row[8]))\n",
    "\n",
    "    # Train for additional epochs\n",
    "    \n",
    "    train_losses, valid_losses,save_dir2 = train_model_with_adversarial_loss_tracking(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        num_epochs=num_additional_epochs,\n",
    "        device=device,\n",
    "        opt_sched=opt_sched,\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "\n",
    "    # Combine the new losses with the existing ones\n",
    "    for key in existing_train_losses.keys():\n",
    "        existing_train_losses[key].extend(train_losses[key])\n",
    "        existing_valid_losses[key].extend(valid_losses[key])\n",
    "        \n",
    "    save_dir3 = os.path.join(save_dir2,\"combined_result\")\n",
    "    os.makedirs(save_dir3, exist_ok=True)\n",
    "    \n",
    "    total_epochs = len(existing_train_losses[\"seg\"])\n",
    "        \n",
    "    updated_csv_path = os.path.join(save_dir3, f\"loss_tracking_updated_{total_epochs}.csv\")\n",
    "    os.makedirs(save_dir3, exist_ok=True)\n",
    "\n",
    "    # Write combined losses to the updated CSV\n",
    "    with open(updated_csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "            \"train_adv_loss\", \n",
    "            \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "            \"valid_adv_loss\", \n",
    "        ])\n",
    "        for epoch in range(len(existing_train_losses[\"seg\"])):\n",
    "            writer.writerow([\n",
    "                epoch + 1,\n",
    "                existing_train_losses[\"seg\"][epoch],\n",
    "                existing_train_losses[\"depth\"][epoch],\n",
    "                existing_train_losses[\"combined\"][epoch],\n",
    "                existing_train_losses[\"adv\"][epoch],\n",
    "                existing_valid_losses[\"seg\"][epoch],\n",
    "                existing_valid_losses[\"depth\"][epoch],\n",
    "                existing_valid_losses[\"combined\"][epoch],\n",
    "                existing_valid_losses[\"adv\"][epoch],\n",
    "            ])\n",
    "\n",
    "    # Generate graphs\n",
    "    for key in existing_train_losses.keys():\n",
    "        plt.figure()\n",
    "        plt.plot(range(len(existing_train_losses[key])), existing_train_losses[key], label=f\"Train {key.capitalize()}\")\n",
    "        plt.plot(range(len(existing_valid_losses[key])), existing_valid_losses[key], label=f\"Valid {key.capitalize()}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(f\"{key.capitalize()} Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"{key.capitalize()} Loss Over Epochs\")\n",
    "        plt.savefig(os.path.join(save_dir3, f\"{key}_loss_graph_epoch_{total_epochs}.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    output_path = os.path.join(save_dir3,'combined_results.gif')\n",
    "    combine_training_gifs(model_dir, save_dir2, output_path)\n",
    "\n",
    "    return existing_train_losses, existing_valid_losses,save_dir2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7fa7c-bca6-4b10-b38e-e8a489333fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdccde2f-05c7-4332-a489-ee2cf3529147",
   "metadata": {},
   "source": [
    "# For first instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aef8c474-94e3-4753-8718-9aa32bef85a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - Training:   0%|          | 0/371 [09:37<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 1 with combined loss 2.4004\n",
      "Epoch 1/40 Results:\n",
      "  Train Losses - Segmentation: 2.4219, Depth: 0.0579, Combined: 2.4500, Adversarial: -2.9780\n",
      "  Valid Losses - Segmentation: 2.3837, Depth: 0.0466, Combined: 2.4004, Adversarial: -2.9848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 - Training:   0%|          | 0/371 [08:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 2 with combined loss 2.1478\n",
      "Epoch 2/40 Results:\n",
      "  Train Losses - Segmentation: 2.1883, Depth: 0.0444, Combined: 2.2027, Adversarial: -2.9958\n",
      "  Valid Losses - Segmentation: 2.1091, Depth: 0.0686, Combined: 2.1478, Adversarial: -2.9893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 Results:\n",
      "  Train Losses - Segmentation: 2.1075, Depth: 0.0420, Combined: 2.1196, Adversarial: -2.9967\n",
      "  Valid Losses - Segmentation: 2.2348, Depth: 0.0638, Combined: 2.2686, Adversarial: -2.9987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 4 with combined loss 2.0933\n",
      "Epoch 4/40 Results:\n",
      "  Train Losses - Segmentation: 2.0822, Depth: 0.0409, Combined: 2.0932, Adversarial: -2.9972\n",
      "  Valid Losses - Segmentation: 2.0770, Depth: 0.0465, Combined: 2.0933, Adversarial: -3.0239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 5 with combined loss 2.0564\n",
      "Epoch 5/40 Results:\n",
      "  Train Losses - Segmentation: 2.0276, Depth: 0.0376, Combined: 2.0352, Adversarial: -2.9978\n",
      "  Valid Losses - Segmentation: 2.0396, Depth: 0.0468, Combined: 2.0564, Adversarial: -2.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 Results:\n",
      "  Train Losses - Segmentation: 1.9941, Depth: 0.0376, Combined: 2.0017, Adversarial: -2.9980\n",
      "  Valid Losses - Segmentation: 2.1045, Depth: 0.0499, Combined: 2.1242, Adversarial: -3.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 Results:\n",
      "  Train Losses - Segmentation: 1.9874, Depth: 0.0376, Combined: 1.9950, Adversarial: -2.9980\n",
      "  Valid Losses - Segmentation: 2.0622, Depth: 0.0585, Combined: 2.0907, Adversarial: -3.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 Results:\n",
      "  Train Losses - Segmentation: 1.9509, Depth: 0.0359, Combined: 1.9569, Adversarial: -2.9984\n",
      "  Valid Losses - Segmentation: 2.0578, Depth: 0.0521, Combined: 2.0798, Adversarial: -3.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 Results:\n",
      "  Train Losses - Segmentation: 1.9244, Depth: 0.0348, Combined: 1.9292, Adversarial: -2.9985\n",
      "  Valid Losses - Segmentation: 2.1080, Depth: 0.0699, Combined: 2.1480, Adversarial: -2.9941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 10 with combined loss 2.0522\n",
      "Epoch 10/40 Results:\n",
      "  Train Losses - Segmentation: 1.9190, Depth: 0.0340, Combined: 1.9230, Adversarial: -2.9985\n",
      "  Valid Losses - Segmentation: 2.0426, Depth: 0.0397, Combined: 2.0522, Adversarial: -3.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 Results:\n",
      "  Train Losses - Segmentation: 1.8911, Depth: 0.0349, Combined: 1.8960, Adversarial: -2.9987\n",
      "  Valid Losses - Segmentation: 2.0226, Depth: 0.0957, Combined: 2.0882, Adversarial: -3.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 12 with combined loss 2.0495\n",
      "Epoch 12/40 Results:\n",
      "  Train Losses - Segmentation: 1.8991, Depth: 0.0341, Combined: 1.9032, Adversarial: -2.9987\n",
      "  Valid Losses - Segmentation: 2.0182, Depth: 0.0612, Combined: 2.0495, Adversarial: -2.9843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 Results:\n",
      "  Train Losses - Segmentation: 1.8789, Depth: 0.0343, Combined: 1.8832, Adversarial: -2.9989\n",
      "  Valid Losses - Segmentation: 2.1770, Depth: 0.0433, Combined: 2.1905, Adversarial: -2.9839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 - Training:   0%|          | 0/371 [08:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 14 with combined loss 2.0154\n",
      "Epoch 14/40 Results:\n",
      "  Train Losses - Segmentation: 1.8733, Depth: 0.0348, Combined: 1.8781, Adversarial: -2.9989\n",
      "  Valid Losses - Segmentation: 2.0055, Depth: 0.0399, Combined: 2.0154, Adversarial: -2.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 15 with combined loss 1.9091\n",
      "Epoch 15/40 Results:\n",
      "  Train Losses - Segmentation: 1.8466, Depth: 0.0328, Combined: 1.8494, Adversarial: -2.9990\n",
      "  Valid Losses - Segmentation: 1.8968, Depth: 0.0422, Combined: 1.9091, Adversarial: -2.9922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 - Training:   0%|          | 0/371 [08:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 Results:\n",
      "  Train Losses - Segmentation: 1.8445, Depth: 0.0325, Combined: 1.8470, Adversarial: -2.9991\n",
      "  Valid Losses - Segmentation: 1.9076, Depth: 0.0414, Combined: 1.9190, Adversarial: -3.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 Results:\n",
      "  Train Losses - Segmentation: 1.8260, Depth: 0.0322, Combined: 1.8282, Adversarial: -2.9992\n",
      "  Valid Losses - Segmentation: 1.9399, Depth: 0.0510, Combined: 1.9608, Adversarial: -3.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 Results:\n",
      "  Train Losses - Segmentation: 1.8323, Depth: 0.0320, Combined: 1.8343, Adversarial: -2.9993\n",
      "  Valid Losses - Segmentation: 1.9881, Depth: 0.0564, Combined: 2.0144, Adversarial: -2.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 Results:\n",
      "  Train Losses - Segmentation: 1.8182, Depth: 0.0309, Combined: 1.8191, Adversarial: -2.9992\n",
      "  Valid Losses - Segmentation: 1.9535, Depth: 0.0825, Combined: 2.0059, Adversarial: -3.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 Results:\n",
      "  Train Losses - Segmentation: 1.8197, Depth: 0.0315, Combined: 1.8212, Adversarial: -2.9994\n",
      "  Valid Losses - Segmentation: 2.0076, Depth: 0.0464, Combined: 2.0240, Adversarial: -2.9984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 21 with combined loss 1.9031\n",
      "Epoch 21/40 Results:\n",
      "  Train Losses - Segmentation: 1.8005, Depth: 0.0312, Combined: 1.8017, Adversarial: -2.9993\n",
      "  Valid Losses - Segmentation: 1.8859, Depth: 0.0473, Combined: 1.9031, Adversarial: -3.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 - Training:   0%|          | 0/371 [08:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 Results:\n",
      "  Train Losses - Segmentation: 1.7869, Depth: 0.0306, Combined: 1.7875, Adversarial: -2.9995\n",
      "  Valid Losses - Segmentation: 1.9414, Depth: 0.0394, Combined: 1.9506, Adversarial: -3.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 Results:\n",
      "  Train Losses - Segmentation: 1.7969, Depth: 0.0302, Combined: 1.7971, Adversarial: -2.9995\n",
      "  Valid Losses - Segmentation: 1.9068, Depth: 0.0351, Combined: 1.9119, Adversarial: -3.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 - Training:   0%|          | 0/371 [08:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 Results:\n",
      "  Train Losses - Segmentation: 1.7738, Depth: 0.0301, Combined: 1.7740, Adversarial: -2.9995\n",
      "  Valid Losses - Segmentation: 1.8993, Depth: 0.0468, Combined: 1.9161, Adversarial: -2.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 25 with combined loss 1.8873\n",
      "Epoch 25/40 Results:\n",
      "  Train Losses - Segmentation: 1.7873, Depth: 0.0302, Combined: 1.7875, Adversarial: -2.9996\n",
      "  Valid Losses - Segmentation: 1.8715, Depth: 0.0458, Combined: 1.8873, Adversarial: -3.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 Results:\n",
      "  Train Losses - Segmentation: 1.7700, Depth: 0.0294, Combined: 1.7694, Adversarial: -2.9996\n",
      "  Valid Losses - Segmentation: 1.8556, Depth: 0.0677, Combined: 1.8932, Adversarial: -3.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 - Training:   0%|          | 0/371 [08:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 27 with combined loss 1.8724\n",
      "Epoch 27/40 Results:\n",
      "  Train Losses - Segmentation: 1.7521, Depth: 0.0286, Combined: 1.7507, Adversarial: -2.9996\n",
      "  Valid Losses - Segmentation: 1.8617, Depth: 0.0407, Combined: 1.8724, Adversarial: -3.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 Results:\n",
      "  Train Losses - Segmentation: 1.7422, Depth: 0.0293, Combined: 1.7414, Adversarial: -2.9997\n",
      "  Valid Losses - Segmentation: 1.8772, Depth: 0.0406, Combined: 1.8877, Adversarial: -3.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 Results:\n",
      "  Train Losses - Segmentation: 1.7655, Depth: 0.0287, Combined: 1.7642, Adversarial: -2.9997\n",
      "  Valid Losses - Segmentation: 1.8699, Depth: 0.0484, Combined: 1.8884, Adversarial: -2.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 30 with combined loss 1.8357\n",
      "Epoch 30/40 Results:\n",
      "  Train Losses - Segmentation: 1.7476, Depth: 0.0286, Combined: 1.7461, Adversarial: -2.9997\n",
      "  Valid Losses - Segmentation: 1.8267, Depth: 0.0390, Combined: 1.8357, Adversarial: -3.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 - Training:   0%|          | 0/371 [08:13<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 Results:\n",
      "  Train Losses - Segmentation: 1.7423, Depth: 0.0286, Combined: 1.7409, Adversarial: -2.9997\n",
      "  Valid Losses - Segmentation: 1.8337, Depth: 0.0495, Combined: 1.8531, Adversarial: -3.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 - Training:   0%|          | 0/371 [08:14<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 Results:\n",
      "  Train Losses - Segmentation: 1.7374, Depth: 0.0284, Combined: 1.7358, Adversarial: -2.9998\n",
      "  Valid Losses - Segmentation: 1.8474, Depth: 0.0336, Combined: 1.8510, Adversarial: -3.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 Results:\n",
      "  Train Losses - Segmentation: 1.7330, Depth: 0.0284, Combined: 1.7314, Adversarial: -2.9998\n",
      "  Valid Losses - Segmentation: 1.9529, Depth: 0.0483, Combined: 1.9713, Adversarial: -2.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 - Training:   0%|          | 0/371 [08:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 Results:\n",
      "  Train Losses - Segmentation: 1.7091, Depth: 0.0275, Combined: 1.7065, Adversarial: -2.9997\n",
      "  Valid Losses - Segmentation: 1.8489, Depth: 0.0395, Combined: 1.8584, Adversarial: -3.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 - Training:   0%|          | 0/371 [08:06<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 35 with combined loss 1.8156\n",
      "Epoch 35/40 Results:\n",
      "  Train Losses - Segmentation: 1.7201, Depth: 0.0276, Combined: 1.7177, Adversarial: -2.9998\n",
      "  Valid Losses - Segmentation: 1.8088, Depth: 0.0367, Combined: 1.8156, Adversarial: -2.9956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 Results:\n",
      "  Train Losses - Segmentation: 1.7099, Depth: 0.0277, Combined: 1.7076, Adversarial: -2.9998\n",
      "  Valid Losses - Segmentation: 1.8385, Depth: 0.0515, Combined: 1.8601, Adversarial: -2.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 Results:\n",
      "  Train Losses - Segmentation: 1.7058, Depth: 0.0276, Combined: 1.7034, Adversarial: -2.9998\n",
      "  Valid Losses - Segmentation: 1.8251, Depth: 0.0382, Combined: 1.8332, Adversarial: -3.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 - Training:   0%|          | 0/371 [08:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 Results:\n",
      "  Train Losses - Segmentation: 1.6985, Depth: 0.0272, Combined: 1.6957, Adversarial: -2.9998\n",
      "  Valid Losses - Segmentation: 1.8412, Depth: 0.0463, Combined: 1.8575, Adversarial: -3.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 Results:\n",
      "  Train Losses - Segmentation: 1.6937, Depth: 0.0276, Combined: 1.6914, Adversarial: -2.9998\n",
      "  Valid Losses - Segmentation: 1.8581, Depth: 0.0476, Combined: 1.8757, Adversarial: -3.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 Results:\n",
      "  Train Losses - Segmentation: 1.6973, Depth: 0.0270, Combined: 1.6943, Adversarial: -2.9998\n",
      "  Valid Losses - Segmentation: 1.8487, Depth: 0.0364, Combined: 1.8551, Adversarial: -2.9977\n",
      "Training visualization saved as GIF at results_test8_final2/20241129_030455/training_visualization_20241129_030455.gif\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "# Instantiate Models\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 40\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "mobilenet_backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "# encoder = MobileNetV3Backbone(mobilenet_backbone.features)\n",
    "model = MultiTaskModel(backbone=mobilenet_backbone.features, num_seg_classes=20, depth_channels=1)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Initialize optimizers and schedulers\n",
    "opt_sched = initialize_optimizers_and_schedulers(model)\n",
    "\n",
    "# Access optimizers\n",
    "optimizers = opt_sched[\"optimizers\"]\n",
    "schedulers = opt_sched[\"schedulers\"]\n",
    "\n",
    "# Prepare Data Loaders (Ensure train_loader and valid_loader are ready)\n",
    "train_losses, valid_losses,save_dir2 = train_model_with_adversarial_loss_tracking(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    num_epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    opt_sched=opt_sched,\n",
    "    save_dir=\"results_test8_final2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03941dac-8ce7-4c0c-b9c4-1aefdf36a9ee",
   "metadata": {},
   "source": [
    "# For second instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5b697a1-5cdc-4683-aaea-c6c46840e916",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/results_test8_final2/20241129_145526/combined_result'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_save_dir = os.path.join(os.getcwd(),'results_test8_final2')\n",
    "model_dir =  os.path.join(root_save_dir,'20241129_145526')\n",
    "model_dir = os.path.join(model_dir,'combined_result')\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1f08f35-de94-4973-acd6-0ae47637544f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.1280355/ipykernel_1497919/725688548.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "Epoch 1/60 - Training:   0%|          | 0/371 [07:20<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 1 with combined loss 1.8251\n",
      "Epoch 1/60 Results:\n",
      "  Train Losses - Segmentation: 1.7052, Depth: 0.0282, Combined: 1.7034, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8172, Depth: 0.0379, Combined: 1.8251, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/60 - Training:   0%|          | 0/371 [07:16<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/60 Results:\n",
      "  Train Losses - Segmentation: 1.6914, Depth: 0.0280, Combined: 1.6895, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8304, Depth: 0.0365, Combined: 1.8369, Adversarial: -2.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/60 - Training:   0%|          | 0/371 [07:18<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/60 Results:\n",
      "  Train Losses - Segmentation: 1.6896, Depth: 0.0283, Combined: 1.6880, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8311, Depth: 0.0380, Combined: 1.8392, Adversarial: -2.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/60 - Training:   0%|          | 0/371 [07:23<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/60 Results:\n",
      "  Train Losses - Segmentation: 1.6928, Depth: 0.0285, Combined: 1.6914, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8314, Depth: 0.0378, Combined: 1.8392, Adversarial: -2.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/60 - Training:   0%|          | 0/371 [07:20<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 5 with combined loss 1.8211\n",
      "Epoch 5/60 Results:\n",
      "  Train Losses - Segmentation: 1.6947, Depth: 0.0282, Combined: 1.6929, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8132, Depth: 0.0378, Combined: 1.8211, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/60 Results:\n",
      "  Train Losses - Segmentation: 1.6891, Depth: 0.0281, Combined: 1.6873, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8373, Depth: 0.0380, Combined: 1.8453, Adversarial: -2.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/60 - Training:   0%|          | 0/371 [07:14<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 7 with combined loss 1.8161\n",
      "Epoch 7/60 Results:\n",
      "  Train Losses - Segmentation: 1.7043, Depth: 0.0283, Combined: 1.7026, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8082, Depth: 0.0379, Combined: 1.8161, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/60 Results:\n",
      "  Train Losses - Segmentation: 1.6945, Depth: 0.0281, Combined: 1.6926, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8299, Depth: 0.0378, Combined: 1.8377, Adversarial: -2.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/60 - Training:   0%|          | 0/371 [07:14<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/60 Results:\n",
      "  Train Losses - Segmentation: 1.6939, Depth: 0.0283, Combined: 1.6923, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8182, Depth: 0.0389, Combined: 1.8271, Adversarial: -2.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/60 - Training:   0%|          | 0/371 [07:16<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 10 with combined loss 1.8158\n",
      "Epoch 10/60 Results:\n",
      "  Train Losses - Segmentation: 1.7057, Depth: 0.0279, Combined: 1.7037, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8084, Depth: 0.0373, Combined: 1.8158, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/60 Results:\n",
      "  Train Losses - Segmentation: 1.6983, Depth: 0.0286, Combined: 1.6970, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8115, Depth: 0.0372, Combined: 1.8188, Adversarial: -2.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 12 with combined loss 1.8099\n",
      "Epoch 12/60 Results:\n",
      "  Train Losses - Segmentation: 1.6894, Depth: 0.0281, Combined: 1.6876, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8027, Depth: 0.0371, Combined: 1.8099, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/60 Results:\n",
      "  Train Losses - Segmentation: 1.6858, Depth: 0.0279, Combined: 1.6838, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8238, Depth: 0.0383, Combined: 1.8322, Adversarial: -2.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/60 - Training:   0%|          | 0/371 [07:18<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/60 Results:\n",
      "  Train Losses - Segmentation: 1.7090, Depth: 0.0285, Combined: 1.7075, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8250, Depth: 0.0373, Combined: 1.8324, Adversarial: -2.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/60 - Training:   0%|          | 0/371 [07:16<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/60 Results:\n",
      "  Train Losses - Segmentation: 1.6954, Depth: 0.0283, Combined: 1.6937, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8210, Depth: 0.0387, Combined: 1.8297, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/60 - Training:   0%|          | 0/371 [07:13<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/60 Results:\n",
      "  Train Losses - Segmentation: 1.6855, Depth: 0.0285, Combined: 1.6840, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8074, Depth: 0.0381, Combined: 1.8156, Adversarial: -2.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/60 - Training:   0%|          | 0/371 [07:13<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/60 Results:\n",
      "  Train Losses - Segmentation: 1.6916, Depth: 0.0282, Combined: 1.6899, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8316, Depth: 0.0378, Combined: 1.8394, Adversarial: -2.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/60 - Training:   0%|          | 0/371 [07:13<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/60 Results:\n",
      "  Train Losses - Segmentation: 1.6856, Depth: 0.0281, Combined: 1.6837, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8327, Depth: 0.0390, Combined: 1.8417, Adversarial: -2.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/60 - Training:   0%|          | 0/371 [07:13<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/60 Results:\n",
      "  Train Losses - Segmentation: 1.6988, Depth: 0.0282, Combined: 1.6971, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8087, Depth: 0.0383, Combined: 1.8171, Adversarial: -2.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/60 - Training:   0%|          | 0/371 [07:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/60 Results:\n",
      "  Train Losses - Segmentation: 1.6874, Depth: 0.0280, Combined: 1.6855, Adversarial: -2.9947\n",
      "  Valid Losses - Segmentation: 1.8098, Depth: 0.0385, Combined: 1.8184, Adversarial: -2.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/60 - Training:   0%|          | 0/371 [07:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/60 Results:\n",
      "  Train Losses - Segmentation: 1.6815, Depth: 0.0283, Combined: 1.6798, Adversarial: -2.9947\n",
      "  Valid Losses - Segmentation: 1.8188, Depth: 0.0388, Combined: 1.8276, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/60 - Training:   0%|          | 0/371 [07:06<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/60 Results:\n",
      "  Train Losses - Segmentation: 1.6947, Depth: 0.0285, Combined: 1.6933, Adversarial: -2.9952\n",
      "  Valid Losses - Segmentation: 1.8380, Depth: 0.0380, Combined: 1.8460, Adversarial: -2.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/60 - Training:   0%|          | 0/371 [07:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/60 Results:\n",
      "  Train Losses - Segmentation: 1.6873, Depth: 0.0276, Combined: 1.6849, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8213, Depth: 0.0388, Combined: 1.8301, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/60 - Training:   0%|          | 0/371 [07:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/60 Results:\n",
      "  Train Losses - Segmentation: 1.6919, Depth: 0.0282, Combined: 1.6902, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8160, Depth: 0.0382, Combined: 1.8242, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/60 - Training:   0%|          | 0/371 [07:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/60 Results:\n",
      "  Train Losses - Segmentation: 1.7011, Depth: 0.0280, Combined: 1.6991, Adversarial: -2.9951\n",
      "  Valid Losses - Segmentation: 1.8149, Depth: 0.0374, Combined: 1.8223, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/60 - Training:   0%|          | 0/371 [07:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/60 Results:\n",
      "  Train Losses - Segmentation: 1.6856, Depth: 0.0283, Combined: 1.6839, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8211, Depth: 0.0380, Combined: 1.8292, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/60 - Training:   0%|          | 0/371 [07:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/60 Results:\n",
      "  Train Losses - Segmentation: 1.6902, Depth: 0.0280, Combined: 1.6882, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8087, Depth: 0.0386, Combined: 1.8174, Adversarial: -2.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/60 - Training:   0%|          | 0/371 [07:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/60 Results:\n",
      "  Train Losses - Segmentation: 1.6876, Depth: 0.0284, Combined: 1.6860, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8224, Depth: 0.0389, Combined: 1.8313, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/60 - Training:   0%|          | 0/371 [07:18<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/60 Results:\n",
      "  Train Losses - Segmentation: 1.6923, Depth: 0.0280, Combined: 1.6904, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8179, Depth: 0.0383, Combined: 1.8262, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/60 - Training:   0%|          | 0/371 [07:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/60 Results:\n",
      "  Train Losses - Segmentation: 1.6821, Depth: 0.0277, Combined: 1.6798, Adversarial: -2.9951\n",
      "  Valid Losses - Segmentation: 1.8176, Depth: 0.0378, Combined: 1.8254, Adversarial: -2.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/60 - Training:   0%|          | 0/371 [07:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/60 Results:\n",
      "  Train Losses - Segmentation: 1.6762, Depth: 0.0281, Combined: 1.6744, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8109, Depth: 0.0374, Combined: 1.8183, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/60 - Training:   0%|          | 0/371 [07:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/60 Results:\n",
      "  Train Losses - Segmentation: 1.6981, Depth: 0.0281, Combined: 1.6963, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8130, Depth: 0.0385, Combined: 1.8215, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/60 - Training:   0%|          | 0/371 [07:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/60 Results:\n",
      "  Train Losses - Segmentation: 1.6866, Depth: 0.0278, Combined: 1.6844, Adversarial: -2.9951\n",
      "  Valid Losses - Segmentation: 1.8139, Depth: 0.0378, Combined: 1.8217, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/60 Results:\n",
      "  Train Losses - Segmentation: 1.7002, Depth: 0.0282, Combined: 1.6985, Adversarial: -2.9946\n",
      "  Valid Losses - Segmentation: 1.8115, Depth: 0.0377, Combined: 1.8192, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/60 - Training:   0%|          | 0/371 [07:16<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/60 Results:\n",
      "  Train Losses - Segmentation: 1.7054, Depth: 0.0284, Combined: 1.7039, Adversarial: -2.9947\n",
      "  Valid Losses - Segmentation: 1.8132, Depth: 0.0380, Combined: 1.8212, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/60 Results:\n",
      "  Train Losses - Segmentation: 1.6911, Depth: 0.0280, Combined: 1.6892, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8169, Depth: 0.0378, Combined: 1.8247, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/60 - Training:   0%|          | 0/371 [07:18<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/60 Results:\n",
      "  Train Losses - Segmentation: 1.6843, Depth: 0.0282, Combined: 1.6826, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8204, Depth: 0.0374, Combined: 1.8278, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/60 Results:\n",
      "  Train Losses - Segmentation: 1.6901, Depth: 0.0282, Combined: 1.6884, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8102, Depth: 0.0373, Combined: 1.8175, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/60 - Training:   0%|          | 0/371 [07:16<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/60 Results:\n",
      "  Train Losses - Segmentation: 1.6810, Depth: 0.0278, Combined: 1.6788, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8132, Depth: 0.0379, Combined: 1.8211, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/60 - Training:   0%|          | 0/371 [07:19<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/60 Results:\n",
      "  Train Losses - Segmentation: 1.7059, Depth: 0.0285, Combined: 1.7044, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8158, Depth: 0.0374, Combined: 1.8233, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/60 - Training:   0%|          | 0/371 [07:17<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/60 Results:\n",
      "  Train Losses - Segmentation: 1.6956, Depth: 0.0281, Combined: 1.6938, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8061, Depth: 0.0379, Combined: 1.8140, Adversarial: -2.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/60 Results:\n",
      "  Train Losses - Segmentation: 1.7153, Depth: 0.0282, Combined: 1.7136, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8158, Depth: 0.0377, Combined: 1.8235, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/60 - Training:   0%|          | 0/371 [07:28<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/60 Results:\n",
      "  Train Losses - Segmentation: 1.6899, Depth: 0.0280, Combined: 1.6879, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8179, Depth: 0.0380, Combined: 1.8259, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/60 - Training:   0%|          | 0/371 [07:19<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/60 Results:\n",
      "  Train Losses - Segmentation: 1.6826, Depth: 0.0282, Combined: 1.6808, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8149, Depth: 0.0379, Combined: 1.8229, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/60 - Training:   0%|          | 0/371 [07:16<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/60 Results:\n",
      "  Train Losses - Segmentation: 1.6970, Depth: 0.0279, Combined: 1.6949, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8094, Depth: 0.0376, Combined: 1.8171, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/60 - Training:   0%|          | 0/371 [07:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/60 Results:\n",
      "  Train Losses - Segmentation: 1.6922, Depth: 0.0283, Combined: 1.6905, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8151, Depth: 0.0379, Combined: 1.8231, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/60 - Training:   0%|          | 0/371 [07:13<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/60 Results:\n",
      "  Train Losses - Segmentation: 1.6871, Depth: 0.0281, Combined: 1.6852, Adversarial: -2.9952\n",
      "  Valid Losses - Segmentation: 1.8092, Depth: 0.0373, Combined: 1.8165, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/60 - Training:   0%|          | 0/371 [07:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/60 Results:\n",
      "  Train Losses - Segmentation: 1.6940, Depth: 0.0280, Combined: 1.6920, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8122, Depth: 0.0377, Combined: 1.8200, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/60 - Training:   0%|          | 0/371 [07:10<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/60 Results:\n",
      "  Train Losses - Segmentation: 1.6892, Depth: 0.0286, Combined: 1.6878, Adversarial: -2.9951\n",
      "  Valid Losses - Segmentation: 1.8110, Depth: 0.0376, Combined: 1.8186, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/60 - Training:   0%|          | 0/371 [07:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/60 Results:\n",
      "  Train Losses - Segmentation: 1.6966, Depth: 0.0284, Combined: 1.6951, Adversarial: -2.9946\n",
      "  Valid Losses - Segmentation: 1.8104, Depth: 0.0375, Combined: 1.8180, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/60 - Training:   0%|          | 0/371 [07:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/60 Results:\n",
      "  Train Losses - Segmentation: 1.6829, Depth: 0.0285, Combined: 1.6814, Adversarial: -2.9951\n",
      "  Valid Losses - Segmentation: 1.8170, Depth: 0.0376, Combined: 1.8246, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/60 - Training:   0%|          | 0/371 [07:13<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/60 Results:\n",
      "  Train Losses - Segmentation: 1.6748, Depth: 0.0281, Combined: 1.6730, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8122, Depth: 0.0379, Combined: 1.8202, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/60 - Training:   0%|          | 0/371 [07:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/60 Results:\n",
      "  Train Losses - Segmentation: 1.6889, Depth: 0.0285, Combined: 1.6874, Adversarial: -2.9948\n",
      "  Valid Losses - Segmentation: 1.8130, Depth: 0.0376, Combined: 1.8206, Adversarial: -2.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/60 - Training:   0%|          | 0/371 [07:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/60 Results:\n",
      "  Train Losses - Segmentation: 1.6893, Depth: 0.0280, Combined: 1.6874, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8115, Depth: 0.0379, Combined: 1.8194, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/60 - Training:   0%|          | 0/371 [07:12<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/60 Results:\n",
      "  Train Losses - Segmentation: 1.6719, Depth: 0.0285, Combined: 1.6704, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8162, Depth: 0.0380, Combined: 1.8243, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/60 Results:\n",
      "  Train Losses - Segmentation: 1.6844, Depth: 0.0281, Combined: 1.6826, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8159, Depth: 0.0379, Combined: 1.8238, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/60 - Training:   0%|          | 0/371 [07:14<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/60 Results:\n",
      "  Train Losses - Segmentation: 1.6914, Depth: 0.0281, Combined: 1.6895, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8150, Depth: 0.0376, Combined: 1.8226, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/60 - Training:   0%|          | 0/371 [07:13<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/60 Results:\n",
      "  Train Losses - Segmentation: 1.6949, Depth: 0.0278, Combined: 1.6927, Adversarial: -2.9950\n",
      "  Valid Losses - Segmentation: 1.8182, Depth: 0.0379, Combined: 1.8261, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/60 - Training:   0%|          | 0/371 [07:15<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/60 Results:\n",
      "  Train Losses - Segmentation: 1.6877, Depth: 0.0278, Combined: 1.6856, Adversarial: -2.9949\n",
      "  Valid Losses - Segmentation: 1.8140, Depth: 0.0378, Combined: 1.8218, Adversarial: -2.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/60 - Training:   0%|          | 0/371 [07:13<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60 Results:\n",
      "  Train Losses - Segmentation: 1.6838, Depth: 0.0280, Combined: 1.6818, Adversarial: -2.9951\n",
      "  Valid Losses - Segmentation: 1.8186, Depth: 0.0382, Combined: 1.8269, Adversarial: -2.9961\n",
      "Training visualization saved as GIF at results_test8_final2/20241130_195345/training_visualization_20241130_195345.gif\n",
      "Combined GIF saved to results_test8_final2/20241130_195345/combined_result/combined_results.gif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/results_test8_final2',\n",
       " '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/results_test8_final2/20241129_145526/combined_result')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root_save_dir = os.path.join(os.getcwd(),'results_test8_final2')\n",
    "# model_dir =  os.path.join(root_save_dir,'20241129_030455')\n",
    "num_additional_epochs = 60  # Number of epochs to continue training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the model class and loaders\n",
    "mobilenet_backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model_class = lambda: MultiTaskModel(backbone=mobilenet_backbone.features, num_seg_classes=20, depth_channels=1)\n",
    "\n",
    "# Initialize optimizers and schedulers\n",
    "model = model_class()\n",
    "opt_sched = initialize_optimizers_and_schedulers(model)\n",
    "\n",
    "\n",
    "# Call the resume training function\n",
    "resume_training_with_loss_tracking(\n",
    "    model_class=model_class,\n",
    "    model_dir=model_dir,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    num_additional_epochs=num_additional_epochs,\n",
    "    device=device,\n",
    "    opt_sched=opt_sched,\n",
    "    save_dir=\"results_test8_final2\"\n",
    ")\n",
    "\n",
    "root_save_dir,model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7a8fa-2148-4685-b4cd-9d6222949210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input(\"Enter timestamp folder value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8dd9190-d327-476b-a992-0f3d1a86f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f871a15-59f2-4357-a525-263a4012dff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78220e94-129e-457c-8865-ecfa98445b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ed5e4-2134-4aee-8a93-4aa65cb10b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64331738-e822-49ee-a26a-71b5a36bd9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cfffa11-a02d-4156-b088-88a3f4830818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined GIF saved to /home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/results_test8_final/20241129_024547/combined_results.gif\n"
     ]
    }
   ],
   "source": [
    "# root = os.path.join(os.getcwd(), 'results_test8_final' )\n",
    "# model_dir = os.path.join(root,'20241129_015944')\n",
    "# save_dir2 = os.path.join(root,'20241129_024547')\n",
    "\n",
    "# output_path = os.path.join(save_dir2,'combined_results.gif')\n",
    "# combine_training_gifs(model_dir, save_dir2, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b24c6-0d86-4c55-9e9f-51782eb1b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with perpetual loss later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec784a8-16ae-4ce3-bfee-c676246a3c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# from datetime import datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.utils import save_image\n",
    "\n",
    "# def train_model_with_loss_tracking(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, opt_sched, save_dir=\"results\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Trains a multi-task model with Conditional GANs, structural consistency, and perceptual loss.\n",
    "\n",
    "#     Args:\n",
    "#         model: The multi-task model to train.\n",
    "#         train_loader: DataLoader for training data.\n",
    "#         valid_loader: DataLoader for validation data.\n",
    "#         num_epochs: Number of epochs to train.\n",
    "#         device: Device for training (\"cuda\" or \"cpu\").\n",
    "#         opt_sched: Dictionary of optimizers and schedulers.\n",
    "#         save_dir: Directory to save results.\n",
    "\n",
    "#     Returns:\n",
    "#         train_losses, valid_losses: Lists of losses for training and validation.\n",
    "#     \"\"\"\n",
    "#     # Create directories for saving results\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     # Prepare CSV file\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_smooth\", \"train_seg_iou\", \"train_seg_perceptual_loss\",\n",
    "#             \"train_depth_perceptual_loss\", \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_smooth\", \"valid_seg_iou\", \"valid_seg_perceptual_loss\",\n",
    "#             \"valid_depth_perceptual_loss\"\n",
    "#         ])\n",
    "\n",
    "#     # Initialize tracking variables\n",
    "#     train_losses = {\n",
    "#         \"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": [],\n",
    "#         \"seg_perceptual\": [], \"depth_perceptual\": []\n",
    "#     }\n",
    "#     valid_losses = {\n",
    "#         \"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": [],\n",
    "#         \"seg_perceptual\": [], \"depth_perceptual\": []\n",
    "#     }\n",
    "#     best_combined_loss = float(\"inf\")\n",
    "#     gif_frames = []\n",
    "\n",
    "#     # Perceptual Loss (example using VGG features)\n",
    "#     perceptual_loss_fn = PerceptualLoss(pretrained_model=\"vgg16\").to(device)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = (\n",
    "#                 batch[\"left\"].to(device),\n",
    "#                 batch[\"mask\"].to(device),\n",
    "#                 batch[\"depth\"].to(device)\n",
    "#             )\n",
    "#             latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#             # Zero gradients\n",
    "#             for optimizer in opt_sched[\"optimizers\"].values():\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(inputs, input_size=inputs.size()[-2:])\n",
    "\n",
    "#             # Loss calculations\n",
    "#             seg_loss_task = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "#             seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "#             seg_loss = seg_loss_task + 0.1 * seg_perceptual_loss\n",
    "\n",
    "#             depth_loss_sidl = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#             depth_loss_huber = inv_huber_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#             depth_loss_smooth = depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "#             depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "#             depth_loss = depth_loss_sidl + depth_loss_huber + depth_loss_smooth + 0.1 * depth_perceptual_loss\n",
    "\n",
    "#             combined_loss = seg_loss + depth_loss\n",
    "\n",
    "#             # Backpropagation\n",
    "#             combined_loss.backward()\n",
    "#             for optimizer in opt_sched[\"optimizers\"].values():\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             # Update training metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += depth_loss.item()\n",
    "#             epoch_train[\"combined\"] += combined_loss.item()\n",
    "#             epoch_train[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_loss_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_loss_smooth.item()\n",
    "#             epoch_train[\"seg_perceptual\"] += seg_perceptual_loss.item()\n",
    "#             epoch_train[\"depth_perceptual\"] += depth_perceptual_loss.item()\n",
    "#             num_batches += 1\n",
    "\n",
    "#         # Average training metrics\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         # Validation loop\n",
    "#         model.eval()\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 inputs, seg_labels, depth_labels = (\n",
    "#                     batch[\"left\"].to(device),\n",
    "#                     batch[\"mask\"].to(device),\n",
    "#                     batch[\"depth\"].to(device)\n",
    "#                 )\n",
    "#                 latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(inputs, input_size=inputs.size()[-2:])\n",
    "\n",
    "#                 # Validation loss calculations\n",
    "#                 seg_loss_task = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "#                 seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "#                 seg_loss = seg_loss_task + 0.1 * seg_perceptual_loss\n",
    "\n",
    "#                 depth_loss_sidl = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#                 depth_loss_huber = inv_huber_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#                 depth_loss_smooth = depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "#                 depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "#                 depth_loss = depth_loss_sidl + depth_loss_huber + depth_loss_smooth + 0.1 * depth_perceptual_loss\n",
    "\n",
    "#                 combined_loss = seg_loss + depth_loss\n",
    "\n",
    "#                 # Update validation metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += depth_loss.item()\n",
    "#                 epoch_valid[\"combined\"] += combined_loss.item()\n",
    "#                 epoch_valid[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_loss_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_loss_smooth.item()\n",
    "#                 epoch_valid[\"seg_perceptual\"] += seg_perceptual_loss.item()\n",
    "#                 epoch_valid[\"depth_perceptual\"] += depth_perceptual_loss.item()\n",
    "#                 num_valid_batches += 1\n",
    "\n",
    "#         # Average validation metrics\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "#         # Save best model\n",
    "#         valid_combined_loss = epoch_valid[\"combined\"] / num_valid_batches\n",
    "#         if valid_combined_loss < best_combined_loss:\n",
    "#             best_combined_loss = valid_combined_loss\n",
    "#             torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "\n",
    "#         # Append metrics to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 epoch_train[\"seg\"] / num_batches,\n",
    "#                 epoch_train[\"depth\"] / num_batches,\n",
    "#                 epoch_train[\"combined\"] / num_batches,\n",
    "#                 epoch_train[\"depth_sidl\"] / num_batches,\n",
    "#                 epoch_train[\"depth_smooth\"] / num_batches,\n",
    "#                 epoch_train[\"iou\"] / num_batches,\n",
    "#                 epoch_train[\"seg_perceptual\"] / num_batches,\n",
    "#                 epoch_train[\"depth_perceptual\"] / num_batches,\n",
    "#                 epoch_valid[\"seg\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"combined\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_sidl\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_smooth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"iou\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"seg_perceptual\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_perceptual\"] / num_valid_batches,\n",
    "#             ])\n",
    "\n",
    "#         # Update schedulers\n",
    "#         for scheduler in opt_sched[\"schedulers\"].values():\n",
    "#             scheduler.step()\n",
    "            \n",
    "#     plot_all_losses(train_losses,valid_losses)\n",
    "\n",
    "    \n",
    "\n",
    "#     return train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1f745-2964-4cc1-8211-49fcf03d216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from datetime import datetime\n",
    "# from torchvision.utils import save_image\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Updated Train Function\n",
    "# def train_model_with_loss_tracking_and_gif(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, save_dir=\"training_output_bicycle_and_pix2pix\"\n",
    "# ):\n",
    "#     # Create directories for saving models and outputs\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "\n",
    "#     # Initialize CSV for saving loss data\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_smooth\", \"train_seg_iou\",\n",
    "#             \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_smooth\", \"valid_seg_iou\"\n",
    "#         ])\n",
    "\n",
    "#     best_combined_loss = float(\"inf\")  # Initialize best combined loss for saving the best model\n",
    "#     train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     gif_frames = []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         model.train()\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         # Training Loop\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "#             latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#             model.optimizer_stage1.zero_grad()\n",
    "#             model.optimizer_stage2.zero_grad()\n",
    "\n",
    "#             # Forward Pass\n",
    "#             outputs = model(inputs, seg_labels, depth_labels, latent_noise)\n",
    "#             seg_output = outputs[\"seg_output\"]\n",
    "#             depth_output = outputs[\"depth_output\"]\n",
    "\n",
    "#             # Loss Calculations\n",
    "#             seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels.squeeze(1))\n",
    "#             seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#             seg_iou = mean_iou(seg_output, seg_labels, num_classes=20)\n",
    "#             seg_loss_total = 0.6 * seg_loss + 0.4 * seg_dice\n",
    "\n",
    "#             depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#             depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#             depth_loss_total = depth_sidl + depth_smooth\n",
    "\n",
    "#             # Combined Loss\n",
    "#             total_loss = seg_loss_total + depth_loss_total\n",
    "#             total_loss.backward()\n",
    "\n",
    "#             # Optimizers Step\n",
    "#             model.optimizer_stage1.step()\n",
    "#             model.optimizer_stage2.step()\n",
    "\n",
    "#             # Accumulate Training Metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += depth_loss_total.item()\n",
    "#             epoch_train[\"combined\"] += total_loss.item()\n",
    "#             epoch_train[\"iou\"] += seg_iou.item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_smooth.item()\n",
    "#             num_batches += 1\n",
    "\n",
    "#             # Save training images for visualization\n",
    "#             if num_batches % 10 == 0:\n",
    "#                 img_grid = torch.cat([inputs[0], seg_output[0].argmax(0, keepdim=True), depth_output[0]], dim=2)\n",
    "#                 save_image(img_grid, os.path.join(save_dir, f\"train_{epoch}_{num_batches}.png\"))\n",
    "#                 gif_frames.append(Image.open(os.path.join(save_dir, f\"train_{epoch}_{num_batches}.png\")))\n",
    "\n",
    "#         model.scheduler_stage1.step()\n",
    "#         model.scheduler_stage2.step(epoch_train[\"combined\"] / num_batches)\n",
    "\n",
    "#         # Average Training Losses\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "#                 latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#                 outputs = model(inputs, seg_labels, depth_labels, latent_noise)\n",
    "#                 seg_output = outputs[\"seg_output\"]\n",
    "#                 depth_output = outputs[\"depth_output\"]\n",
    "\n",
    "#                 # Validation Loss Calculations\n",
    "#                 seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels.squeeze(1))\n",
    "#                 seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#                 seg_iou = mean_iou(seg_output, seg_labels, num_classes=20)\n",
    "#                 seg_loss_total = 0.6 * seg_loss + 0.4 * seg_dice\n",
    "\n",
    "#                 depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#                 depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#                 depth_loss_total = depth_sidl + depth_smooth\n",
    "\n",
    "#                 # Accumulate Validation Metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += depth_loss_total.item()\n",
    "#                 epoch_valid[\"combined\"] += (seg_loss_total + depth_loss_total).item()\n",
    "#                 epoch_valid[\"iou\"] += seg_iou.item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_smooth.item()\n",
    "#                 num_valid_batches += 1\n",
    "            \n",
    "#             frame = save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels)\n",
    "#             gif_frames.append(frame)\n",
    "              \n",
    "\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "#         # Save Model if Validation Loss Improves\n",
    "#         valid_combined_loss = epoch_valid[\"combined\"] / num_valid_batches\n",
    "#         if valid_combined_loss < best_combined_loss:\n",
    "#             best_combined_loss = valid_combined_loss\n",
    "#             torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "#             print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "#         if epoch%10==0:\n",
    "#             gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "#             gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "            \n",
    "\n",
    "#         # Append Validation Metrics to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 epoch_train[\"seg\"] / num_batches,\n",
    "#                 epoch_train[\"depth\"] / num_batches,\n",
    "#                 epoch_train[\"combined\"] / num_batches,\n",
    "#                 epoch_train[\"depth_sidl\"] / num_batches,\n",
    "#                 epoch_train[\"depth_smooth\"] / num_batches,\n",
    "#                 epoch_train[\"iou\"] / num_batches,\n",
    "#                 epoch_valid[\"seg\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"combined\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_sidl\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_smooth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"iou\"] / num_valid_batches,\n",
    "#             ])\n",
    "\n",
    "#     # Save GIF\n",
    "#     gif_frames[0].save(\n",
    "#         gif_path, save_all=True, append_images=gif_frames[1:], duration=200, loop=0\n",
    "#     )\n",
    "\n",
    "#     return train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf71573-fe0d-4309-beb5-7e12adf7cbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c01b5-8405-467c-ba4f-731f05cef85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce4c3f7b-a9d8-45e3-8ca2-288c1734d22f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def train_model_with_loss_tracking_and_gif(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, save_dir=\"training_output_bicycle_and_pix2pix\"):\n",
    "#     # Create directory for saving models and outputs\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "\n",
    "#     # Initialize CSV for saving loss data\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_inv_huber\", \"train_depth_contrastive\", \"train_depth_smooth\",\n",
    "#             \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_inv_huber\", \"valid_depth_contrastive\", \"valid_depth_smooth\"\n",
    "#         ])\n",
    "\n",
    "#     best_combined_loss = float(\"inf\")  # Initialize best combined loss for saving the best model\n",
    "\n",
    "#     train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     # , \"depth_inv_huber\": [], \"depth_contrastive\": []\n",
    "\n",
    "#     gif_frames = []\n",
    "#     num_classes = 20\n",
    "#     # Optimizer for latent noise\n",
    "    \n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#         model.train()\n",
    "#         # epoch_train_seg_loss = 0\n",
    "#         # epoch_train_depth_loss = 0\n",
    "#         # epoch_train_iou = 0\n",
    "#         # epoch_train_combined_loss = 0\n",
    "#         # epoch_train_depth_sidl = 0\n",
    "#         # epoch_train_depth_inv_huber = 0\n",
    "#         # epoch_train_depth_contrastive = 0\n",
    "#         # epoch_train_depth_smooth = 0\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         reconstruction_layer = nn.Conv2d(256, 3, kernel_size=1).to(device)\n",
    "        \n",
    "#         # scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#         # Training Loop\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "#             print(inputs.shape,seg_labels.shape,depth_labels.shape) # torch.Size([8, 3, 200, 512]) torch.Size([8, 1, 200, 512]) torch.Size([8, 1, 1, 200, 512])\n",
    "#             return\n",
    "        \n",
    "                       \n",
    "\n",
    "\n",
    "#             # Forward pass\n",
    "#             seg_output, depth_output, backbone_features = model(...)\n",
    "            \n",
    "\n",
    "\n",
    "#             seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#             seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#             seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#             seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "            \n",
    "#             depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#             depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#             depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#             depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "            \n",
    "           \n",
    "           \n",
    "#             # Combined Loss\n",
    "#             total_loss = bicycle_loss + pix2pix_total_loss\n",
    "\n",
    "#             # Single backward pass\n",
    "#             total_loss.backward()\n",
    "\n",
    "#             # Update both optimizers\n",
    "#             model.optimizer_stage1.step()\n",
    "#             model.optimizer_stage2.step()\n",
    "#             latent_optimizer.step()\n",
    "\n",
    "#             # Accumulate Training Metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#             epoch_train[\"combined\"] += total_loss.item()\n",
    "#             epoch_train[\"iou\"] += seg_iou.item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_smooth.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#         model.scheduler_stage1.step()\n",
    "#         model.scheduler_stage2.step(epoch_train[\"combined\"]/num_batches)\n",
    "\n",
    "\n",
    "#         # Average Training Losses\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Train Seg Loss: {epoch_train['seg']:.4f}, \"\n",
    "#             f\"Train Depth Loss: {epoch_train['depth']:.4f}, Train Combined Loss: {epoch_train['combined']:.4f}, \"\n",
    "#             f\"Train mIOU: {epoch_train['iou']:.4f}, Train sidl Loss: {epoch_train['depth_sidl']:.4f}, \"\n",
    "#             f\"Train depth smooth: {epoch_train['depth_smooth']:.4f}\"\n",
    "#     )       \n",
    "\n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 # print(\"inside valid\")\n",
    "#                 inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "\n",
    "#                 # Ensure depth_labels and segmentation labels have correct dimensions\n",
    "                \n",
    "\n",
    "               \n",
    "\n",
    "\n",
    "#                 seg_output_old =seg_output\n",
    "#                 # Resize seg_output to match the spatial dimensions of seg_labels\n",
    "#                 seg_output_resized = F.interpolate(seg_output, size=seg_labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "#                 seg_output = seg_output_resized\n",
    "\n",
    "#                 depth_output_old = depth_output\n",
    "#                 depth_output_resized = F.interpolate(depth_output, size=depth_labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "#                 depth_output =depth_output_resized\n",
    "\n",
    "\n",
    "#                 # Segmentation Loss\n",
    "#                 seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#                 seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#                 seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#                 seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "                \n",
    "#                 depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#                 depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#                 depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#                 depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "\n",
    "#                 pix2pix_loss = seg_loss_total + depth_loss_total\n",
    "\n",
    "#                 # Combined Validation Loss\n",
    "#                 combined_loss = pix2pix_loss\n",
    "\n",
    "#                 # Accumulate Validation Metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#                 epoch_valid[\"combined\"] += combined_loss.item()\n",
    "#                 epoch_valid[\"iou\"] += seg_iou.item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_smooth.item()\n",
    "                \n",
    "#                 num_valid_batches += 1\n",
    "                \n",
    "#             frame = save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels)\n",
    "#             gif_frames.append(frame)\n",
    "                \n",
    "                \n",
    "#         # Calculate epoch averages\n",
    "#         # Average Validation Losses\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Valid Seg Loss: {epoch_valid['seg']:.4f}, \"\n",
    "#             f\"Valid Depth Loss: {epoch_valid['depth']:.4f}, Valid Combined Loss: {epoch_valid['combined']:.4f}, \"\n",
    "#             f\"Valid mIOU: {epoch_valid['iou']:.4f}, Valid sidl Loss: {epoch_valid['depth_sidl']:.4f}, \"\n",
    "#             f\"Valid depth smooth: {epoch_valid['depth_smooth']:.4f}\"\n",
    "#         )\n",
    "\n",
    "#         # Write the losses to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 train_losses[\"seg\"], train_losses[\"depth\"], train_losses[\"combined\"],\n",
    "#                 train_losses[\"depth_sidl\"], 0,0,\n",
    "#                 # avg_train_depth_inv_huber, avg_train_depth_contrastive,\n",
    "#                 train_losses[\"depth_smooth\"],\n",
    "#                 valid_losses[\"seg\"], valid_losses[\"depth\"], valid_losses['combined'],\n",
    "#                 valid_losses[\"depth_sidl\"],0,0,\n",
    "#                 # avg_valid_depth_inv_huber, avg_valid_depth_contrastive, \n",
    "#                 valid_losses[\"depth_smooth\"]\n",
    "#             ])\n",
    "\n",
    "       \n",
    "#         # Save best model\n",
    "#         if valid_losses[\"combined\"][-1] < best_combined_loss:\n",
    "#             best_combined_loss = valid_losses[\"combined\"][-1]\n",
    "#             torch.save(model, os.path.join(save_dir, \"best_model_resnetBackbone.pth\"))\n",
    "#             print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "#         if epoch%10==0:\n",
    "#             gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "#             gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "    \n",
    "    \n",
    "#     plot_loss(train_losses, valid_losses, save_dir)\n",
    "#     gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return train_losses,valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28f06b7e-8a7c-40d1-9ff2-d5bc2219bc74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Create your model instance\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = 'cpu'\n",
    "# model = MultiTaskModel(num_seg_classes=20, feature_channels=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6a0e2e4-241b-4096-ab67-41abda0c3a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Set the number of epochs\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Call the training function\n",
    "# train_losses, valid_losses = train_model_with_loss_tracking_and_gif(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     valid_loader=valid_loader,\n",
    "#     num_epochs=num_epochs,\n",
    "#     device=device,\n",
    "#     save_dir=\"test7_res\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c51a6d-a75d-429c-aef9-8e547a5fe8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789b4aa-a31c-4298-ae2a-c8c13c30da39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d70c33-2426-4327-9b66-208e8c419e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75f3e6-0e5b-4748-a1d0-718357fc8092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3db3d8-d07c-4a04-8615-1f6a59f5d1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e599ee-b582-4b1f-aa67-3b756e18c23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c1039-9a3d-434c-8585-ded47ce50261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720176f-f280-452e-8809-5274dc540273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876589d-4937-4435-ac84-cc235e43a99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64d7f8b2-0558-4745-9647-e6c65811861a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def train_model_with_loss_tracking_and_gif(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, save_dir=\"training_output_bicycle_and_pix2pix\"):\n",
    "#     # Create directory for saving models and outputs\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "\n",
    "#     # Initialize CSV for saving loss data\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_inv_huber\", \"train_depth_contrastive\", \"train_depth_smooth\",\n",
    "#             \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_inv_huber\", \"valid_depth_contrastive\", \"valid_depth_smooth\"\n",
    "#         ])\n",
    "\n",
    "#     best_combined_loss = float(\"inf\")  # Initialize best combined loss for saving the best model\n",
    "\n",
    "#     train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     # , \"depth_inv_huber\": [], \"depth_contrastive\": []\n",
    "\n",
    "#     gif_frames = []\n",
    "#     num_classes = 20\n",
    "#     # Optimizer for latent noise\n",
    "    \n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#         model.train()\n",
    "#         # epoch_train_seg_loss = 0\n",
    "#         # epoch_train_depth_loss = 0\n",
    "#         # epoch_train_iou = 0\n",
    "#         # epoch_train_combined_loss = 0\n",
    "#         # epoch_train_depth_sidl = 0\n",
    "#         # epoch_train_depth_inv_huber = 0\n",
    "#         # epoch_train_depth_contrastive = 0\n",
    "#         # epoch_train_depth_smooth = 0\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         reconstruction_layer = nn.Conv2d(256, 3, kernel_size=1).to(device)\n",
    "        \n",
    "#         # scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#         # Training Loop\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "\n",
    "#             # Ensure depth_labels and segmentation labels have correct dimensions\n",
    "#             if depth_labels.dim() == 5:\n",
    "#                 depth_labels = depth_labels.squeeze(2)\n",
    "#             if seg_labels.dim() == 4 and seg_labels.shape[1] == 1:\n",
    "#                 seg_labels = seg_labels.squeeze(1)\n",
    "\n",
    "#             # Transform depth labels\n",
    "#             # depth_labels = torch.log(depth_labels.flatten(start_dim=1)) / 5\n",
    "#             # depth_labels = depth_labels.view_as(depth_labels)  # Restore shape\n",
    "#             # depth_labels = torch.clamp(depth_labels, min=1e-5) \n",
    "#             # depth_labels = torch.log(depth_labels + 1e-5) / 5  # Avoid log(0)\n",
    "\n",
    "#             # print(f'seg_labels shape : {seg_labels.shape}')\n",
    "#             # print(f'depth_labels shape: {depth_labels.shape}')\n",
    "\n",
    "#             # Start with random noise as latent condition\n",
    "#             if epoch == 0:\n",
    "#                 latent_noise = torch.randn_like(inputs).to(device)\n",
    "#                 # print(f\"latent_noise: {latent_noise.shape}\")\n",
    "#                 latent_noise.requires_grad = True  # Make it trainable\n",
    "#                 latent_optimizer = torch.optim.Adam([latent_noise], lr=1e-3)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "#             # Stage 1: Train BicycleGAN (Backbone Features)\n",
    "            \n",
    "\n",
    "#             # Reset gradients for both optimizers\n",
    "#             model.optimizer_stage1.zero_grad()\n",
    "#             model.optimizer_stage2.zero_grad()\n",
    "#             latent_optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             seg_output, depth_output, backbone_features = model(inputs, latent_noise)\n",
    "#             # print(f'seg_ouput shape : {seg_output.shape}')\n",
    "#             # print(f'depth_output shape: {depth_output.shape}')\n",
    "#             # print(backbone_features.shape)\n",
    "#             # return\n",
    "\n",
    "\n",
    "#             seg_output_old =seg_output\n",
    "#             # Resize seg_output to match the spatial dimensions of seg_labels\n",
    "#             seg_output_resized = F.interpolate(seg_output, size=seg_labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "#             seg_output = seg_output_resized\n",
    "\n",
    "#             # print(f\"depth_output shape before resize: {depth_output.shape}\")\n",
    "#             # print(f\"depth_labels shape: {depth_labels.shape}\")\n",
    "#             # return\n",
    "\n",
    "#             depth_output_old = depth_output\n",
    "#             depth_output_resized = F.interpolate(depth_output, size=depth_labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "#             depth_output = depth_output_resized\n",
    "\n",
    "\n",
    "#             # Pix2Pix Losses\n",
    "#             seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#             seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#             seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#             seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "            \n",
    "#             depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#             depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#             depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#             depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "            \n",
    "#             pix2pix_loss = seg_loss_total + depth_loss_total\n",
    "\n",
    "#             # Reconstruction loss\n",
    "#             # inputs_resized = F.interpolate(inputs, size=(backbone_features.size(2), backbone_features.size(3)))\n",
    "#             # reconstructed_image = reconstruction_layer(backbone_features)\n",
    "#             # recon_loss = nn.L1Loss()(reconstructed_image, inputs_resized)\n",
    "#             # adaptive_weight = 1 / (1 + torch.exp(-recon_loss))\n",
    "#             # adaptive_weight_value = adaptive_weight.item() \n",
    "\n",
    "#             # loss_stage1 = nn.MSELoss()(real_validity, torch.ones_like(real_validity).to(device)) + recon_loss\n",
    "#             # loss_stage1.backward(retain_graph=True)\n",
    "#             # model.optimizer_stage1.step()\n",
    "\n",
    "#             # Pix2Pix Adversarial Losses\n",
    "#             seg_validity = model.segmentation_discriminator(seg_output)\n",
    "#             depth_validity = model.depth_discriminator(depth_output)\n",
    "#             adv_seg_loss = nn.MSELoss()(seg_validity, torch.ones_like(seg_validity))\n",
    "#             adv_depth_loss = nn.MSELoss()(depth_validity, torch.ones_like(depth_validity))\n",
    "#             pix2pix_total_loss = pix2pix_loss + adv_seg_loss + adv_depth_loss\n",
    "\n",
    "\n",
    "#             # BicycleGAN Loss with Pix2Pix Condition\n",
    "#             # real_validity = model.bicycle_discriminator(backbone_features)\n",
    "#             # recon_loss = nn.L1Loss()(backbone_features, inputs)\n",
    "#             # bicycle_loss = nn.MSELoss()(real_validity, torch.ones_like(real_validity)) + recon_loss\n",
    "#             # conditional_bicycle_loss = bicycle_loss + pix2pix_loss\n",
    "#             # conditional_bicycle_loss.backward(retain_graph=True)\n",
    "#             # model.optimizer_stage1.step()\n",
    "\n",
    "#             # BicycleGAN Loss with Pix2Pix Condition\n",
    "#             real_validity = model.bicycle_discriminator(backbone_features,latent_noise)\n",
    "\n",
    "#             # Resize inputs to match backbone_features\n",
    "#             inputs_resized = F.interpolate(inputs, size=backbone_features.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "#             # print(f\"backbone_features shape: {backbone_features.shape}, inputs shape: {inputs.shape}\")\n",
    "#             # print(f\"inputs_resized shape: {inputs_resized.shape}\")\n",
    "#             # recon_loss = nn.L1Loss()(backbone_features, inputs_resized)\n",
    "#             bicycle_loss = adv_seg_loss + adv_depth_loss\n",
    "#             # + recon_loss\n",
    "\n",
    "#             # Combined Loss\n",
    "#             total_loss = bicycle_loss + pix2pix_total_loss\n",
    "\n",
    "#             # Single backward pass\n",
    "#             total_loss.backward()\n",
    "\n",
    "#             # Update both optimizers\n",
    "#             model.optimizer_stage1.step()\n",
    "#             model.optimizer_stage2.step()\n",
    "#             latent_optimizer.step()\n",
    "\n",
    "#             # Accumulate Training Metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#             epoch_train[\"combined\"] += total_loss.item()\n",
    "#             epoch_train[\"iou\"] += seg_iou.item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_smooth.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#         model.scheduler_stage1.step()\n",
    "#         model.scheduler_stage2.step(epoch_train[\"combined\"]/num_batches)\n",
    "\n",
    "\n",
    "#         # Average Training Losses\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Train Seg Loss: {epoch_train['seg']:.4f}, \"\n",
    "#             f\"Train Depth Loss: {epoch_train['depth']:.4f}, Train Combined Loss: {epoch_train['combined']:.4f}, \"\n",
    "#             f\"Train mIOU: {epoch_train['iou']:.4f}, Train sidl Loss: {epoch_train['depth_sidl']:.4f}, \"\n",
    "#             f\"Train depth smooth: {epoch_train['depth_smooth']:.4f}\"\n",
    "#     )       \n",
    "\n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         # epoch_valid_seg_loss = 0\n",
    "#         # epoch_valid_depth_loss = 0\n",
    "#         # epoch_valid_iou =0\n",
    "#         # epoch_valid_combined_loss = 0\n",
    "#         # epoch_valid_depth_sidl = 0\n",
    "#         # epoch_valid_depth_inv_huber = 0\n",
    "#         # epoch_valid_depth_contrastive = 0\n",
    "#         # epoch_valid_depth_smooth = 0\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 # print(\"inside valid\")\n",
    "#                 inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "\n",
    "#                 # Ensure depth_labels and segmentation labels have correct dimensions\n",
    "#                 if depth_labels.dim() == 5:\n",
    "#                     depth_labels = depth_labels.squeeze(2)\n",
    "#                 if seg_labels.dim() == 4 and seg_labels.shape[1] == 1:\n",
    "#                     seg_labels = seg_labels.squeeze(1)\n",
    "\n",
    "#                 # Transform depth labels\n",
    "#                 # depth_labels = torch.log(depth_labels.flatten(start_dim=1)) / 5\n",
    "#                 # depth_labels = depth_labels.view_as(depth_labels)  # Restore shape\n",
    "#                 # depth_labels = torch.clamp(depth_labels, min=1e-5) \n",
    "#                 # depth_labels = torch.log(depth_labels + 1e-5) / 5  # Avoid log(0)\n",
    "\n",
    "#                 # Latent noise for validation\n",
    "#                 latent_noise = torch.randn_like(inputs).to(device)\n",
    "#                 seg_output, depth_output, backbone_features = model(inputs, latent_noise)\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "#                 seg_output_old =seg_output\n",
    "#                 # Resize seg_output to match the spatial dimensions of seg_labels\n",
    "#                 seg_output_resized = F.interpolate(seg_output, size=seg_labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "#                 seg_output = seg_output_resized\n",
    "\n",
    "#                 depth_output_old = depth_output\n",
    "#                 depth_output_resized = F.interpolate(depth_output, size=depth_labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "#                 depth_output =depth_output_resized\n",
    "\n",
    "\n",
    "#                 # Segmentation Loss\n",
    "#                 seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#                 seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#                 seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#                 seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "                \n",
    "#                 depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#                 depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#                 depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#                 depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "\n",
    "#                 pix2pix_loss = seg_loss_total + depth_loss_total\n",
    "\n",
    "#                 # Combined Validation Loss\n",
    "#                 combined_loss = pix2pix_loss\n",
    "\n",
    "#                 # Accumulate Validation Metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#                 epoch_valid[\"combined\"] += combined_loss.item()\n",
    "#                 epoch_valid[\"iou\"] += seg_iou.item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_smooth.item()\n",
    "                \n",
    "#                 num_valid_batches += 1\n",
    "                \n",
    "#                 # epoch, inputs, seg_output, depth_output, seg_labels, depth_labels, gif_frames\n",
    "#             frame = save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels)\n",
    "#             gif_frames.append(frame)\n",
    "                \n",
    "                \n",
    "#         # Calculate epoch averages\n",
    "#         # Average Validation Losses\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "        \n",
    "        \n",
    "# # train_losses = { \"depth_sidl\": [], \"depth_inv_huber\": [], \"depth_contrastive\": [], \"depth_smooth\": []}\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Valid Seg Loss: {epoch_valid['seg']:.4f}, \"\n",
    "#             f\"Valid Depth Loss: {epoch_valid['depth']:.4f}, Valid Combined Loss: {epoch_valid['combined']:.4f}, \"\n",
    "#             f\"Valid mIOU: {epoch_valid['iou']:.4f}, Valid sidl Loss: {epoch_valid['depth_sidl']:.4f}, \"\n",
    "#             f\"Valid depth smooth: {epoch_valid['depth_smooth']:.4f}\"\n",
    "#         )\n",
    "\n",
    "#         # Write the losses to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 train_losses[\"seg\"], train_losses[\"depth\"], train_losses[\"combined\"],\n",
    "#                 train_losses[\"depth_sidl\"], 0,0,\n",
    "#                 # avg_train_depth_inv_huber, avg_train_depth_contrastive,\n",
    "#                 train_losses[\"depth_smooth\"],\n",
    "#                 valid_losses[\"seg\"], valid_losses[\"depth\"], valid_losses['combined'],\n",
    "#                 valid_losses[\"depth_sidl\"],0,0,\n",
    "#                 # avg_valid_depth_inv_huber, avg_valid_depth_contrastive, \n",
    "#                 valid_losses[\"depth_smooth\"]\n",
    "#             ])\n",
    "\n",
    "#         # Save GIF visualization frames\n",
    "#         # save_training_visualization_as_gif(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels, gif_frames)\n",
    "\n",
    "#         # Save best model\n",
    "#         if valid_losses[\"combined\"][-1] < best_combined_loss:\n",
    "#             best_combined_loss = valid_losses[\"combined\"][-1]\n",
    "#             torch.save(model, os.path.join(save_dir, \"best_model_resnetBackbone.pth\"))\n",
    "#             print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "#         if epoch%10==0:\n",
    "#             gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "#             gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "#     # Save GIF\n",
    "#     # gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "    \n",
    "#     plot_loss(train_losses, valid_losses, save_dir)\n",
    "#     gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return train_losses,valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f553ec-993f-4642-a88f-f519abbeac8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa457c-8d5c-48cf-a9eb-d3ad1b361e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
