{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ba9b76-3c4a-4dba-a295-b930ea39f73c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes',\n",
       " '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/cityscapes_dataset')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,Subset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from sklearn.metrics import jaccard_score\n",
    "from torchvision.models.mobilenetv3 import MobileNet_V3_Small_Weights\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from labels import labels\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "curr_dir=os.getcwd()\n",
    "root= os.path.join(curr_dir,\"cityscapes_dataset\")\n",
    "curr_dir,root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a94aa7-dba7-4018-9134-f063b811cd98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "# from albumentations.augmentations.transforms import RandomShadow\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\" Normalizes RGB image to  0-mean 1-std_dev \"\"\" \n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], depth_norm=5, max_depth=250):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.depth_norm = depth_norm\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            \n",
    "        return {'left': TF.normalize(left, self.mean, self.std), \n",
    "                'mask': mask, \n",
    "                'depth' : torch.clip( # saftey clip :)\n",
    "                            torch.log(torch.clip(depth, 0, self.max_depth))/self.depth_norm, \n",
    "                            0, \n",
    "                            self.max_depth)}\n",
    "\n",
    "\n",
    "class AddColorJitter(object):\n",
    "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\" \n",
    "    def __init__(self, brightness, contrast, saturation, hue):\n",
    "        ''' Applies brightness, constrast, saturation, and hue jitter to image ''' \n",
    "        self.color_jitter = transforms.ColorJitter(brightness, contrast, saturation, hue)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        return {'left': self.color_jitter(left), \n",
    "                'mask': mask, \n",
    "                'depth' : depth}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\" Rescales images with bilinear interpolation and masks with nearest interpolation \"\"\"\n",
    "\n",
    "    def __init__(self, h, w):\n",
    "        self.h, self.w = h, w\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "# mask interpolation Nearest is import to have smoothness\n",
    "        return {'left': TF.resize(left, (self.h, self.w)), \n",
    "                'mask': TF.resize(mask.unsqueeze(0), (self.h, self.w), transforms.InterpolationMode.NEAREST), \n",
    "                'depth' : TF.resize(depth.unsqueeze(0), (self.h, self.w))}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, h, w, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.scale = scale\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "        i, j, h, w = transforms.RandomResizedCrop.get_params(left, scale=self.scale, ratio=self.ratio)\n",
    "\n",
    "        return {'left': TF.resized_crop(left, i, j, h, w, (self.h, self.w)), \n",
    "                'mask': TF.resized_crop(mask.unsqueeze(0), i, j, h, w, (self.h, self.w), interpolation=TF.InterpolationMode.NEAREST),\n",
    "                'depth' : TF.resized_crop(depth.unsqueeze(0), i, j, h, w, (self.h, self.w))}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "         \n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        return {'left': transforms.ToTensor()(left), \n",
    "                'mask': torch.as_tensor(mask, dtype=torch.int64),\n",
    "                'depth' : transforms.ToTensor()(depth).type(torch.float32)}\n",
    "    \n",
    "\n",
    "class ElasticTransform(object):\n",
    "    def __init__(self, alpha=25.0, sigma=5.0, prob=0.5):\n",
    "        self.alpha = [1.0, alpha]\n",
    "        self.sigma = [1, sigma]\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            _, H, W = mask.shape\n",
    "            displacement = transforms.ElasticTransform.get_params(self.alpha, self.sigma, [H, W])\n",
    "\n",
    "            # # TEMP\n",
    "            # print(TF.elastic_transform(left, displacement).shape)\n",
    "            # print(TF.elastic_transform(mask.unsqueeze(0), displacement, interpolation=TF.InterpolationMode.NEAREST).shape)\n",
    "            # print(torch.clip(TF.elastic_transform(depth, displacement), 0, depth.max()).shape)\n",
    "\n",
    "            return {'left': TF.elastic_transform(left, displacement), \n",
    "                    'mask': TF.elastic_transform(mask.unsqueeze(0), displacement, interpolation=TF.InterpolationMode.NEAREST), \n",
    "                    'depth' : torch.clip(TF.elastic_transform(depth, displacement), 0, depth.max())} \n",
    "        \n",
    "        else:\n",
    "            return sample\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "# new transform to rotate the images\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, angle):\n",
    "        if not isinstance(angle, (list, tuple)):\n",
    "            self.angle = (-abs(angle), abs(angle))\n",
    "        else:\n",
    "            self.angle = angle\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        angle = transforms.RandomRotation.get_params(self.angle)\n",
    "\n",
    "        return {'left': TF.rotate(left, angle), \n",
    "                'mask': TF.rotate(mask.unsqueeze(0), angle), \n",
    "                'depth' : TF.rotate(depth, angle)}\n",
    "    \n",
    "    \n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            return {'left': TF.hflip(left), \n",
    "                    'mask': TF.hflip(mask), \n",
    "                    'depth' : TF.hflip(depth)}\n",
    "        else:\n",
    "            return sample\n",
    "        \n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            return {'left': TF.vflip(left), \n",
    "                    'mask': TF.vflip(mask), \n",
    "                    'depth' : TF.vflip(depth)}\n",
    "        else:\n",
    "            return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22ee78a-1a67-4c7b-a0ef-2ec953859137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_numpy(image):\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        if len(image.shape) == 2:\n",
    "            image = image.detach().cpu().numpy()\n",
    "        else:\n",
    "            image = image.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_color_mask(mask, labels, id_type='id'):\n",
    "    try:\n",
    "        h, w = mask.shape\n",
    "    except ValueError:\n",
    "        mask = mask.squeeze(-1)\n",
    "        h, w = mask.shape\n",
    "\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    if id_type == 'id':\n",
    "        for lbl in labels:\n",
    "            color_mask[mask == lbl.id] = lbl.color\n",
    "    elif id_type == 'trainId':\n",
    "        for lbl in labels:\n",
    "            if (lbl.trainId != 255) | (lbl.trainId != -1):\n",
    "                color_mask[mask == lbl.trainId] = lbl.color\n",
    "\n",
    "    return color_mask\n",
    "\n",
    "\n",
    "def plot_items(left, mask, depth, labels=None, num_seg_labels=34, id_type='id'):\n",
    "    left = convert_to_numpy(left)\n",
    "    mask = convert_to_numpy(mask)\n",
    "    depth = convert_to_numpy(depth)\n",
    "\n",
    "    # unnormalize left image\n",
    "    left = (left*np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
    "\n",
    "    # cmaps: 'prism', 'terrain', 'turbo', 'gist_rainbow_r', 'nipy_spectral_r'\n",
    "    \n",
    "    \n",
    "    _, ax = plt.subplots(1, 3, figsize=(15,10))\n",
    "    ax[0].imshow(left)\n",
    "    ax[0].set_title(\"Left Image\")\n",
    "\n",
    "    if labels:\n",
    "        color_mask = get_color_mask(mask, labels, id_type)\n",
    "        ax[1].imshow(color_mask)\n",
    "    else:\n",
    "        cmap = mpl.colormaps.get_cmap('nipy_spectral_r').resampled(num_seg_labels)\n",
    "        ax[1].imshow(mask, cmap=cmap)\n",
    "\n",
    "    ax[1].set_title(\"Seg Mask\")\n",
    "    ax[2].imshow(depth, cmap='plasma')\n",
    "    ax[2].set_title(\"Depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e366997-a6f1-47cd-8190-1fb12596df2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_invariant_depth_loss(pred, target, lambda_weight=0.1):\n",
    "    if pred.shape != target.shape:\n",
    "        pred = F.interpolate(pred, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "    \n",
    "    diff = pred - target\n",
    "    n = diff.numel()\n",
    "    mse = torch.sum(diff**2) / n\n",
    "    scale_invariant = mse - (lambda_weight / (n**2)) * (torch.sum(diff))**2\n",
    "    return scale_invariant\n",
    "\n",
    "def depth_smoothness_loss(pred, img, alpha=1.0):\n",
    "    depth_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])\n",
    "    depth_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])\n",
    "    img_grad_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), dim=1, keepdim=True)\n",
    "    img_grad_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), dim=1, keepdim=True)\n",
    "    smoothness_x = depth_grad_x * torch.exp(-alpha * img_grad_x)\n",
    "    smoothness_y = depth_grad_y * torch.exp(-alpha * img_grad_y)\n",
    "    return smoothness_x.mean() + smoothness_y.mean()\n",
    "\n",
    "\n",
    "def inv_huber_loss(pred, target, delta=0.1):\n",
    "    \"\"\"\n",
    "    Inverse Huber loss for depth prediction.\n",
    "    Args:\n",
    "        pred (Tensor): Predicted depth map.\n",
    "        target (Tensor): Ground truth depth map.\n",
    "        delta (float): Threshold for switching between quadratic and linear terms.\n",
    "    Returns:\n",
    "        Tensor: Inverse Huber loss.\n",
    "    \"\"\"\n",
    "    abs_diff = torch.abs(pred - target)\n",
    "    delta_tensor = torch.tensor(delta, dtype=abs_diff.dtype, device=abs_diff.device)  # Convert delta to tensor\n",
    "    quadratic = torch.minimum(abs_diff, delta_tensor)\n",
    "    linear = abs_diff - quadratic\n",
    "    return (0.5 * quadratic**2 + delta_tensor * linear).mean()\n",
    "\n",
    "\n",
    "def mean_iou(pred, target, num_classes):\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    intersection = torch.logical_and(pred == target, target != 255).float()  # Ignore class 255\n",
    "    union = torch.logical_or(pred == target, target != 255).float()\n",
    "    iou = torch.sum(intersection) / torch.sum(union)\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def contrastive_loss(pred, target, margin=1.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss to ensure the depth map predictions are closer to the target.\n",
    "    \"\"\"\n",
    "    # Flatten the tensors for element-wise operations\n",
    "    pred_flat = pred.view(pred.size(0), -1)  # Flatten except for the batch dimension\n",
    "    target_flat = target.view(target.size(0), -1)  # Flatten except for the batch dimension\n",
    "\n",
    "    # Compute the pairwise distances\n",
    "    distances = torch.sqrt(torch.sum((pred_flat - target_flat) ** 2, dim=1))  # Batch-wise distances\n",
    "\n",
    "    # Create labels for contrastive loss\n",
    "    labels = (torch.abs(pred_flat - target_flat).mean(dim=1) < margin).float()  # Batch-wise labels\n",
    "\n",
    "    # Calculate contrastive loss\n",
    "    similar_loss = labels * distances**2\n",
    "    dissimilar_loss = (1 - labels) * torch.clamp(margin - distances, min=0)**2\n",
    "    loss = (similar_loss + dissimilar_loss).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def dice_loss(predictions, targets, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Dice Loss for segmentation.\n",
    "    Args:\n",
    "        predictions (torch.Tensor): The predicted segmentation map (logits or probabilities).\n",
    "                                    Shape: [batch_size, num_classes, height, width]\n",
    "        targets (torch.Tensor): The ground truth segmentation map (one-hot encoded or integer labels).\n",
    "                                Shape: [batch_size, height, width]\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    Returns:\n",
    "        torch.Tensor: Dice Loss (scalar).\n",
    "    \"\"\"\n",
    "    # Convert integer labels to one-hot if needed\n",
    "    if predictions.shape != targets.shape:\n",
    "        targets = F.one_hot(targets, num_classes=predictions.shape[1]).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    # Apply softmax to predictions for multi-class segmentation\n",
    "    predictions = torch.softmax(predictions, dim=1)\n",
    "    \n",
    "    # Flatten tensors to calculate intersection and union\n",
    "    predictions_flat = predictions.view(predictions.shape[0], predictions.shape[1], -1)\n",
    "    targets_flat = targets.view(targets.shape[0], targets.shape[1], -1)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = (predictions_flat * targets_flat).sum(dim=-1)\n",
    "    union = predictions_flat.sum(dim=-1) + targets_flat.sum(dim=-1)\n",
    "    \n",
    "    # Calculate Dice Coefficient\n",
    "    dice_coeff = (2 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # Dice Loss\n",
    "    return 1 - dice_coeff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8ec23d0-1fad-4dfe-8b94-6de4c2579d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, valid_losses, save_dir):\n",
    "    epochs = range(1, len(train_losses[\"seg\"]) + 1)\n",
    "\n",
    "    # Plot Segmentation Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses[\"seg\"], label=\"Train Seg Loss\")\n",
    "    plt.plot(epochs, valid_losses[\"seg\"], label=\"Valid Seg Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Segmentation Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Segmentation Loss Over Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir, \"segmentation_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Depth Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses[\"depth\"], label=\"Train Depth Loss\")\n",
    "    plt.plot(epochs, valid_losses[\"depth\"], label=\"Valid Depth Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Depth Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Depth Loss Over Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir, \"depth_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Combined Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses[\"combined\"], label=\"Train Combined Loss\")\n",
    "    plt.plot(epochs, valid_losses[\"combined\"], label=\"Valid Combined Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Combined Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Combined Loss Over Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir, \"combined_loss.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b8fe187-44fc-45aa-bf13-87b9b28936af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels):\n",
    "    inputs = inputs.detach().cpu()\n",
    "    seg_output = torch.argmax(seg_output, dim=1).detach().cpu()\n",
    "    depth_output = depth_output.detach().cpu()\n",
    "    seg_labels = seg_labels.detach().cpu()\n",
    "    depth_labels = depth_labels.detach().cpu()\n",
    "    \n",
    "#     inputs_rgb = (inputs - inputs.min()) / (inputs.max() - inputs.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "    \n",
    "#     # Normalize depth maps for visualization\n",
    "#     depth_labels_vis = (depth_labels - depth_labels.min()) / (depth_labels.max() - depth_labels.min() + 1e-5)\n",
    "#     depth_preds_vis = (depth_output - depth_output.min()) / (depth_output.max() - depth_output.min() + 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = min(4, inputs.size(0))  # Limit to 4 samples for visualization\n",
    "    fig, axes = plt.subplots(batch_size, 5, figsize=(15, 4 * batch_size))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        inputs_temp = inputs[i]\n",
    "        # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "        inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "        \n",
    "        depth_labels_vis = (depth_labels[i] - depth_labels[i].min()) / (depth_labels[i].max() - depth_labels[i].min() + 1e-5)\n",
    "        depth_preds = depth_output[i]\n",
    "        depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5)\n",
    "        # print(f\"depth_labels_vis: {depth_labels_vis.shape}\")\n",
    "        # print(f\"depth_preds_vis: {depth_preds_vis.shape}\")\n",
    "\n",
    "    \n",
    "        \n",
    "        # Row 1: Ground truth\n",
    "        axes[i, 0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "        axes[i, 0].set_title(\"RGB Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(seg_labels[i], cmap=\"tab20\")\n",
    "        axes[i, 1].set_title(\"GT Segmentation\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(depth_labels_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 2].set_title(\"GT Depth\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "        # Row 2: Predictions\n",
    "        axes[i, 3].imshow(seg_output[i], cmap=\"tab20\")\n",
    "        axes[i, 3].set_title(\"Generated Segmentation\")\n",
    "        axes[i, 3].axis(\"off\")\n",
    "\n",
    "        axes[i, 4].imshow(depth_preds_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 4].set_title(\"Generated Depth\")\n",
    "        axes[i, 4].axis(\"off\")\n",
    "        \n",
    "    # Remove axes for cleaner visualization\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    fig.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # # Save current epoch as an image for GIF\n",
    "    # epoch_img_path = os.path.join(gif_path, f\"epoch_{epoch}.png\")\n",
    "    # os.makedirs(gif_path, exist_ok=True)\n",
    "    # plt.savefig(epoch_img_path)\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    # return epoch_img_path\n",
    "    frame = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)  # Updated to buffer_rgba\n",
    "    frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (4,))  # RGBA has 4 channels\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Convert to PIL.Image for GIF\n",
    "    frame_rgb = frame[:, :, :3] \n",
    "\n",
    "    # Return as PIL.Image for GIF creation\n",
    "    # return Image.fromarray(frame)\n",
    "    return Image.fromarray(frame_rgb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55d241-89e1-47d7-9552-60deb4df0251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01be9a0-180e-44bf-a549-ba003cbf2bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Define the ResBlock\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "# Define the CRPBlock\n",
    "class CRPBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, n_stages=4, groups=False):\n",
    "        super(CRPBlock, self).__init__()\n",
    "        self.n_stages = n_stages\n",
    "        groups = in_chans if groups else 1\n",
    "        self.mini_blocks = nn.ModuleList()\n",
    "        for _ in range(n_stages):\n",
    "            self.mini_blocks.append(nn.MaxPool2d(kernel_size=5, stride=1, padding=2))\n",
    "            self.mini_blocks.append(nn.Conv2d(in_chans, out_chans, kernel_size=1, bias=False, groups=groups))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for block in self.mini_blocks:\n",
    "            out = block(out)\n",
    "            x = x + out\n",
    "        return x\n",
    "\n",
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, feature_dim=256):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        base_model = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Freeze pre-trained layers\n",
    "        for param in base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Extract ResNet layers and modify strides/pooling to preserve spatial dimensions\n",
    "        layers = list(base_model.children())[:-2]  # Remove FC and AvgPool layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                layer.stride = (1, 1)  # Set stride to 1\n",
    "            elif isinstance(layer, nn.MaxPool2d) or isinstance(layer, nn.AvgPool2d):\n",
    "                layer.stride = (1, 1)  # Avoid reducing dimensions with pooling layers\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # Adjust final feature dimension using a 1x1 convolution\n",
    "        self.feature_dim = feature_dim\n",
    "        self.adjust_channels = nn.Conv2d(base_model.fc.in_features, feature_dim, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Extract features without changing spatial dimensions\n",
    "        x = self.adjust_channels(x)  # Adjust feature channels\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70ae93ff-1a2e-46c9-92cf-57767c645dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CityScapesDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, split='train', label_map='id', crop=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "        self.crop = crop\n",
    "\n",
    "        self.left_paths = glob(os.path.join(root, 'leftImg8bit', split, '**/*.png'))\n",
    "        self.mask_paths = glob(os.path.join(root, 'gtFine', split, '**/*gtFine_labelIds.png'))\n",
    "        self.depth_paths = glob(os.path.join(root, 'crestereo_depth2', split, '**/*.npy'))\n",
    "\n",
    "        sorted(self.left_paths)\n",
    "        sorted(self.mask_paths)\n",
    "        sorted(self.depth_paths)\n",
    "\n",
    "        # get label mappings\n",
    "        self.id_2_train = {}\n",
    "        self.id_2_cat = {}\n",
    "        self.train_2_id = {}\n",
    "        self.id_2_name = {-1 : 'unlabeled'}\n",
    "        self.trainid_2_name = {19 : 'unlabeled'} # {255 : 'unlabeled', -1 : 'unlabeled'}\n",
    "\n",
    "        for lbl in labels:\n",
    "            self.id_2_train.update({lbl.id : lbl.trainId})\n",
    "            self.id_2_cat.update({lbl.id : lbl.categoryId})\n",
    "            if lbl.trainId != 19: # (lbl.trainId > 0) and (lbl.trainId != 255):\n",
    "                self.trainid_2_name.update({lbl.trainId : lbl.name})\n",
    "                self.train_2_id.update({lbl.trainId : lbl.id})\n",
    "            if lbl.id > 0:\n",
    "                self.id_2_name.update({lbl.id : lbl.name})\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left = cv2.cvtColor(cv2.imread(self.left_paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_UNCHANGED).astype(np.uint8)\n",
    "        depth = np.load(self.depth_paths[idx]) # data is type float16\n",
    "\n",
    "        if self.crop:\n",
    "            left = left[:800, :, :]\n",
    "            mask = mask[:800, :]\n",
    "            depth = depth[:800, :]\n",
    "\n",
    "        # get label id\n",
    "        if self.label_map == 'id':\n",
    "            mask[mask==-1] == 0\n",
    "        elif self.label_map == 'trainId':\n",
    "            for _id, train_id in self.id_2_train.items():\n",
    "                mask[mask==_id] = train_id\n",
    "        elif self.label_map == 'categoryId':\n",
    "            for _id, train_id in self.id_2_cat.items():\n",
    "                mask[mask==_id] = train_id\n",
    "\n",
    "        sample = {'left' : left, 'mask' : mask, 'depth' : depth}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # ensure that no depth values are less than 0\n",
    "        depth[depth < 0] = 0\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        print(f\"Number of RGB images: {len(self.left_paths)}\")\n",
    "        print(f\"Number of Mask images: {len(self.mask_paths)}\")\n",
    "        print(f\"Number of depth images: {len(self.depth_paths)}\")\n",
    "        return len(self.left_paths)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92a35c2-3d5c-4f87-87ac-4b189c154bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OG_W, OG_H = 2048, 800 # OG width and height after crop\n",
    "W, H = OG_W//4, OG_H//4 # resize w,h for training\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    RandomCrop(H, W),\n",
    "    # ElasticTransform(alpha=100.0, sigma=25.0, prob=0.5),\n",
    "    AddColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    RandomHorizontalFlip(0.5),\n",
    "    RandomVerticalFlip(0.2),\n",
    "    # RandomRotate((-30, 30)),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Rescale(H, W),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CityScapesDataset(root, transform=transform, split='train', label_map='trainId') # 'trainId')\n",
    "train_subset = Subset(train_dataset, indices=range(2968)) #2968\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n",
    "\n",
    "\n",
    "valid_dataset = CityScapesDataset(root, transform=valid_transform, split='val', label_map='trainId')\n",
    "val_subset = Subset(valid_dataset, indices=range(496)) #496\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=False)\n",
    "valid_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4612e4a-638e-4b58-9cd1-7c6f59edf9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b96a8e6-8ceb-4ce9-b268-73df64cc0c12",
   "metadata": {},
   "source": [
    "# shared Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1dc8478-39f9-41f3-bb56-02ffccc724bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SharedGenerator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         Shared Generator for both tasks.\n",
    "#         Contains shared layers for skip connection processing and refinement.\n",
    "#         \"\"\"\n",
    "#         super(SharedGenerator, self).__init__()\n",
    "#         # Shared convolution layers to process each skip connection\n",
    "#         self.shared_conv1 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l11_out (1/32)\n",
    "#         self.shared_conv2 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l7_out (1/16)\n",
    "#         self.shared_conv3 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l3_out (1/8)\n",
    "#         self.shared_conv4 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l1_out (1/4)\n",
    "\n",
    "#         # Shared CRP blocks for refinement\n",
    "#         self.shared_crp1 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/32\n",
    "#         self.shared_crp2 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/16\n",
    "#         self.shared_crp3 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/8\n",
    "#         self.shared_crp4 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/4\n",
    "\n",
    "#     def forward(self, skips):\n",
    "#         \"\"\"\n",
    "#         Process skips with shared layers for task-specific generation.\n",
    "#         Args:\n",
    "#             skips (dict): Skip connections from the encoder.\n",
    "#         Returns:\n",
    "#             dict: Processed skip connections.\n",
    "#         \"\"\"\n",
    "#         x1 = self.shared_crp1(self.shared_conv1(skips[\"l11_out\"]))\n",
    "#         x2 = self.shared_crp2(self.shared_conv2(skips[\"l7_out\"]))\n",
    "#         x3 = self.shared_crp3(self.shared_conv3(skips[\"l3_out\"]))\n",
    "#         x4 = self.shared_crp4(self.shared_conv4(skips[\"l1_out\"]))\n",
    "\n",
    "#         return {\"x1\": x1, \"x2\": x2, \"x3\": x3, \"x4\": x4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e4d36-98bd-4506-ad2f-3220cd3661f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb487e-4641-481a-9b15-56d1ce9a38ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e74154-b3ec-4e4c-aa36-aa2e9f5545fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa913d77-fd04-4079-a039-6caf557329a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SharedPix2PixGenerator(nn.Module):\n",
    "#     def __init__(self, seg_output_channels=20, depth_output_channels=1):\n",
    "#         \"\"\"\n",
    "#         Shared Pix2Pix Generator for Segmentation and Depth tasks.\n",
    "#         Args:\n",
    "#             seg_output_channels (int): Number of output channels for segmentation.\n",
    "#             depth_output_channels (int): Number of output channels for depth estimation.\n",
    "#         \"\"\"\n",
    "#         super(SharedPix2PixGenerator, self).__init__()\n",
    "#         self.shared_generator = SharedGenerator()\n",
    "#         self.seg_output_layer = TaskOutputLayer(output_channels=seg_output_channels)\n",
    "#         self.depth_output_layer = TaskOutputLayer(output_channels=depth_output_channels)\n",
    "\n",
    "#     def forward(self, skips, input_size):\n",
    "#         \"\"\"\n",
    "#         Forward pass for both tasks.\n",
    "#         Args:\n",
    "#             skips (dict): Skip connections from the encoder.\n",
    "#             input_size (tuple): Original input size (H, W).\n",
    "#         Returns:\n",
    "#             dict: Outputs for segmentation and depth tasks.\n",
    "#         \"\"\"\n",
    "#         shared_features = self.shared_generator(skips)\n",
    "\n",
    "#         # Task-specific outputs\n",
    "#         seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "#         depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "#         return {\n",
    "#             \"seg_output\": seg_output,\n",
    "#             \"depth_output\": depth_output\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80b7b6-15b9-47dd-bdea-af6fa22b055b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59b8dc73-e69d-4f80-b1e4-09507649a504",
   "metadata": {},
   "source": [
    "# Saving batch gif code And function to plot al losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36faf655-f78f-4cc8-99af-8de9a1ecabcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels):\n",
    "    inputs = inputs.detach().cpu()\n",
    "    seg_output = torch.argmax(seg_output, dim=1).detach().cpu()\n",
    "    depth_output = depth_output.detach().cpu()\n",
    "    seg_labels = seg_labels.detach().cpu()\n",
    "    depth_labels = depth_labels.detach().cpu()\n",
    "    \n",
    "#     inputs_rgb = (inputs - inputs.min()) / (inputs.max() - inputs.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "    \n",
    "#     # Normalize depth maps for visualization\n",
    "#     depth_labels_vis = (depth_labels - depth_labels.min()) / (depth_labels.max() - depth_labels.min() + 1e-5)\n",
    "#     depth_preds_vis = (depth_output - depth_output.min()) / (depth_output.max() - depth_output.min() + 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = min(4, inputs.size(0))  # Limit to 4 samples for visualization\n",
    "    fig, axes = plt.subplots(batch_size, 5, figsize=(15, 4 * batch_size))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        inputs_temp = inputs[i]\n",
    "        # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "        inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "        \n",
    "        depth_labels_vis = (depth_labels[i] - depth_labels[i].min()) / (depth_labels[i].max() - depth_labels[i].min() + 1e-5)\n",
    "        depth_preds = depth_output[i]\n",
    "        depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5)\n",
    "        # print(f\"depth_labels_vis: {depth_labels_vis.shape}\")\n",
    "        # print(f\"depth_preds_vis: {depth_preds_vis.shape}\")\n",
    "\n",
    "    \n",
    "        \n",
    "        # Row 1: Ground truth\n",
    "        axes[i, 0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "        axes[i, 0].set_title(\"RGB Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(seg_labels[i], cmap=\"tab20\")\n",
    "        axes[i, 1].set_title(\"GT Segmentation\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(depth_labels_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 2].set_title(\"GT Depth\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "        # Row 2: Predictions\n",
    "        axes[i, 3].imshow(seg_output[i], cmap=\"tab20\")\n",
    "        axes[i, 3].set_title(\"Generated Segmentation\")\n",
    "        axes[i, 3].axis(\"off\")\n",
    "\n",
    "        axes[i, 4].imshow(depth_preds_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 4].set_title(\"Generated Depth\")\n",
    "        axes[i, 4].axis(\"off\")\n",
    "        \n",
    "    # Remove axes for cleaner visualization\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    fig.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # # Save current epoch as an image for GIF\n",
    "    # epoch_img_path = os.path.join(gif_path, f\"epoch_{epoch}.png\")\n",
    "    # os.makedirs(gif_path, exist_ok=True)\n",
    "    # plt.savefig(epoch_img_path)\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    # return epoch_img_path\n",
    "    frame = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)  # Updated to buffer_rgba\n",
    "    frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (4,))  # RGBA has 4 channels\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Convert to PIL.Image for GIF\n",
    "    frame_rgb = frame[:, :, :3] \n",
    "\n",
    "    # Return as PIL.Image for GIF creation\n",
    "    # return Image.fromarray(frame)\n",
    "    return Image.fromarray(frame_rgb)\n",
    "\n",
    "def plot_all_losses(train_losses,valid_losses,save_dir):\n",
    "    # Plot training and validation losses\n",
    "    for key in train_losses.keys():\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses[key], label=f\"Train {key}\")\n",
    "        plt.plot(valid_losses[key], label=f\"Valid {key}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(key.replace(\"_\", \" \").title())\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, f\"{key}_loss.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f670c-bfdb-49d7-90f9-b1f42c9a3152",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e338594d-fd9d-4121-aeef-288680c916cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"vgg16\", layers=[\"relu3_3\"], device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Perceptual loss class.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model (str): Pretrained model to use (e.g., \"vgg16\").\n",
    "            layers (list of str): Layers to extract features from.\n",
    "            device (str): Device to load the pretrained model on (\"cuda\" or \"cpu\").\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained model\n",
    "        if pretrained_model == \"vgg16\":\n",
    "            vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pretrained model: {pretrained_model}\")\n",
    "\n",
    "        # Freeze the parameters\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Select layers\n",
    "        self.layers = layers\n",
    "        self.feature_extractor = nn.ModuleDict({\n",
    "            layer: vgg[:i] for i, layer in enumerate(vgg._modules.keys()) if layer in self.layers\n",
    "        })\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        \"\"\"\n",
    "        Compute perceptual loss between generated and target images.\n",
    "\n",
    "        Args:\n",
    "            generated (torch.Tensor): Generated image batch.\n",
    "            target (torch.Tensor): Target image batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: MSE loss between extracted features.\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        for layer_name, extractor in self.feature_extractor.items():\n",
    "            gen_features = extractor(generated)\n",
    "            target_features = extractor(target)\n",
    "            loss += F.mse_loss(gen_features, target_features)\n",
    "        return loss\n",
    "\n",
    "def scale_invariant_depth_loss(pred, target, lambda_weight=0.1):\n",
    "    if pred.shape != target.shape:\n",
    "        pred = F.interpolate(pred, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "    \n",
    "    diff = pred - target\n",
    "    n = diff.numel()\n",
    "    mse = torch.sum(diff**2) / n\n",
    "    scale_invariant = mse - (lambda_weight / (n**2)) * (torch.sum(diff))**2\n",
    "    return scale_invariant\n",
    "\n",
    "def depth_smoothness_loss(pred, img, alpha=1.0):\n",
    "    depth_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])\n",
    "    depth_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])\n",
    "    img_grad_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), dim=1, keepdim=True)\n",
    "    img_grad_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), dim=1, keepdim=True)\n",
    "    smoothness_x = depth_grad_x * torch.exp(-alpha * img_grad_x)\n",
    "    smoothness_y = depth_grad_y * torch.exp(-alpha * img_grad_y)\n",
    "    return smoothness_x.mean() + smoothness_y.mean()\n",
    "\n",
    "\n",
    "def inv_huber_loss(pred, target, delta=0.1):\n",
    "    \"\"\"\n",
    "    Inverse Huber loss for depth prediction.\n",
    "    Args:\n",
    "        pred (Tensor): Predicted depth map.\n",
    "        target (Tensor): Ground truth depth map.\n",
    "        delta (float): Threshold for switching between quadratic and linear terms.\n",
    "    Returns:\n",
    "        Tensor: Inverse Huber loss.\n",
    "    \"\"\"\n",
    "    abs_diff = torch.abs(pred - target)\n",
    "    delta_tensor = torch.tensor(delta, dtype=abs_diff.dtype, device=abs_diff.device)  # Convert delta to tensor\n",
    "    quadratic = torch.minimum(abs_diff, delta_tensor)\n",
    "    linear = abs_diff - quadratic\n",
    "    return (0.5 * quadratic**2 + delta_tensor * linear).mean()\n",
    "\n",
    "\n",
    "def mean_iou(pred, target, num_classes):\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    intersection = torch.logical_and(pred == target, target != 255).float()  # Ignore class 255\n",
    "    union = torch.logical_or(pred == target, target != 255).float()\n",
    "    iou = torch.sum(intersection) / torch.sum(union)\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def contrastive_loss(pred, target, margin=1.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss to ensure the depth map predictions are closer to the target.\n",
    "    \"\"\"\n",
    "    # Flatten the tensors for element-wise operations\n",
    "    pred_flat = pred.view(pred.size(0), -1)  # Flatten except for the batch dimension\n",
    "    target_flat = target.view(target.size(0), -1)  # Flatten except for the batch dimension\n",
    "\n",
    "    # Compute the pairwise distances\n",
    "    distances = torch.sqrt(torch.sum((pred_flat - target_flat) ** 2, dim=1))  # Batch-wise distances\n",
    "\n",
    "    # Create labels for contrastive loss\n",
    "    labels = (torch.abs(pred_flat - target_flat).mean(dim=1) < margin).float()  # Batch-wise labels\n",
    "\n",
    "    # Calculate contrastive loss\n",
    "    similar_loss = labels * distances**2\n",
    "    dissimilar_loss = (1 - labels) * torch.clamp(margin - distances, min=0)**2\n",
    "    loss = (similar_loss + dissimilar_loss).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def dice_loss(predictions, targets, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Dice Loss for segmentation.\n",
    "    Args:\n",
    "        predictions (torch.Tensor): The predicted segmentation map (logits or probabilities).\n",
    "                                    Shape: [batch_size, num_classes, height, width]\n",
    "        targets (torch.Tensor): The ground truth segmentation map (one-hot encoded or integer labels).\n",
    "                                Shape: [batch_size, height, width]\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    Returns:\n",
    "        torch.Tensor: Dice Loss (scalar).\n",
    "    \"\"\"\n",
    "    # Convert integer labels to one-hot if needed\n",
    "    if predictions.shape != targets.shape:\n",
    "        targets = F.one_hot(targets, num_classes=predictions.shape[1]).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    # Apply softmax to predictions for multi-class segmentation\n",
    "    predictions = torch.softmax(predictions, dim=1)\n",
    "    \n",
    "    # Flatten tensors to calculate intersection and union\n",
    "    predictions_flat = predictions.view(predictions.shape[0], predictions.shape[1], -1)\n",
    "    targets_flat = targets.view(targets.shape[0], targets.shape[1], -1)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = (predictions_flat * targets_flat).sum(dim=-1)\n",
    "    union = predictions_flat.sum(dim=-1) + targets_flat.sum(dim=-1)\n",
    "    \n",
    "    # Calculate Dice Coefficient\n",
    "    dice_coeff = (2 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # Dice Loss\n",
    "    return 1 - dice_coeff.mean()\n",
    "\n",
    "\n",
    "def initialize_optimizers_and_schedulers(model, lr_gen=1e-4, lr_disc=1e-4, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Initialize optimizers and schedulers for all generators and discriminators.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): MultiTaskModel instance.\n",
    "        lr_gen (float): Learning rate for generators.\n",
    "        lr_disc (float): Learning rate for discriminators.\n",
    "        weight_decay (float): Weight decay for optimizers.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Optimizers and schedulers for generators and discriminators.\n",
    "    \"\"\"\n",
    "    # Optimizers for shared generator\n",
    "    optimizer_shared_gen = torch.optim.AdamW(\n",
    "        model.feature_generator.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_shared_gen = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_shared_gen, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # Optimizer and scheduler for the shared generator's refinement layer\n",
    "    optimizer_shared_refine = torch.optim.AdamW(\n",
    "        model.shared_generator.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_shared_refine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_shared_refine, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # Optimizers and schedulers for task-specific generators\n",
    "    optimizer_seg_gen = torch.optim.AdamW(\n",
    "        model.seg_output_layer.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_seg_gen = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_seg_gen, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    optimizer_depth_gen = torch.optim.AdamW(\n",
    "        model.depth_output_layer.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_depth_gen = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_depth_gen, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    # Optimizers and schedulers for task-specific discriminators\n",
    "    optimizer_seg_disc = torch.optim.AdamW(\n",
    "        model.seg_discriminator.parameters(),\n",
    "        lr=lr_disc,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_seg_disc = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_seg_disc, step_size=20, gamma=0.1\n",
    "    )\n",
    "\n",
    "    optimizer_depth_disc = torch.optim.AdamW(\n",
    "        model.depth_discriminator.parameters(),\n",
    "        lr=lr_disc,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_depth_disc = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_depth_disc, step_size=20, gamma=0.1\n",
    "    )\n",
    "\n",
    "    # Optimizer and scheduler for the multi-task discriminator\n",
    "    optimizer_multi_task_disc = torch.optim.AdamW(\n",
    "        model.multi_task_discriminator.parameters(),\n",
    "        lr=lr_disc,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_multi_task_disc = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_multi_task_disc, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"optimizers\": {\n",
    "            \"shared_gen\": optimizer_shared_gen,\n",
    "            \"shared_refine\": optimizer_shared_refine,\n",
    "            \"seg_gen\": optimizer_seg_gen,\n",
    "            \"depth_gen\": optimizer_depth_gen,\n",
    "            \"seg_disc\": optimizer_seg_disc,\n",
    "            \"depth_disc\": optimizer_depth_disc,\n",
    "            \"multi_task_disc\": optimizer_multi_task_disc\n",
    "        },\n",
    "        \"schedulers\": {\n",
    "            \"shared_gen\": scheduler_shared_gen,\n",
    "            \"shared_refine\": scheduler_shared_refine,\n",
    "            \"seg_gen\": scheduler_seg_gen,\n",
    "            \"depth_gen\": scheduler_depth_gen,\n",
    "            \"seg_disc\": scheduler_seg_disc,\n",
    "            \"depth_disc\": scheduler_depth_disc,\n",
    "            \"multi_task_disc\": scheduler_multi_task_disc\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b91ad4-6f1c-4f07-b1ee-098fb0b0b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13bfe288-9700-45ec-8835-9795a3d51277",
   "metadata": {},
   "source": [
    "## MultiTaskModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb51d0-879f-40b1-9092-731e2d247b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70ad8610-be7e-460c-93f2-cb1f579d6b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MobileNetV3Backbone(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.proj_l1 = nn.Conv2d(16, 576, kernel_size=1, bias=False)   # For l1_out (1/4 resolution)\n",
    "        self.proj_l3 = nn.Conv2d(24, 576, kernel_size=1, bias=False)  # For l3_out (1/8 resolution)\n",
    "        self.proj_l7 = nn.Conv2d(48, 576, kernel_size=1, bias=False)  # For l7_out (1/16 resolution)\n",
    "        self.proj_l11 = nn.Conv2d(96, 576, kernel_size=1, bias=False) # For l11_out (1/32 resolution)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Passes input theough MobileNetV3 backbone feature extraction layers\n",
    "            layers to add connections to (0 indexed)\n",
    "                - 1:  1/4 res\n",
    "                - 3:  1/8 res\n",
    "                - 7, 8:  1/16 res\n",
    "                - 10, 11: 1/32 res\n",
    "           \"\"\"\n",
    "        # skips = nn.ParameterDict()\n",
    "        # for i in range(len(self.backbone) - 1):\n",
    "        #     x = self.backbone[i](x)\n",
    "        #     # add skip connection outputs\n",
    "        #     if i in [1, 3, 7, 11]:\n",
    "        #         skips.update({f\"l{i}_out\" : x})\n",
    "\n",
    "        # return skips\n",
    "        skips = {}  # Dictionary to store skip connections\n",
    "\n",
    "        for i, layer in enumerate(self.backbone):\n",
    "            x = layer(x)\n",
    "            # Add skip connections for specific layers\n",
    "            if i == 1:\n",
    "                skips[\"l1_out\"] = self.proj_l1(x)  # Project l1_out\n",
    "            elif i == 3:\n",
    "                skips[\"l3_out\"] = self.proj_l3(x)  # Project l3_out\n",
    "            elif i == 7:\n",
    "                skips[\"l7_out\"] = self.proj_l7(x)  # Project l7_out\n",
    "            elif i == 11:\n",
    "                skips[\"l11_out\"] = self.proj_l11(x)  # Project l11_out\n",
    "\n",
    "        return skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9bf6050-8f9c-439c-b125-bc1b9955a4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnhancedSharedGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Enhanced Shared Generator for Segmentation and Depth tasks.\n",
    "        Includes additional refinement layers for better generalization.\n",
    "        \"\"\"\n",
    "        super(EnhancedSharedGenerator, self).__init__()\n",
    "        # Shared convolution layers to process each skip connection\n",
    "        self.shared_conv1 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l11_out (1/32)\n",
    "        self.shared_conv2 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l7_out (1/16)\n",
    "        self.shared_conv3 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l3_out (1/8)\n",
    "        self.shared_conv4 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l1_out (1/4)\n",
    "\n",
    "        # CRP blocks for refinement\n",
    "        self.shared_crp1 = CRPBlock(256, 256, n_stages=4)\n",
    "        self.shared_crp2 = CRPBlock(256, 256, n_stages=4)\n",
    "        self.shared_crp3 = CRPBlock(256, 256, n_stages=4)\n",
    "        self.shared_crp4 = CRPBlock(256, 256, n_stages=4)\n",
    "\n",
    "        # Additional refinement layers\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, skips):\n",
    "        \"\"\"\n",
    "        Process skips with shared layers for task-specific generation.\n",
    "        Args:\n",
    "            skips (dict): Skip connections from the encoder.\n",
    "        Returns:\n",
    "            dict: Processed skip connections.\n",
    "        \"\"\"\n",
    "        x1 = self.shared_crp1(self.shared_conv1(skips[\"l11_out\"]))\n",
    "        x1 = self.refine(x1)  # Extra refinement\n",
    "\n",
    "        x2 = self.shared_crp2(self.shared_conv2(skips[\"l7_out\"]))\n",
    "        x2 = self.refine(x2)\n",
    "\n",
    "        x3 = self.shared_crp3(self.shared_conv3(skips[\"l3_out\"]))\n",
    "        x3 = self.refine(x3)\n",
    "\n",
    "        x4 = self.shared_crp4(self.shared_conv4(skips[\"l1_out\"]))\n",
    "        x4 = self.refine(x4)\n",
    "\n",
    "        return {\"x1\": x1, \"x2\": x2, \"x3\": x3, \"x4\": x4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2783c167-438d-4093-9b90-256011e88b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaskOutputLayer(nn.Module):\n",
    "    def __init__(self, output_channels):\n",
    "        \"\"\"\n",
    "        Task-specific output layers for generating final predictions.\n",
    "        Args:\n",
    "            output_channels (int): Number of output channels (e.g., 20 for segmentation, 1 for depth).\n",
    "        \"\"\"\n",
    "        super(TaskOutputLayer, self).__init__()\n",
    "        self.final_conv = nn.Conv2d(256, output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, input_size):\n",
    "        \"\"\"\n",
    "        Generate task-specific output.\n",
    "        Args:\n",
    "            x (Tensor): Input feature map.\n",
    "            input_size (tuple): Original input size (H, W).\n",
    "        Returns:\n",
    "            Tensor: Task-specific output.\n",
    "        \"\"\"\n",
    "        x = self.final_conv(x)\n",
    "        return nn.functional.interpolate(x, size=input_size, mode=\"bilinear\", align_corners=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac107db7-0257-45ec-b7af-e09ff552760d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaskSpecificDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(TaskSpecificDiscriminator, self).__init__()\n",
    "        self.adapt_conv = nn.Conv2d(input_channels+input_channels, input_channels, kernel_size=1, bias=False)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, task_output, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the discriminator.\n",
    "\n",
    "        Args:\n",
    "            task_output (Tensor): Output from the generator (e.g., seg_output or depth_output).\n",
    "            labels (Tensor, optional): Ground truth labels. If provided, aligns channels with task_output.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Discriminator's prediction.\n",
    "        \"\"\"\n",
    "        if labels is not None:\n",
    "            # Ensure labels match the shape of task_output\n",
    "            if labels.dim() < task_output.dim():\n",
    "                labels = labels.unsqueeze(1)  # Add channel dimension if needed\n",
    "            if labels.size(1) != task_output.size(1):\n",
    "                labels = torch.nn.functional.one_hot(labels.squeeze(1), num_classes=task_output.size(1))\n",
    "                labels = labels.permute(0, 3, 1, 2).float().to(task_output.device)\n",
    "            combined = torch.cat([task_output, labels], dim=1)\n",
    "            combined = self.adapt_conv(combined)\n",
    "        else:\n",
    "            combined = task_output\n",
    "\n",
    "        return self.model(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc64fce4-dfa5-4884-addd-2d46b50dba0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiTaskDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        \"\"\"\n",
    "        Multi-Task Discriminator for evaluating all task-specific outputs.\n",
    "        Args:\n",
    "            input_channels (int): Number of input channels for concatenated features and outputs.\n",
    "        \"\"\"\n",
    "        super(MultiTaskDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Evaluate input image and task-specific outputs.\n",
    "        Args:\n",
    "            inputs (Tensor): Input image.\n",
    "            outputs (list[Tensor]): List of task-specific outputs.\n",
    "        Returns:\n",
    "            Tensor: Discriminator output.\n",
    "        \"\"\"\n",
    "        # combined = torch.cat([inputs] + outputs, dim=1)\n",
    "        return self.model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd983afc-6f6f-4d21-ab3e-9d45f90e78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone, num_seg_classes=20, depth_channels=1):\n",
    "        \"\"\"\n",
    "        Multi-task model with shared Pix2Pix Generator, task-specific discriminators,\n",
    "        and a multi-task discriminator.\n",
    "        Args:\n",
    "            backbone (nn.Module): Encoder backbone for feature extraction.\n",
    "            num_seg_classes (int): Number of segmentation classes.\n",
    "            depth_channels (int): Number of output channels for depth.\n",
    "        \"\"\"\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.feature_generator = MobileNetV3Backbone(backbone)\n",
    "        self.shared_generator = EnhancedSharedGenerator()\n",
    "        self.seg_output_layer = TaskOutputLayer(output_channels=num_seg_classes)\n",
    "        self.depth_output_layer = TaskOutputLayer(output_channels=depth_channels)\n",
    "\n",
    "        # Task-specific discriminators\n",
    "        self.seg_discriminator = TaskSpecificDiscriminator(input_channels=num_seg_classes)\n",
    "        self.depth_discriminator = TaskSpecificDiscriminator(input_channels=depth_channels)\n",
    "\n",
    "        # Multi-task discriminator\n",
    "        self.multi_task_discriminator = MultiTaskDiscriminator(input_channels=3 + num_seg_classes + depth_channels)\n",
    "        \n",
    "    def forward(self, inputs, input_size, seg_labels=None, depth_labels=None, return_discriminator_outputs=False):\n",
    "        # Extract features from the encoder\n",
    "        skips = self.feature_generator(inputs)\n",
    "        shared_features = self.shared_generator(skips)\n",
    "\n",
    "        # Task-specific outputs\n",
    "        seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "        depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "        output_dict = {\n",
    "            \"seg_output\": seg_output,\n",
    "            \"depth_output\": depth_output,\n",
    "        }\n",
    "\n",
    "        if return_discriminator_outputs:\n",
    "            \n",
    "            # Detach outputs to prevent discriminator backward from interfering with the generator\n",
    "            seg_output_detached = seg_output.detach()\n",
    "            depth_output_detached = depth_output.detach()\n",
    "            \n",
    "            # Adversarial feedback from task-specific discriminators\n",
    "            seg_real_disc = self.seg_discriminator(seg_output_detached, seg_labels) if seg_labels is not None else None\n",
    "            seg_fake_disc = self.seg_discriminator(seg_output_detached, None)\n",
    "\n",
    "            depth_real_disc = self.depth_discriminator(depth_output_detached, depth_labels) if depth_labels is not None else None\n",
    "            depth_fake_disc = self.depth_discriminator(depth_output_detached, None)\n",
    "\n",
    "            # Multi-task discriminator feedback\n",
    "            combined_real_input = torch.cat([inputs, seg_labels, depth_labels], dim=1) if seg_labels is not None and depth_labels is not None else None\n",
    "            combined_fake_input = torch.cat([inputs, seg_output, depth_output], dim=1)\n",
    "\n",
    "            combined_real_disc = self.multi_task_discriminator(combined_real_input) if combined_real_input is not None else None\n",
    "            combined_fake_disc = self.multi_task_discriminator(combined_fake_input.detach())\n",
    "\n",
    "            output_dict.update({\n",
    "                \"seg_real_disc\": seg_real_disc,\n",
    "                \"seg_fake_disc\": seg_fake_disc,\n",
    "                \"depth_real_disc\": depth_real_disc,\n",
    "                \"depth_fake_disc\": depth_fake_disc,\n",
    "                \"combined_real_disc\": combined_real_disc,\n",
    "                \"combined_fake_disc\": combined_fake_disc,\n",
    "            })\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "#     def forward(self, inputs, input_size, seg_labels=None, depth_labels=None, return_discriminator_outputs=False):\n",
    "#         \"\"\"\n",
    "#         Forward pass for multi-task model.\n",
    "#         Args:\n",
    "#             inputs (Tensor): Input images.\n",
    "#             input_size (tuple): Original input size.\n",
    "#             seg_labels (Tensor, optional): Ground truth segmentation labels. Required for discriminator feedback.\n",
    "#             depth_labels (Tensor, optional): Ground truth depth labels. Required for discriminator feedback.\n",
    "#             return_discriminator_outputs (bool): If True, returns discriminator outputs for adversarial loss.\n",
    "#         Returns:\n",
    "#             dict: Outputs for segmentation and depth tasks, and optionally discriminator outputs.\n",
    "#         \"\"\"\n",
    "#         skips = self.feature_generator(inputs)\n",
    "#         shared_features = self.shared_generator(skips)\n",
    "\n",
    "#         # Task-specific outputs\n",
    "#         seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "#         depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "#         if return_discriminator_outputs:\n",
    "#             # Adversarial feedback from task-specific discriminators\n",
    "#             seg_real_disc = self.seg_discriminator(seg_output, seg_labels) if seg_labels is not None else None\n",
    "#             depth_real_disc = self.depth_discriminator(depth_output, depth_labels) if depth_labels is not None else None\n",
    "\n",
    "#             # Multi-task discriminator feedback\n",
    "#             combined_real_disc = self.multi_task_discriminator(inputs, [seg_output, depth_output])\n",
    "\n",
    "#             return {\n",
    "#                 \"seg_output\": seg_output,\n",
    "#                 \"depth_output\": depth_output,\n",
    "#                 \"seg_real_disc\": seg_real_disc,\n",
    "#                 \"depth_real_disc\": depth_real_disc,\n",
    "#                 \"combined_real_disc\": combined_real_disc\n",
    "#             }\n",
    "\n",
    "#         return {\n",
    "#             \"seg_output\": seg_output,\n",
    "#             \"depth_output\": depth_output\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc322fb-dbb7-4cd2-a934-33118521696b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b6e25-1715-412c-b906-714e85c0e266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dce8dd65-e71e-4d5b-86b0-a28e02fdaa23",
   "metadata": {},
   "source": [
    "# Saving loss charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce716570-bcca-40b6-91be-72d6713ff523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_all_losses(epoch, train_losses,valid_losses,save_dir):\n",
    "    # Plot training and validation losses\n",
    "    for key in train_losses.keys():\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses[key], label=f\"Train {key}\")\n",
    "        plt.plot(valid_losses[key], label=f\"Valid {key}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(key.replace(\"_\", \" \").title())\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, f\"{key}_loss_after_epoch_{epoch}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11165c9f-97c8-4240-9985-956b229c2656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb691baa-d9fc-4d44-9abb-305a026d5847",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "238ab35f-3458-488c-b170-a1def0a1b714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_with_adversarial_loss_tracking(\n",
    "    model, train_loader, valid_loader, num_epochs, device, opt_sched, save_dir=\"results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a multi-task model with adversarial feedback and tracks losses.\n",
    "    \n",
    "    Args:\n",
    "        model: Multi-task model with integrated generators and discriminators.\n",
    "        train_loader: DataLoader for training data.\n",
    "        valid_loader: DataLoader for validation data.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        device: Device for training (\"cuda\" or \"cpu\").\n",
    "        opt_sched: Dictionary of optimizers and schedulers for generators and discriminators.\n",
    "        save_dir: Directory to save results.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, valid_losses: Lists of losses for training and validation.\n",
    "    \"\"\"\n",
    "    # Create directories for saving results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = os.path.join(save_dir, timestamp)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare CSV file for loss tracking\n",
    "    csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "    gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "    \n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "            \"train_adv_loss\", \n",
    "            # \"train_seg_iou\",\n",
    "            \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "            \"valid_adv_loss\", \n",
    "            # \"valid_seg_iou\"\n",
    "        ])\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    best_combined_loss = float(\"inf\")\n",
    "    gif_frames =[]\n",
    "    perceptual_loss_fn = PerceptualLoss(pretrained_model=\"vgg16\").to(device)\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "        num_batches = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", unit=\"batch\") as pbar:\n",
    "            for batch in train_loader:\n",
    "                inputs, seg_labels, depth_labels = (\n",
    "                    batch[\"left\"].to(device),\n",
    "                    batch[\"mask\"].to(device),\n",
    "                    batch[\"depth\"].to(device),\n",
    "                )\n",
    "                input_size = inputs.size()[-2:]\n",
    "\n",
    "                # Preprocess seg_labels to one-hot encoding\n",
    "                if seg_labels.size(1) == 1:  # If class indices are given\n",
    "                    seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "                    seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)  # Convert to [B, C, H, W]\n",
    "\n",
    "                # Ensure depth_labels has correct dimensions\n",
    "                if depth_labels.dim() == 5:  # If depth_labels has extra dimensions\n",
    "                    depth_labels = depth_labels.squeeze(2)\n",
    "\n",
    "                # Zero gradients\n",
    "                for optimizer in opt_sched[\"optimizers\"].values():\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass with discriminator outputs\n",
    "                outputs = model(\n",
    "                    inputs,\n",
    "                    input_size=input_size,\n",
    "                    seg_labels=seg_labels,\n",
    "                    depth_labels=depth_labels,\n",
    "                    return_discriminator_outputs=True,\n",
    "                )\n",
    "\n",
    "                # Generator losses\n",
    "                seg_loss = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + \\\n",
    "                           dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "                depth_loss = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                             inv_huber_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                             depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "                \n",
    "                seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "                depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "                \n",
    "                seg_loss = seg_loss + 0.1 * seg_perceptual_loss\n",
    "                depth_loss = depth_loss + 0.1 * depth_perceptual_loss\n",
    "                \n",
    "\n",
    "                adv_loss = -(\n",
    "                    torch.mean(outputs[\"seg_real_disc\"]) +\n",
    "                    torch.mean(outputs[\"depth_real_disc\"]) +\n",
    "                    torch.mean(outputs[\"combined_real_disc\"])\n",
    "                )\n",
    "\n",
    "                combined_loss = seg_loss + depth_loss + 0.01 * adv_loss\n",
    "\n",
    "                # Backpropagation for generators\n",
    "                combined_loss.backward(retain_graph=True)\n",
    "                # opt_sched[\"optimizers\"][\"generator\"].step()\n",
    "                opt_sched[\"optimizers\"][\"shared_gen\"].step()\n",
    "                opt_sched[\"optimizers\"][\"shared_refine\"].step()\n",
    "                opt_sched[\"optimizers\"][\"seg_gen\"].step()\n",
    "                opt_sched[\"optimizers\"][\"depth_gen\"].step()\n",
    "\n",
    "\n",
    "                # Update task-specific discriminators\n",
    "                for task, disc_optimizer in [\n",
    "                    (\"seg\", \"seg_disc\"),\n",
    "                    (\"depth\", \"depth_disc\"),\n",
    "                ]:\n",
    "                    opt_sched[\"optimizers\"][disc_optimizer].zero_grad()\n",
    "                    real_disc_loss = torch.mean(\n",
    "                        (outputs[f\"{task}_real_disc\"] - 1) ** 2\n",
    "                    )\n",
    "                    fake_disc_loss = torch.mean(\n",
    "                        (outputs[f\"{task}_fake_disc\"].detach()) ** 2\n",
    "                    )\n",
    "                    disc_loss = (real_disc_loss + fake_disc_loss) / 2\n",
    "                    disc_loss.backward()\n",
    "                    opt_sched[\"optimizers\"][disc_optimizer].step()\n",
    "\n",
    "                # Update multi-task discriminator\n",
    "                opt_sched[\"optimizers\"][\"multi_task_disc\"].zero_grad()\n",
    "                real_combined_loss = torch.mean(\n",
    "                    (outputs[\"combined_real_disc\"] - 1) ** 2\n",
    "                )\n",
    "                fake_combined_loss = torch.mean(\n",
    "                    (outputs[\"combined_fake_disc\"].detach()) ** 2\n",
    "                )\n",
    "                combined_disc_loss = (real_combined_loss + fake_combined_loss) / 2\n",
    "                combined_disc_loss.backward()\n",
    "                opt_sched[\"optimizers\"][\"multi_task_disc\"].step()\n",
    "\n",
    "                # Update training metrics\n",
    "                epoch_train[\"seg\"] += seg_loss.item()\n",
    "                epoch_train[\"depth\"] += depth_loss.item()\n",
    "                epoch_train[\"combined\"] += combined_loss.item()\n",
    "                epoch_train[\"adv\"] += adv_loss.item()\n",
    "                # epoch_train[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "                num_batches += 1\n",
    "\n",
    "            # Average training metrics\n",
    "            for key in epoch_train.keys():\n",
    "                train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "            num_valid_batches = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    inputs, seg_labels, depth_labels = (\n",
    "                        batch[\"left\"].to(device),\n",
    "                        batch[\"mask\"].to(device),\n",
    "                        batch[\"depth\"].to(device),\n",
    "                    )\n",
    "                    input_size = inputs.size()[-2:]\n",
    "\n",
    "                    # Preprocess seg_labels to one-hot encoding\n",
    "                    if seg_labels.size(1) == 1:\n",
    "                        seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "                        seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "                    # Ensure depth_labels has correct dimensions\n",
    "                    if depth_labels.dim() == 5:\n",
    "                        depth_labels = depth_labels.squeeze(2)\n",
    "\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(\n",
    "                        inputs,\n",
    "                        input_size=input_size,\n",
    "                        seg_labels=seg_labels,\n",
    "                        depth_labels=depth_labels,\n",
    "                        return_discriminator_outputs=True,\n",
    "                    )\n",
    "\n",
    "                    # Validation loss calculations\n",
    "                    seg_loss = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + \\\n",
    "                               dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "                    depth_loss = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                                 inv_huber_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                                 depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "                    \n",
    "                    seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "                    depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "\n",
    "                    seg_loss = seg_loss + 0.1 * seg_perceptual_loss\n",
    "                    depth_loss = depth_loss + 0.1 * depth_perceptual_loss\n",
    "\n",
    "\n",
    "                    adv_loss = -(\n",
    "                        torch.mean(outputs[\"seg_real_disc\"]) +\n",
    "                        torch.mean(outputs[\"depth_real_disc\"]) +\n",
    "                        torch.mean(outputs[\"combined_real_disc\"])\n",
    "                    )\n",
    "\n",
    "                    combined_loss = seg_loss + depth_loss + 0.01 * adv_loss\n",
    "\n",
    "                    # Update validation metrics\n",
    "                    epoch_valid[\"seg\"] += seg_loss.item()\n",
    "                    epoch_valid[\"depth\"] += depth_loss.item()\n",
    "                    epoch_valid[\"combined\"] += combined_loss.item()\n",
    "                    epoch_valid[\"adv\"] += adv_loss.item()\n",
    "                    # epoch_valid[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "                    num_valid_batches += 1\n",
    "\n",
    "        # Average validation metrics\n",
    "        for key in epoch_valid.keys():\n",
    "            valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "        # Save best model\n",
    "        valid_combined_loss = epoch_valid[\"combined\"] / num_valid_batches\n",
    "        if valid_combined_loss < best_combined_loss:\n",
    "            best_combined_loss = valid_combined_loss\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "            print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "        frame = save_training_visualization_as_gif2(epoch, inputs, outputs[\"seg_output\"], outputs[\"depth_output\"], torch.argmax(seg_labels, dim=1), depth_labels)\n",
    "        gif_frames.append(frame)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Append metrics to CSV\n",
    "        with open(csv_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                epoch + 1,\n",
    "                epoch_train[\"seg\"] / num_batches,\n",
    "                epoch_train[\"depth\"] / num_batches,\n",
    "                epoch_train[\"combined\"] / num_batches,\n",
    "                epoch_train[\"adv\"] / num_batches,\n",
    "                # epoch_train[\"iou\"] / num_batches,\n",
    "                epoch_valid[\"seg\"] / num_valid_batches,\n",
    "                epoch_valid[\"depth\"] / num_valid_batches,\n",
    "                epoch_valid[\"combined\"] / num_valid_batches,\n",
    "                epoch_valid[\"adv\"] / num_valid_batches,\n",
    "                # epoch_valid[\"iou\"] / num_valid_batches,\n",
    "            ])\n",
    "            \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Results:\")\n",
    "\n",
    "        # Print training losses\n",
    "        print(f\"  Train Losses - Segmentation: {epoch_train['seg']/num_batches:.4f}, Depth: {epoch_train['depth']/num_batches:.4f}, \"\n",
    "              f\"Combined: {epoch_train['combined']/num_batches:.4f}, Adversarial: {epoch_train['adv']/num_batches:.4f}\")\n",
    "\n",
    "        # Print validation losses\n",
    "        print(f\"  Valid Losses - Segmentation: {epoch_valid['seg']/ num_valid_batches:.4f}, Depth: {epoch_valid['depth']/ num_valid_batches:.4f}, \"\n",
    "              f\"Combined: {epoch_valid['combined']/ num_valid_batches:.4f}, Adversarial: {epoch_valid['adv']/ num_valid_batches:.4f}\")\n",
    "\n",
    "\n",
    "        # Update schedulers\n",
    "        for name, scheduler in opt_sched[\"schedulers\"].items():\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                # Pass the appropriate metric to ReduceLROnPlateau\n",
    "                scheduler.step(valid_losses[\"combined\"][-1])  # Use the most recent validation combined loss\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "        if epoch %10 == 0:\n",
    "            gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "            gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "            plot_all_losses(epoch, train_losses,valid_losses,save_dir)\n",
    "            \n",
    "    \n",
    "    gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "    print(f\"Training visualization saved as GIF at {gif_path}\")\n",
    "\n",
    "    \n",
    "    return train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65280f26-3ba1-4290-b7f5-a7367903c170",
   "metadata": {},
   "source": [
    "# Visualize using torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a9b60b-8613-492d-b51f-7b3536c187a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torchviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dfaa3ba-62f9-4574-9cb6-ebddd23fb237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1285c2d-3d19-4088-ac9d-96182fdce9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskModel(\n",
       "  (feature_generator): MobileNetV3Backbone(\n",
       "    (backbone): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): Conv2dNormActivation(\n",
       "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "    )\n",
       "    (proj_l1): Conv2d(16, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (proj_l3): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (proj_l7): Conv2d(48, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (proj_l11): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (shared_generator): EnhancedSharedGenerator(\n",
       "    (shared_conv1): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (shared_conv2): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (shared_conv3): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (shared_conv4): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (shared_crp1): CRPBlock(\n",
       "      (mini_blocks): ModuleList(\n",
       "        (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (7): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (shared_crp2): CRPBlock(\n",
       "      (mini_blocks): ModuleList(\n",
       "        (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (7): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (shared_crp3): CRPBlock(\n",
       "      (mini_blocks): ModuleList(\n",
       "        (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (7): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (shared_crp4): CRPBlock(\n",
       "      (mini_blocks): ModuleList(\n",
       "        (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        (7): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (refine): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (seg_output_layer): TaskOutputLayer(\n",
       "    (final_conv): Conv2d(256, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (depth_output_layer): TaskOutputLayer(\n",
       "    (final_conv): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (seg_discriminator): TaskSpecificDiscriminator(\n",
       "    (adapt_conv): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(20, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (depth_discriminator): TaskSpecificDiscriminator(\n",
       "    (adapt_conv): Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (multi_task_discriminator): MultiTaskDiscriminator(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(24, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (8): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "mobilenet_backbone = mobilenet_v3_small(weights=\"IMAGENET1K_V1\").features\n",
    "model = MultiTaskModel(backbone=mobilenet_backbone, num_seg_classes=20, depth_channels=1)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd08d20-9810-4e51-a19d-037574573182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([224, 224])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy input tensor (adjust shape based on your model input requirements)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  # Example: batch_size=1, channels=3, height=224, width=224\n",
    "input_size = dummy_input.size()[-2:]\n",
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c69687a-aa86-48bf-905e-e15ccba728d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "output = model(dummy_input,input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eaf5bd33-0d8f-4b11-8095-c601df76ad97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model visualization saved as multitask_model_graph.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model\n",
    "dot = make_dot(output[\"seg_output\"], params=dict(model.named_parameters()))  # Adjust output key if necessary\n",
    "dot.format = \"png\"\n",
    "dot.render(\"multitask_model_graph.png\")  # This saves the graph as multitask_model_graph.png\n",
    "\n",
    "print(\"Model visualization saved as multitask_model_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9d0e194-61d1-4466-9d11-9dc9708bf2ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model visualization saved as multitask_model_graph2.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model\n",
    "dot = make_dot(output[\"seg_output\"], params=dict(model.named_parameters()), show_attrs=True, show_saved=True)  # Adjust output key if necessary\n",
    "dot.format = \"png\"\n",
    "dot.render(\"multitask_model_graph2.png\")  # This saves the graph as multitask_model_graph.png\n",
    "\n",
    "print(\"Model visualization saved as multitask_model_graph2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37c7fa7c-bca6-4b10-b38e-e8a489333fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model visualization saved as model_with_mobilenetv3_as_block_seg.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model\n",
    "dot = make_dot(output[\"seg_output\"], params=dict(model.named_parameters()), show_attrs=True, show_saved=True)  # Adjust output key if necessary\n",
    "dot.format = \"png\"\n",
    "dot.render(\"model_with_mobilenetv3_as_block_seg\")  # This saves the graph as multitask_model_graph.png\n",
    "\n",
    "print(\"Model visualization saved as model_with_mobilenetv3_as_block_seg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba507dec-4390-4caf-839f-7d03ea0fceb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model visualization saved as model_with_mobilenetv3_as_block_depth.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model\n",
    "dot = make_dot(output[\"depth_output\"], params=dict(model.named_parameters()), show_attrs=True, show_saved=True)  # Adjust output key if necessary\n",
    "dot.format = \"png\"\n",
    "dot.render(\"model_with_mobilenetv3_as_block_depth.png\")  # This saves the graph as multitask_model_graph.png\n",
    "\n",
    "print(\"Model visualization saved as model_with_mobilenetv3_as_block_depth.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7844cf9d-a0e6-479d-884a-bdc7714e2c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi_task_model.png'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Instantiate the model\n",
    "mobilenet_backbone = mobilenet_v3_small(weights=\"IMAGENET1K_V1\").features\n",
    "model = MultiTaskModel(backbone=mobilenet_backbone, num_seg_classes=20, depth_channels=1)\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(dummy_input, input_size=(224, 224))\n",
    "\n",
    "# Visualize the model\n",
    "dot = make_dot(outputs[\"seg_output\"], params=dict(model.named_parameters()))\n",
    "dot.format = \"png\"\n",
    "dot.render(\"multi_task_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2676843-8277-4a56-9cf1-a440cc8369f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ee32a-3b3d-41a8-a615-746ba5ba6f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04430878-c05d-448f-8c49-3a06205ae063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d1060-4501-45dc-a44d-4c6233579058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be398f-c3e8-4e8c-acbc-fdeb394b2c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2dac0-fa07-438b-85a9-98605593cb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c489a610-d431-4d64-87f0-acda7feaa63b",
   "metadata": {},
   "source": [
    "# running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aef8c474-94e3-4753-8718-9aa32bef85a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# Instantiate Models\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 30\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "mobilenet_backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "# encoder = MobileNetV3Backbone(mobilenet_backbone.features)\n",
    "model = MultiTaskModel(backbone=mobilenet_backbone.features, num_seg_classes=20, depth_channels=1)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Initialize optimizers and schedulers\n",
    "opt_sched = initialize_optimizers_and_schedulers(model)\n",
    "\n",
    "# Access optimizers\n",
    "optimizers = opt_sched[\"optimizers\"]\n",
    "schedulers = opt_sched[\"schedulers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586de327-ab6a-4a19-bb8b-c3ed685bda26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1f08f35-de94-4973-acd6-0ae47637544f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Training:   0%|          | 0/371 [08:56<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 1 with combined loss 2.4547\n",
      "Epoch 1/30 Results:\n",
      "  Train Losses - Segmentation: 2.4241, Depth: 0.0622, Combined: 2.4565, Adversarial: -2.9767\n",
      "  Valid Losses - Segmentation: 2.3414, Depth: 0.1430, Combined: 2.4547, Adversarial: -2.9770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 2 with combined loss 2.2811\n",
      "Epoch 2/30 Results:\n",
      "  Train Losses - Segmentation: 2.2065, Depth: 0.0461, Combined: 2.2226, Adversarial: -2.9956\n",
      "  Valid Losses - Segmentation: 2.2700, Depth: 0.0409, Combined: 2.2811, Adversarial: -2.9745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 3 with combined loss 2.2520\n",
      "Epoch 3/30 Results:\n",
      "  Train Losses - Segmentation: 2.1411, Depth: 0.0424, Combined: 2.1535, Adversarial: -2.9968\n",
      "  Valid Losses - Segmentation: 2.2153, Depth: 0.0670, Combined: 2.2520, Adversarial: -3.0336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 4 with combined loss 2.0698\n",
      "Epoch 4/30 Results:\n",
      "  Train Losses - Segmentation: 2.0640, Depth: 0.0398, Combined: 2.0739, Adversarial: -2.9975\n",
      "  Valid Losses - Segmentation: 2.0565, Depth: 0.0431, Combined: 2.0698, Adversarial: -2.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - Training:   0%|          | 0/371 [08:06<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 Results:\n",
      "  Train Losses - Segmentation: 2.0353, Depth: 0.0392, Combined: 2.0445, Adversarial: -2.9978\n",
      "  Valid Losses - Segmentation: 2.0531, Depth: 0.0663, Combined: 2.0896, Adversarial: -2.9907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 Results:\n",
      "  Train Losses - Segmentation: 2.0003, Depth: 0.0376, Combined: 2.0080, Adversarial: -2.9981\n",
      "  Valid Losses - Segmentation: 2.1355, Depth: 0.0628, Combined: 2.1683, Adversarial: -2.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 7 with combined loss 2.0074\n",
      "Epoch 7/30 Results:\n",
      "  Train Losses - Segmentation: 1.9766, Depth: 0.0361, Combined: 1.9827, Adversarial: -2.9980\n",
      "  Valid Losses - Segmentation: 1.9935, Depth: 0.0438, Combined: 2.0074, Adversarial: -2.9877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 Results:\n",
      "  Train Losses - Segmentation: 1.9690, Depth: 0.0366, Combined: 1.9757, Adversarial: -2.9984\n",
      "  Valid Losses - Segmentation: 1.9993, Depth: 0.0763, Combined: 2.0457, Adversarial: -2.9892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 9 with combined loss 1.9606\n",
      "Epoch 9/30 Results:\n",
      "  Train Losses - Segmentation: 1.9558, Depth: 0.0352, Combined: 1.9610, Adversarial: -2.9984\n",
      "  Valid Losses - Segmentation: 1.9211, Depth: 0.0696, Combined: 1.9606, Adversarial: -3.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - Training:   0%|          | 0/371 [08:11<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 Results:\n",
      "  Train Losses - Segmentation: 1.9284, Depth: 0.0344, Combined: 1.9328, Adversarial: -2.9985\n",
      "  Valid Losses - Segmentation: 2.0639, Depth: 0.0570, Combined: 2.0910, Adversarial: -2.9877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 Results:\n",
      "  Train Losses - Segmentation: 1.9304, Depth: 0.0344, Combined: 1.9348, Adversarial: -2.9987\n",
      "  Valid Losses - Segmentation: 2.0771, Depth: 0.0677, Combined: 2.1145, Adversarial: -3.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 Results:\n",
      "  Train Losses - Segmentation: 1.9135, Depth: 0.0354, Combined: 1.9189, Adversarial: -2.9988\n",
      "  Valid Losses - Segmentation: 1.9600, Depth: 0.0468, Combined: 1.9768, Adversarial: -3.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 Results:\n",
      "  Train Losses - Segmentation: 1.8651, Depth: 0.0335, Combined: 1.8686, Adversarial: -2.9989\n",
      "  Valid Losses - Segmentation: 1.9520, Depth: 0.0414, Combined: 1.9635, Adversarial: -2.9953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 Results:\n",
      "  Train Losses - Segmentation: 1.8722, Depth: 0.0327, Combined: 1.8749, Adversarial: -2.9990\n",
      "  Valid Losses - Segmentation: 1.9996, Depth: 0.0552, Combined: 2.0248, Adversarial: -2.9971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 Results:\n",
      "  Train Losses - Segmentation: 1.8755, Depth: 0.0331, Combined: 1.8787, Adversarial: -2.9991\n",
      "  Valid Losses - Segmentation: 1.9893, Depth: 0.0478, Combined: 2.0071, Adversarial: -3.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - Training:   0%|          | 0/371 [08:06<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 Results:\n",
      "  Train Losses - Segmentation: 1.8393, Depth: 0.0318, Combined: 1.8411, Adversarial: -2.9991\n",
      "  Valid Losses - Segmentation: 2.0337, Depth: 0.0464, Combined: 2.0501, Adversarial: -3.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 17 with combined loss 1.9592\n",
      "Epoch 17/30 Results:\n",
      "  Train Losses - Segmentation: 1.8255, Depth: 0.0317, Combined: 1.8272, Adversarial: -2.9993\n",
      "  Valid Losses - Segmentation: 1.9454, Depth: 0.0436, Combined: 1.9592, Adversarial: -2.9847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 Results:\n",
      "  Train Losses - Segmentation: 1.8452, Depth: 0.0313, Combined: 1.8464, Adversarial: -2.9992\n",
      "  Valid Losses - Segmentation: 2.0587, Depth: 0.0494, Combined: 2.0780, Adversarial: -3.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 19 with combined loss 1.8837\n",
      "Epoch 19/30 Results:\n",
      "  Train Losses - Segmentation: 1.8174, Depth: 0.0309, Combined: 1.8183, Adversarial: -2.9993\n",
      "  Valid Losses - Segmentation: 1.8717, Depth: 0.0418, Combined: 1.8837, Adversarial: -2.9841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - Training:   0%|          | 0/371 [08:06<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 Results:\n",
      "  Train Losses - Segmentation: 1.8296, Depth: 0.0316, Combined: 1.8312, Adversarial: -2.9994\n",
      "  Valid Losses - Segmentation: 2.0162, Depth: 0.0428, Combined: 2.0291, Adversarial: -2.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 Results:\n",
      "  Train Losses - Segmentation: 1.8039, Depth: 0.0301, Combined: 1.8040, Adversarial: -2.9994\n",
      "  Valid Losses - Segmentation: 1.9842, Depth: 0.0625, Combined: 2.0167, Adversarial: -2.9956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 22 with combined loss 1.8619\n",
      "Epoch 22/30 Results:\n",
      "  Train Losses - Segmentation: 1.7949, Depth: 0.0302, Combined: 1.7950, Adversarial: -2.9995\n",
      "  Valid Losses - Segmentation: 1.8484, Depth: 0.0435, Combined: 1.8619, Adversarial: -2.9881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - Training:   0%|          | 0/371 [08:07<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 Results:\n",
      "  Train Losses - Segmentation: 1.7947, Depth: 0.0294, Combined: 1.7942, Adversarial: -2.9995\n",
      "  Valid Losses - Segmentation: 1.9298, Depth: 0.0499, Combined: 1.9496, Adversarial: -3.0075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 Results:\n",
      "  Train Losses - Segmentation: 1.7751, Depth: 0.0295, Combined: 1.7746, Adversarial: -2.9996\n",
      "  Valid Losses - Segmentation: 1.9406, Depth: 0.0457, Combined: 1.9563, Adversarial: -3.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 Results:\n",
      "  Train Losses - Segmentation: 1.7948, Depth: 0.0300, Combined: 1.7948, Adversarial: -2.9996\n",
      "  Valid Losses - Segmentation: 1.9196, Depth: 0.0580, Combined: 1.9476, Adversarial: -2.9954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 Results:\n",
      "  Train Losses - Segmentation: 1.7758, Depth: 0.0295, Combined: 1.7752, Adversarial: -2.9996\n",
      "  Valid Losses - Segmentation: 1.8867, Depth: 0.0347, Combined: 1.8913, Adversarial: -2.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 Results:\n",
      "  Train Losses - Segmentation: 1.7570, Depth: 0.0295, Combined: 1.7565, Adversarial: -2.9996\n",
      "  Valid Losses - Segmentation: 1.8660, Depth: 0.0414, Combined: 1.8773, Adversarial: -3.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - Training:   0%|          | 0/371 [08:09<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 Results:\n",
      "  Train Losses - Segmentation: 1.7395, Depth: 0.0284, Combined: 1.7379, Adversarial: -2.9997\n",
      "  Valid Losses - Segmentation: 1.9218, Depth: 0.0453, Combined: 1.9371, Adversarial: -3.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - Training:   0%|          | 0/371 [08:08<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 Results:\n",
      "  Train Losses - Segmentation: 1.7425, Depth: 0.0278, Combined: 1.7403, Adversarial: -2.9997\n",
      "  Valid Losses - Segmentation: 1.8592, Depth: 0.0434, Combined: 1.8727, Adversarial: -2.9902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - Training:   0%|          | 0/371 [08:06<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 Results:\n",
      "  Train Losses - Segmentation: 1.7296, Depth: 0.0284, Combined: 1.7280, Adversarial: -2.9997\n",
      "  Valid Losses - Segmentation: 1.9274, Depth: 0.0409, Combined: 1.9383, Adversarial: -3.0075\n",
      "Training visualization saved as GIF at results_test8_final/20241127_132230/training_visualization_20241127_132230.gif\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data Loaders (Ensure train_loader and valid_loader are ready)\n",
    "train_losses, valid_losses = train_model_with_adversarial_loss_tracking(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    num_epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    opt_sched=opt_sched,\n",
    "    save_dir=\"results_test8_final\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd9190-d327-476b-a992-0f3d1a86f805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f871a15-59f2-4357-a525-263a4012dff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78220e94-129e-457c-8865-ecfa98445b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ed5e4-2134-4aee-8a93-4aa65cb10b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64331738-e822-49ee-a26a-71b5a36bd9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfffa11-a02d-4156-b088-88a3f4830818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b24c6-0d86-4c55-9e9f-51782eb1b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with perpetual loss later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec784a8-16ae-4ce3-bfee-c676246a3c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# from datetime import datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.utils import save_image\n",
    "\n",
    "# def train_model_with_loss_tracking(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, opt_sched, save_dir=\"results\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Trains a multi-task model with Conditional GANs, structural consistency, and perceptual loss.\n",
    "\n",
    "#     Args:\n",
    "#         model: The multi-task model to train.\n",
    "#         train_loader: DataLoader for training data.\n",
    "#         valid_loader: DataLoader for validation data.\n",
    "#         num_epochs: Number of epochs to train.\n",
    "#         device: Device for training (\"cuda\" or \"cpu\").\n",
    "#         opt_sched: Dictionary of optimizers and schedulers.\n",
    "#         save_dir: Directory to save results.\n",
    "\n",
    "#     Returns:\n",
    "#         train_losses, valid_losses: Lists of losses for training and validation.\n",
    "#     \"\"\"\n",
    "#     # Create directories for saving results\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     # Prepare CSV file\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_smooth\", \"train_seg_iou\", \"train_seg_perceptual_loss\",\n",
    "#             \"train_depth_perceptual_loss\", \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_smooth\", \"valid_seg_iou\", \"valid_seg_perceptual_loss\",\n",
    "#             \"valid_depth_perceptual_loss\"\n",
    "#         ])\n",
    "\n",
    "#     # Initialize tracking variables\n",
    "#     train_losses = {\n",
    "#         \"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": [],\n",
    "#         \"seg_perceptual\": [], \"depth_perceptual\": []\n",
    "#     }\n",
    "#     valid_losses = {\n",
    "#         \"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": [],\n",
    "#         \"seg_perceptual\": [], \"depth_perceptual\": []\n",
    "#     }\n",
    "#     best_combined_loss = float(\"inf\")\n",
    "#     gif_frames = []\n",
    "\n",
    "#     # Perceptual Loss (example using VGG features)\n",
    "#     perceptual_loss_fn = PerceptualLoss(pretrained_model=\"vgg16\").to(device)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = (\n",
    "#                 batch[\"left\"].to(device),\n",
    "#                 batch[\"mask\"].to(device),\n",
    "#                 batch[\"depth\"].to(device)\n",
    "#             )\n",
    "#             latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#             # Zero gradients\n",
    "#             for optimizer in opt_sched[\"optimizers\"].values():\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(inputs, input_size=inputs.size()[-2:])\n",
    "\n",
    "#             # Loss calculations\n",
    "#             seg_loss_task = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "#             seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "#             seg_loss = seg_loss_task + 0.1 * seg_perceptual_loss\n",
    "\n",
    "#             depth_loss_sidl = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#             depth_loss_huber = inv_huber_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#             depth_loss_smooth = depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "#             depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "#             depth_loss = depth_loss_sidl + depth_loss_huber + depth_loss_smooth + 0.1 * depth_perceptual_loss\n",
    "\n",
    "#             combined_loss = seg_loss + depth_loss\n",
    "\n",
    "#             # Backpropagation\n",
    "#             combined_loss.backward()\n",
    "#             for optimizer in opt_sched[\"optimizers\"].values():\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             # Update training metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += depth_loss.item()\n",
    "#             epoch_train[\"combined\"] += combined_loss.item()\n",
    "#             epoch_train[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_loss_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_loss_smooth.item()\n",
    "#             epoch_train[\"seg_perceptual\"] += seg_perceptual_loss.item()\n",
    "#             epoch_train[\"depth_perceptual\"] += depth_perceptual_loss.item()\n",
    "#             num_batches += 1\n",
    "\n",
    "#         # Average training metrics\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         # Validation loop\n",
    "#         model.eval()\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 inputs, seg_labels, depth_labels = (\n",
    "#                     batch[\"left\"].to(device),\n",
    "#                     batch[\"mask\"].to(device),\n",
    "#                     batch[\"depth\"].to(device)\n",
    "#                 )\n",
    "#                 latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(inputs, input_size=inputs.size()[-2:])\n",
    "\n",
    "#                 # Validation loss calculations\n",
    "#                 seg_loss_task = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "#                 seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "#                 seg_loss = seg_loss_task + 0.1 * seg_perceptual_loss\n",
    "\n",
    "#                 depth_loss_sidl = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#                 depth_loss_huber = inv_huber_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#                 depth_loss_smooth = depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "#                 depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "#                 depth_loss = depth_loss_sidl + depth_loss_huber + depth_loss_smooth + 0.1 * depth_perceptual_loss\n",
    "\n",
    "#                 combined_loss = seg_loss + depth_loss\n",
    "\n",
    "#                 # Update validation metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += depth_loss.item()\n",
    "#                 epoch_valid[\"combined\"] += combined_loss.item()\n",
    "#                 epoch_valid[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_loss_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_loss_smooth.item()\n",
    "#                 epoch_valid[\"seg_perceptual\"] += seg_perceptual_loss.item()\n",
    "#                 epoch_valid[\"depth_perceptual\"] += depth_perceptual_loss.item()\n",
    "#                 num_valid_batches += 1\n",
    "\n",
    "#         # Average validation metrics\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "#         # Save best model\n",
    "#         valid_combined_loss = epoch_valid[\"combined\"] / num_valid_batches\n",
    "#         if valid_combined_loss < best_combined_loss:\n",
    "#             best_combined_loss = valid_combined_loss\n",
    "#             torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "\n",
    "#         # Append metrics to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 epoch_train[\"seg\"] / num_batches,\n",
    "#                 epoch_train[\"depth\"] / num_batches,\n",
    "#                 epoch_train[\"combined\"] / num_batches,\n",
    "#                 epoch_train[\"depth_sidl\"] / num_batches,\n",
    "#                 epoch_train[\"depth_smooth\"] / num_batches,\n",
    "#                 epoch_train[\"iou\"] / num_batches,\n",
    "#                 epoch_train[\"seg_perceptual\"] / num_batches,\n",
    "#                 epoch_train[\"depth_perceptual\"] / num_batches,\n",
    "#                 epoch_valid[\"seg\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"combined\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_sidl\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_smooth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"iou\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"seg_perceptual\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_perceptual\"] / num_valid_batches,\n",
    "#             ])\n",
    "\n",
    "#         # Update schedulers\n",
    "#         for scheduler in opt_sched[\"schedulers\"].values():\n",
    "#             scheduler.step()\n",
    "            \n",
    "#     plot_all_losses(train_losses,valid_losses)\n",
    "\n",
    "    \n",
    "\n",
    "#     return train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1f745-2964-4cc1-8211-49fcf03d216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from datetime import datetime\n",
    "# from torchvision.utils import save_image\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Updated Train Function\n",
    "# def train_model_with_loss_tracking_and_gif(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, save_dir=\"training_output_bicycle_and_pix2pix\"\n",
    "# ):\n",
    "#     # Create directories for saving models and outputs\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "\n",
    "#     # Initialize CSV for saving loss data\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_smooth\", \"train_seg_iou\",\n",
    "#             \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_smooth\", \"valid_seg_iou\"\n",
    "#         ])\n",
    "\n",
    "#     best_combined_loss = float(\"inf\")  # Initialize best combined loss for saving the best model\n",
    "#     train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     gif_frames = []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         model.train()\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         # Training Loop\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "#             latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#             model.optimizer_stage1.zero_grad()\n",
    "#             model.optimizer_stage2.zero_grad()\n",
    "\n",
    "#             # Forward Pass\n",
    "#             outputs = model(inputs, seg_labels, depth_labels, latent_noise)\n",
    "#             seg_output = outputs[\"seg_output\"]\n",
    "#             depth_output = outputs[\"depth_output\"]\n",
    "\n",
    "#             # Loss Calculations\n",
    "#             seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels.squeeze(1))\n",
    "#             seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#             seg_iou = mean_iou(seg_output, seg_labels, num_classes=20)\n",
    "#             seg_loss_total = 0.6 * seg_loss + 0.4 * seg_dice\n",
    "\n",
    "#             depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#             depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#             depth_loss_total = depth_sidl + depth_smooth\n",
    "\n",
    "#             # Combined Loss\n",
    "#             total_loss = seg_loss_total + depth_loss_total\n",
    "#             total_loss.backward()\n",
    "\n",
    "#             # Optimizers Step\n",
    "#             model.optimizer_stage1.step()\n",
    "#             model.optimizer_stage2.step()\n",
    "\n",
    "#             # Accumulate Training Metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += depth_loss_total.item()\n",
    "#             epoch_train[\"combined\"] += total_loss.item()\n",
    "#             epoch_train[\"iou\"] += seg_iou.item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_smooth.item()\n",
    "#             num_batches += 1\n",
    "\n",
    "#             # Save training images for visualization\n",
    "#             if num_batches % 10 == 0:\n",
    "#                 img_grid = torch.cat([inputs[0], seg_output[0].argmax(0, keepdim=True), depth_output[0]], dim=2)\n",
    "#                 save_image(img_grid, os.path.join(save_dir, f\"train_{epoch}_{num_batches}.png\"))\n",
    "#                 gif_frames.append(Image.open(os.path.join(save_dir, f\"train_{epoch}_{num_batches}.png\")))\n",
    "\n",
    "#         model.scheduler_stage1.step()\n",
    "#         model.scheduler_stage2.step(epoch_train[\"combined\"] / num_batches)\n",
    "\n",
    "#         # Average Training Losses\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "#                 latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#                 outputs = model(inputs, seg_labels, depth_labels, latent_noise)\n",
    "#                 seg_output = outputs[\"seg_output\"]\n",
    "#                 depth_output = outputs[\"depth_output\"]\n",
    "\n",
    "#                 # Validation Loss Calculations\n",
    "#                 seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels.squeeze(1))\n",
    "#                 seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#                 seg_iou = mean_iou(seg_output, seg_labels, num_classes=20)\n",
    "#                 seg_loss_total = 0.6 * seg_loss + 0.4 * seg_dice\n",
    "\n",
    "#                 depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#                 depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#                 depth_loss_total = depth_sidl + depth_smooth\n",
    "\n",
    "#                 # Accumulate Validation Metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += depth_loss_total.item()\n",
    "#                 epoch_valid[\"combined\"] += (seg_loss_total + depth_loss_total).item()\n",
    "#                 epoch_valid[\"iou\"] += seg_iou.item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_smooth.item()\n",
    "#                 num_valid_batches += 1\n",
    "            \n",
    "#             frame = save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels)\n",
    "#             gif_frames.append(frame)\n",
    "              \n",
    "\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "#         # Save Model if Validation Loss Improves\n",
    "#         valid_combined_loss = epoch_valid[\"combined\"] / num_valid_batches\n",
    "#         if valid_combined_loss < best_combined_loss:\n",
    "#             best_combined_loss = valid_combined_loss\n",
    "#             torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "#             print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "#         if epoch%10==0:\n",
    "#             gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "#             gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "            \n",
    "\n",
    "#         # Append Validation Metrics to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 epoch_train[\"seg\"] / num_batches,\n",
    "#                 epoch_train[\"depth\"] / num_batches,\n",
    "#                 epoch_train[\"combined\"] / num_batches,\n",
    "#                 epoch_train[\"depth_sidl\"] / num_batches,\n",
    "#                 epoch_train[\"depth_smooth\"] / num_batches,\n",
    "#                 epoch_train[\"iou\"] / num_batches,\n",
    "#                 epoch_valid[\"seg\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"combined\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_sidl\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_smooth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"iou\"] / num_valid_batches,\n",
    "#             ])\n",
    "\n",
    "#     # Save GIF\n",
    "#     gif_frames[0].save(\n",
    "#         gif_path, save_all=True, append_images=gif_frames[1:], duration=200, loop=0\n",
    "#     )\n",
    "\n",
    "#     return train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf71573-fe0d-4309-beb5-7e12adf7cbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c01b5-8405-467c-ba4f-731f05cef85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce4c3f7b-a9d8-45e3-8ca2-288c1734d22f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def train_model_with_loss_tracking_and_gif(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, save_dir=\"training_output_bicycle_and_pix2pix\"):\n",
    "#     # Create directory for saving models and outputs\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "\n",
    "#     # Initialize CSV for saving loss data\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_inv_huber\", \"train_depth_contrastive\", \"train_depth_smooth\",\n",
    "#             \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_inv_huber\", \"valid_depth_contrastive\", \"valid_depth_smooth\"\n",
    "#         ])\n",
    "\n",
    "#     best_combined_loss = float(\"inf\")  # Initialize best combined loss for saving the best model\n",
    "\n",
    "#     train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     # , \"depth_inv_huber\": [], \"depth_contrastive\": []\n",
    "\n",
    "#     gif_frames = []\n",
    "#     num_classes = 20\n",
    "#     # Optimizer for latent noise\n",
    "    \n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#         model.train()\n",
    "#         # epoch_train_seg_loss = 0\n",
    "#         # epoch_train_depth_loss = 0\n",
    "#         # epoch_train_iou = 0\n",
    "#         # epoch_train_combined_loss = 0\n",
    "#         # epoch_train_depth_sidl = 0\n",
    "#         # epoch_train_depth_inv_huber = 0\n",
    "#         # epoch_train_depth_contrastive = 0\n",
    "#         # epoch_train_depth_smooth = 0\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         reconstruction_layer = nn.Conv2d(256, 3, kernel_size=1).to(device)\n",
    "        \n",
    "#         # scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#         # Training Loop\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "#             print(inputs.shape,seg_labels.shape,depth_labels.shape) # torch.Size([8, 3, 200, 512]) torch.Size([8, 1, 200, 512]) torch.Size([8, 1, 1, 200, 512])\n",
    "#             return\n",
    "        \n",
    "                       \n",
    "\n",
    "\n",
    "#             # Forward pass\n",
    "#             seg_output, depth_output, backbone_features = model(...)\n",
    "            \n",
    "\n",
    "\n",
    "#             seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#             seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#             seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#             seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "            \n",
    "#             depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#             depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#             depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#             depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "            \n",
    "           \n",
    "           \n",
    "#             # Combined Loss\n",
    "#             total_loss = bicycle_loss + pix2pix_total_loss\n",
    "\n",
    "#             # Single backward pass\n",
    "#             total_loss.backward()\n",
    "\n",
    "#             # Update both optimizers\n",
    "#             model.optimizer_stage1.step()\n",
    "#             model.optimizer_stage2.step()\n",
    "#             latent_optimizer.step()\n",
    "\n",
    "#             # Accumulate Training Metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#             epoch_train[\"combined\"] += total_loss.item()\n",
    "#             epoch_train[\"iou\"] += seg_iou.item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_smooth.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#         model.scheduler_stage1.step()\n",
    "#         model.scheduler_stage2.step(epoch_train[\"combined\"]/num_batches)\n",
    "\n",
    "\n",
    "#         # Average Training Losses\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Train Seg Loss: {epoch_train['seg']:.4f}, \"\n",
    "#             f\"Train Depth Loss: {epoch_train['depth']:.4f}, Train Combined Loss: {epoch_train['combined']:.4f}, \"\n",
    "#             f\"Train mIOU: {epoch_train['iou']:.4f}, Train sidl Loss: {epoch_train['depth_sidl']:.4f}, \"\n",
    "#             f\"Train depth smooth: {epoch_train['depth_smooth']:.4f}\"\n",
    "#     )       \n",
    "\n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 # print(\"inside valid\")\n",
    "#                 inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "\n",
    "#                 # Ensure depth_labels and segmentation labels have correct dimensions\n",
    "                \n",
    "\n",
    "               \n",
    "\n",
    "\n",
    "#                 seg_output_old =seg_output\n",
    "#                 # Resize seg_output to match the spatial dimensions of seg_labels\n",
    "#                 seg_output_resized = F.interpolate(seg_output, size=seg_labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "#                 seg_output = seg_output_resized\n",
    "\n",
    "#                 depth_output_old = depth_output\n",
    "#                 depth_output_resized = F.interpolate(depth_output, size=depth_labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "#                 depth_output =depth_output_resized\n",
    "\n",
    "\n",
    "#                 # Segmentation Loss\n",
    "#                 seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#                 seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#                 seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#                 seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "                \n",
    "#                 depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#                 depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#                 depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#                 depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "\n",
    "#                 pix2pix_loss = seg_loss_total + depth_loss_total\n",
    "\n",
    "#                 # Combined Validation Loss\n",
    "#                 combined_loss = pix2pix_loss\n",
    "\n",
    "#                 # Accumulate Validation Metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#                 epoch_valid[\"combined\"] += combined_loss.item()\n",
    "#                 epoch_valid[\"iou\"] += seg_iou.item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_smooth.item()\n",
    "                \n",
    "#                 num_valid_batches += 1\n",
    "                \n",
    "#             frame = save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels)\n",
    "#             gif_frames.append(frame)\n",
    "                \n",
    "                \n",
    "#         # Calculate epoch averages\n",
    "#         # Average Validation Losses\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Valid Seg Loss: {epoch_valid['seg']:.4f}, \"\n",
    "#             f\"Valid Depth Loss: {epoch_valid['depth']:.4f}, Valid Combined Loss: {epoch_valid['combined']:.4f}, \"\n",
    "#             f\"Valid mIOU: {epoch_valid['iou']:.4f}, Valid sidl Loss: {epoch_valid['depth_sidl']:.4f}, \"\n",
    "#             f\"Valid depth smooth: {epoch_valid['depth_smooth']:.4f}\"\n",
    "#         )\n",
    "\n",
    "#         # Write the losses to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 train_losses[\"seg\"], train_losses[\"depth\"], train_losses[\"combined\"],\n",
    "#                 train_losses[\"depth_sidl\"], 0,0,\n",
    "#                 # avg_train_depth_inv_huber, avg_train_depth_contrastive,\n",
    "#                 train_losses[\"depth_smooth\"],\n",
    "#                 valid_losses[\"seg\"], valid_losses[\"depth\"], valid_losses['combined'],\n",
    "#                 valid_losses[\"depth_sidl\"],0,0,\n",
    "#                 # avg_valid_depth_inv_huber, avg_valid_depth_contrastive, \n",
    "#                 valid_losses[\"depth_smooth\"]\n",
    "#             ])\n",
    "\n",
    "       \n",
    "#         # Save best model\n",
    "#         if valid_losses[\"combined\"][-1] < best_combined_loss:\n",
    "#             best_combined_loss = valid_losses[\"combined\"][-1]\n",
    "#             torch.save(model, os.path.join(save_dir, \"best_model_resnetBackbone.pth\"))\n",
    "#             print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "#         if epoch%10==0:\n",
    "#             gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "#             gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "    \n",
    "    \n",
    "#     plot_loss(train_losses, valid_losses, save_dir)\n",
    "#     gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return train_losses,valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28f06b7e-8a7c-40d1-9ff2-d5bc2219bc74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Create your model instance\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = 'cpu'\n",
    "# model = MultiTaskModel(num_seg_classes=20, feature_channels=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6a0e2e4-241b-4096-ab67-41abda0c3a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Set the number of epochs\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Call the training function\n",
    "# train_losses, valid_losses = train_model_with_loss_tracking_and_gif(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     valid_loader=valid_loader,\n",
    "#     num_epochs=num_epochs,\n",
    "#     device=device,\n",
    "#     save_dir=\"test7_res\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c51a6d-a75d-429c-aef9-8e547a5fe8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789b4aa-a31c-4298-ae2a-c8c13c30da39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d70c33-2426-4327-9b66-208e8c419e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75f3e6-0e5b-4748-a1d0-718357fc8092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3db3d8-d07c-4a04-8615-1f6a59f5d1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e599ee-b582-4b1f-aa67-3b756e18c23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c1039-9a3d-434c-8585-ded47ce50261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720176f-f280-452e-8809-5274dc540273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876589d-4937-4435-ac84-cc235e43a99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64d7f8b2-0558-4745-9647-e6c65811861a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def train_model_with_loss_tracking_and_gif(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, save_dir=\"training_output_bicycle_and_pix2pix\"):\n",
    "#     # Create directory for saving models and outputs\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "\n",
    "#     # Initialize CSV for saving loss data\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_inv_huber\", \"train_depth_contrastive\", \"train_depth_smooth\",\n",
    "#             \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_inv_huber\", \"valid_depth_contrastive\", \"valid_depth_smooth\"\n",
    "#         ])\n",
    "\n",
    "#     best_combined_loss = float(\"inf\")  # Initialize best combined loss for saving the best model\n",
    "\n",
    "#     train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     # , \"depth_inv_huber\": [], \"depth_contrastive\": []\n",
    "\n",
    "#     gif_frames = []\n",
    "#     num_classes = 20\n",
    "#     # Optimizer for latent noise\n",
    "    \n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#         model.train()\n",
    "#         # epoch_train_seg_loss = 0\n",
    "#         # epoch_train_depth_loss = 0\n",
    "#         # epoch_train_iou = 0\n",
    "#         # epoch_train_combined_loss = 0\n",
    "#         # epoch_train_depth_sidl = 0\n",
    "#         # epoch_train_depth_inv_huber = 0\n",
    "#         # epoch_train_depth_contrastive = 0\n",
    "#         # epoch_train_depth_smooth = 0\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         reconstruction_layer = nn.Conv2d(256, 3, kernel_size=1).to(device)\n",
    "        \n",
    "#         # scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#         # Training Loop\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "\n",
    "#             # Ensure depth_labels and segmentation labels have correct dimensions\n",
    "#             if depth_labels.dim() == 5:\n",
    "#                 depth_labels = depth_labels.squeeze(2)\n",
    "#             if seg_labels.dim() == 4 and seg_labels.shape[1] == 1:\n",
    "#                 seg_labels = seg_labels.squeeze(1)\n",
    "\n",
    "#             # Transform depth labels\n",
    "#             # depth_labels = torch.log(depth_labels.flatten(start_dim=1)) / 5\n",
    "#             # depth_labels = depth_labels.view_as(depth_labels)  # Restore shape\n",
    "#             # depth_labels = torch.clamp(depth_labels, min=1e-5) \n",
    "#             # depth_labels = torch.log(depth_labels + 1e-5) / 5  # Avoid log(0)\n",
    "\n",
    "#             # print(f'seg_labels shape : {seg_labels.shape}')\n",
    "#             # print(f'depth_labels shape: {depth_labels.shape}')\n",
    "\n",
    "#             # Start with random noise as latent condition\n",
    "#             if epoch == 0:\n",
    "#                 latent_noise = torch.randn_like(inputs).to(device)\n",
    "#                 # print(f\"latent_noise: {latent_noise.shape}\")\n",
    "#                 latent_noise.requires_grad = True  # Make it trainable\n",
    "#                 latent_optimizer = torch.optim.Adam([latent_noise], lr=1e-3)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "#             # Stage 1: Train BicycleGAN (Backbone Features)\n",
    "            \n",
    "\n",
    "#             # Reset gradients for both optimizers\n",
    "#             model.optimizer_stage1.zero_grad()\n",
    "#             model.optimizer_stage2.zero_grad()\n",
    "#             latent_optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             seg_output, depth_output, backbone_features = model(inputs, latent_noise)\n",
    "#             # print(f'seg_ouput shape : {seg_output.shape}')\n",
    "#             # print(f'depth_output shape: {depth_output.shape}')\n",
    "#             # print(backbone_features.shape)\n",
    "#             # return\n",
    "\n",
    "\n",
    "#             seg_output_old =seg_output\n",
    "#             # Resize seg_output to match the spatial dimensions of seg_labels\n",
    "#             seg_output_resized = F.interpolate(seg_output, size=seg_labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "#             seg_output = seg_output_resized\n",
    "\n",
    "#             # print(f\"depth_output shape before resize: {depth_output.shape}\")\n",
    "#             # print(f\"depth_labels shape: {depth_labels.shape}\")\n",
    "#             # return\n",
    "\n",
    "#             depth_output_old = depth_output\n",
    "#             depth_output_resized = F.interpolate(depth_output, size=depth_labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "#             depth_output = depth_output_resized\n",
    "\n",
    "\n",
    "#             # Pix2Pix Losses\n",
    "#             seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#             seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#             seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#             seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "            \n",
    "#             depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#             depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#             depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#             depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "            \n",
    "#             pix2pix_loss = seg_loss_total + depth_loss_total\n",
    "\n",
    "#             # Reconstruction loss\n",
    "#             # inputs_resized = F.interpolate(inputs, size=(backbone_features.size(2), backbone_features.size(3)))\n",
    "#             # reconstructed_image = reconstruction_layer(backbone_features)\n",
    "#             # recon_loss = nn.L1Loss()(reconstructed_image, inputs_resized)\n",
    "#             # adaptive_weight = 1 / (1 + torch.exp(-recon_loss))\n",
    "#             # adaptive_weight_value = adaptive_weight.item() \n",
    "\n",
    "#             # loss_stage1 = nn.MSELoss()(real_validity, torch.ones_like(real_validity).to(device)) + recon_loss\n",
    "#             # loss_stage1.backward(retain_graph=True)\n",
    "#             # model.optimizer_stage1.step()\n",
    "\n",
    "#             # Pix2Pix Adversarial Losses\n",
    "#             seg_validity = model.segmentation_discriminator(seg_output)\n",
    "#             depth_validity = model.depth_discriminator(depth_output)\n",
    "#             adv_seg_loss = nn.MSELoss()(seg_validity, torch.ones_like(seg_validity))\n",
    "#             adv_depth_loss = nn.MSELoss()(depth_validity, torch.ones_like(depth_validity))\n",
    "#             pix2pix_total_loss = pix2pix_loss + adv_seg_loss + adv_depth_loss\n",
    "\n",
    "\n",
    "#             # BicycleGAN Loss with Pix2Pix Condition\n",
    "#             # real_validity = model.bicycle_discriminator(backbone_features)\n",
    "#             # recon_loss = nn.L1Loss()(backbone_features, inputs)\n",
    "#             # bicycle_loss = nn.MSELoss()(real_validity, torch.ones_like(real_validity)) + recon_loss\n",
    "#             # conditional_bicycle_loss = bicycle_loss + pix2pix_loss\n",
    "#             # conditional_bicycle_loss.backward(retain_graph=True)\n",
    "#             # model.optimizer_stage1.step()\n",
    "\n",
    "#             # BicycleGAN Loss with Pix2Pix Condition\n",
    "#             real_validity = model.bicycle_discriminator(backbone_features,latent_noise)\n",
    "\n",
    "#             # Resize inputs to match backbone_features\n",
    "#             inputs_resized = F.interpolate(inputs, size=backbone_features.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "#             # print(f\"backbone_features shape: {backbone_features.shape}, inputs shape: {inputs.shape}\")\n",
    "#             # print(f\"inputs_resized shape: {inputs_resized.shape}\")\n",
    "#             # recon_loss = nn.L1Loss()(backbone_features, inputs_resized)\n",
    "#             bicycle_loss = adv_seg_loss + adv_depth_loss\n",
    "#             # + recon_loss\n",
    "\n",
    "#             # Combined Loss\n",
    "#             total_loss = bicycle_loss + pix2pix_total_loss\n",
    "\n",
    "#             # Single backward pass\n",
    "#             total_loss.backward()\n",
    "\n",
    "#             # Update both optimizers\n",
    "#             model.optimizer_stage1.step()\n",
    "#             model.optimizer_stage2.step()\n",
    "#             latent_optimizer.step()\n",
    "\n",
    "#             # Accumulate Training Metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#             epoch_train[\"combined\"] += total_loss.item()\n",
    "#             epoch_train[\"iou\"] += seg_iou.item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_smooth.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#         model.scheduler_stage1.step()\n",
    "#         model.scheduler_stage2.step(epoch_train[\"combined\"]/num_batches)\n",
    "\n",
    "\n",
    "#         # Average Training Losses\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Train Seg Loss: {epoch_train['seg']:.4f}, \"\n",
    "#             f\"Train Depth Loss: {epoch_train['depth']:.4f}, Train Combined Loss: {epoch_train['combined']:.4f}, \"\n",
    "#             f\"Train mIOU: {epoch_train['iou']:.4f}, Train sidl Loss: {epoch_train['depth_sidl']:.4f}, \"\n",
    "#             f\"Train depth smooth: {epoch_train['depth_smooth']:.4f}\"\n",
    "#     )       \n",
    "\n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         # epoch_valid_seg_loss = 0\n",
    "#         # epoch_valid_depth_loss = 0\n",
    "#         # epoch_valid_iou =0\n",
    "#         # epoch_valid_combined_loss = 0\n",
    "#         # epoch_valid_depth_sidl = 0\n",
    "#         # epoch_valid_depth_inv_huber = 0\n",
    "#         # epoch_valid_depth_contrastive = 0\n",
    "#         # epoch_valid_depth_smooth = 0\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 # print(\"inside valid\")\n",
    "#                 inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "\n",
    "#                 # Ensure depth_labels and segmentation labels have correct dimensions\n",
    "#                 if depth_labels.dim() == 5:\n",
    "#                     depth_labels = depth_labels.squeeze(2)\n",
    "#                 if seg_labels.dim() == 4 and seg_labels.shape[1] == 1:\n",
    "#                     seg_labels = seg_labels.squeeze(1)\n",
    "\n",
    "#                 # Transform depth labels\n",
    "#                 # depth_labels = torch.log(depth_labels.flatten(start_dim=1)) / 5\n",
    "#                 # depth_labels = depth_labels.view_as(depth_labels)  # Restore shape\n",
    "#                 # depth_labels = torch.clamp(depth_labels, min=1e-5) \n",
    "#                 # depth_labels = torch.log(depth_labels + 1e-5) / 5  # Avoid log(0)\n",
    "\n",
    "#                 # Latent noise for validation\n",
    "#                 latent_noise = torch.randn_like(inputs).to(device)\n",
    "#                 seg_output, depth_output, backbone_features = model(inputs, latent_noise)\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "#                 seg_output_old =seg_output\n",
    "#                 # Resize seg_output to match the spatial dimensions of seg_labels\n",
    "#                 seg_output_resized = F.interpolate(seg_output, size=seg_labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "#                 seg_output = seg_output_resized\n",
    "\n",
    "#                 depth_output_old = depth_output\n",
    "#                 depth_output_resized = F.interpolate(depth_output, size=depth_labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "#                 depth_output =depth_output_resized\n",
    "\n",
    "\n",
    "#                 # Segmentation Loss\n",
    "#                 seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#                 seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#                 seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#                 seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "                \n",
    "#                 depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#                 depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#                 depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#                 depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "\n",
    "#                 pix2pix_loss = seg_loss_total + depth_loss_total\n",
    "\n",
    "#                 # Combined Validation Loss\n",
    "#                 combined_loss = pix2pix_loss\n",
    "\n",
    "#                 # Accumulate Validation Metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#                 epoch_valid[\"combined\"] += combined_loss.item()\n",
    "#                 epoch_valid[\"iou\"] += seg_iou.item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_smooth.item()\n",
    "                \n",
    "#                 num_valid_batches += 1\n",
    "                \n",
    "#                 # epoch, inputs, seg_output, depth_output, seg_labels, depth_labels, gif_frames\n",
    "#             frame = save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels)\n",
    "#             gif_frames.append(frame)\n",
    "                \n",
    "                \n",
    "#         # Calculate epoch averages\n",
    "#         # Average Validation Losses\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "        \n",
    "        \n",
    "# # train_losses = { \"depth_sidl\": [], \"depth_inv_huber\": [], \"depth_contrastive\": [], \"depth_smooth\": []}\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Valid Seg Loss: {epoch_valid['seg']:.4f}, \"\n",
    "#             f\"Valid Depth Loss: {epoch_valid['depth']:.4f}, Valid Combined Loss: {epoch_valid['combined']:.4f}, \"\n",
    "#             f\"Valid mIOU: {epoch_valid['iou']:.4f}, Valid sidl Loss: {epoch_valid['depth_sidl']:.4f}, \"\n",
    "#             f\"Valid depth smooth: {epoch_valid['depth_smooth']:.4f}\"\n",
    "#         )\n",
    "\n",
    "#         # Write the losses to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 train_losses[\"seg\"], train_losses[\"depth\"], train_losses[\"combined\"],\n",
    "#                 train_losses[\"depth_sidl\"], 0,0,\n",
    "#                 # avg_train_depth_inv_huber, avg_train_depth_contrastive,\n",
    "#                 train_losses[\"depth_smooth\"],\n",
    "#                 valid_losses[\"seg\"], valid_losses[\"depth\"], valid_losses['combined'],\n",
    "#                 valid_losses[\"depth_sidl\"],0,0,\n",
    "#                 # avg_valid_depth_inv_huber, avg_valid_depth_contrastive, \n",
    "#                 valid_losses[\"depth_smooth\"]\n",
    "#             ])\n",
    "\n",
    "#         # Save GIF visualization frames\n",
    "#         # save_training_visualization_as_gif(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels, gif_frames)\n",
    "\n",
    "#         # Save best model\n",
    "#         if valid_losses[\"combined\"][-1] < best_combined_loss:\n",
    "#             best_combined_loss = valid_losses[\"combined\"][-1]\n",
    "#             torch.save(model, os.path.join(save_dir, \"best_model_resnetBackbone.pth\"))\n",
    "#             print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "#         if epoch%10==0:\n",
    "#             gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "#             gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "#     # Save GIF\n",
    "#     # gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "    \n",
    "#     plot_loss(train_losses, valid_losses, save_dir)\n",
    "#     gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return train_losses,valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f553ec-993f-4642-a88f-f519abbeac8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa457c-8d5c-48cf-a9eb-d3ad1b361e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
