{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "49ba9b76-3c4a-4dba-a295-b930ea39f73c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes',\n",
       " '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/cityscapes_dataset')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,Subset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from sklearn.metrics import jaccard_score\n",
    "from torchvision.models.mobilenetv3 import MobileNet_V3_Small_Weights\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from labels import labels\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "curr_dir=os.getcwd()\n",
    "root= os.path.join(curr_dir,\"cityscapes_dataset\")\n",
    "curr_dir,root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a7a94aa7-dba7-4018-9134-f063b811cd98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "# from albumentations.augmentations.transforms import RandomShadow\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\" Normalizes RGB image to  0-mean 1-std_dev \"\"\" \n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], depth_norm=5, max_depth=250):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.depth_norm = depth_norm\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            \n",
    "        return {'left': TF.normalize(left, self.mean, self.std), \n",
    "                'mask': mask, \n",
    "                'depth' : torch.clip( # saftey clip :)\n",
    "                            torch.log(torch.clip(depth, 0, self.max_depth))/self.depth_norm, \n",
    "                            0, \n",
    "                            self.max_depth)}\n",
    "\n",
    "\n",
    "class AddColorJitter(object):\n",
    "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\" \n",
    "    def __init__(self, brightness, contrast, saturation, hue):\n",
    "        ''' Applies brightness, constrast, saturation, and hue jitter to image ''' \n",
    "        self.color_jitter = transforms.ColorJitter(brightness, contrast, saturation, hue)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        return {'left': self.color_jitter(left), \n",
    "                'mask': mask, \n",
    "                'depth' : depth}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\" Rescales images with bilinear interpolation and masks with nearest interpolation \"\"\"\n",
    "\n",
    "    def __init__(self, h, w):\n",
    "        self.h, self.w = h, w\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "# mask interpolation Nearest is import to have smoothness\n",
    "        return {'left': TF.resize(left, (self.h, self.w)), \n",
    "                'mask': TF.resize(mask.unsqueeze(0), (self.h, self.w), transforms.InterpolationMode.NEAREST), \n",
    "                'depth' : TF.resize(depth.unsqueeze(0), (self.h, self.w))}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, h, w, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.scale = scale\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "        i, j, h, w = transforms.RandomResizedCrop.get_params(left, scale=self.scale, ratio=self.ratio)\n",
    "\n",
    "        return {'left': TF.resized_crop(left, i, j, h, w, (self.h, self.w)), \n",
    "                'mask': TF.resized_crop(mask.unsqueeze(0), i, j, h, w, (self.h, self.w), interpolation=TF.InterpolationMode.NEAREST),\n",
    "                'depth' : TF.resized_crop(depth.unsqueeze(0), i, j, h, w, (self.h, self.w))}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "         \n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        return {'left': transforms.ToTensor()(left), \n",
    "                'mask': torch.as_tensor(mask, dtype=torch.int64),\n",
    "                'depth' : transforms.ToTensor()(depth).type(torch.float32)}\n",
    "    \n",
    "\n",
    "class ElasticTransform(object):\n",
    "    def __init__(self, alpha=25.0, sigma=5.0, prob=0.5):\n",
    "        self.alpha = [1.0, alpha]\n",
    "        self.sigma = [1, sigma]\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            _, H, W = mask.shape\n",
    "            displacement = transforms.ElasticTransform.get_params(self.alpha, self.sigma, [H, W])\n",
    "\n",
    "            # # TEMP\n",
    "            # print(TF.elastic_transform(left, displacement).shape)\n",
    "            # print(TF.elastic_transform(mask.unsqueeze(0), displacement, interpolation=TF.InterpolationMode.NEAREST).shape)\n",
    "            # print(torch.clip(TF.elastic_transform(depth, displacement), 0, depth.max()).shape)\n",
    "\n",
    "            return {'left': TF.elastic_transform(left, displacement), \n",
    "                    'mask': TF.elastic_transform(mask.unsqueeze(0), displacement, interpolation=TF.InterpolationMode.NEAREST), \n",
    "                    'depth' : torch.clip(TF.elastic_transform(depth, displacement), 0, depth.max())} \n",
    "        \n",
    "        else:\n",
    "            return sample\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "# new transform to rotate the images\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, angle):\n",
    "        if not isinstance(angle, (list, tuple)):\n",
    "            self.angle = (-abs(angle), abs(angle))\n",
    "        else:\n",
    "            self.angle = angle\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        angle = transforms.RandomRotation.get_params(self.angle)\n",
    "\n",
    "        return {'left': TF.rotate(left, angle), \n",
    "                'mask': TF.rotate(mask.unsqueeze(0), angle), \n",
    "                'depth' : TF.rotate(depth, angle)}\n",
    "    \n",
    "    \n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            return {'left': TF.hflip(left), \n",
    "                    'mask': TF.hflip(mask), \n",
    "                    'depth' : TF.hflip(depth)}\n",
    "        else:\n",
    "            return sample\n",
    "        \n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            return {'left': TF.vflip(left), \n",
    "                    'mask': TF.vflip(mask), \n",
    "                    'depth' : TF.vflip(depth)}\n",
    "        else:\n",
    "            return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f22ee78a-1a67-4c7b-a0ef-2ec953859137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_numpy(image):\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        if len(image.shape) == 2:\n",
    "            image = image.detach().cpu().numpy()\n",
    "        else:\n",
    "            image = image.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_color_mask(mask, labels, id_type='id'):\n",
    "    try:\n",
    "        h, w = mask.shape\n",
    "    except ValueError:\n",
    "        mask = mask.squeeze(-1)\n",
    "        h, w = mask.shape\n",
    "\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    if id_type == 'id':\n",
    "        for lbl in labels:\n",
    "            color_mask[mask == lbl.id] = lbl.color\n",
    "    elif id_type == 'trainId':\n",
    "        for lbl in labels:\n",
    "            if (lbl.trainId != 255) | (lbl.trainId != -1):\n",
    "                color_mask[mask == lbl.trainId] = lbl.color\n",
    "\n",
    "    return color_mask\n",
    "\n",
    "\n",
    "def plot_items(left, mask, depth, labels=None, num_seg_labels=34, id_type='id'):\n",
    "    left = convert_to_numpy(left)\n",
    "    mask = convert_to_numpy(mask)\n",
    "    depth = convert_to_numpy(depth)\n",
    "\n",
    "    # unnormalize left image\n",
    "    left = (left*np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
    "\n",
    "    # cmaps: 'prism', 'terrain', 'turbo', 'gist_rainbow_r', 'nipy_spectral_r'\n",
    "    \n",
    "    \n",
    "    _, ax = plt.subplots(1, 3, figsize=(15,10))\n",
    "    ax[0].imshow(left)\n",
    "    ax[0].set_title(\"Left Image\")\n",
    "\n",
    "    if labels:\n",
    "        color_mask = get_color_mask(mask, labels, id_type)\n",
    "        ax[1].imshow(color_mask)\n",
    "    else:\n",
    "        cmap = mpl.colormaps.get_cmap('nipy_spectral_r').resampled(num_seg_labels)\n",
    "        ax[1].imshow(mask, cmap=cmap)\n",
    "\n",
    "    ax[1].set_title(\"Seg Mask\")\n",
    "    ax[2].imshow(depth, cmap='plasma')\n",
    "    ax[2].set_title(\"Depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6e366997-a6f1-47cd-8190-1fb12596df2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_invariant_depth_loss(pred, target, lambda_weight=0.1):\n",
    "    if pred.shape != target.shape:\n",
    "        pred = F.interpolate(pred, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "    \n",
    "    diff = pred - target\n",
    "    n = diff.numel()\n",
    "    mse = torch.sum(diff**2) / n\n",
    "    scale_invariant = mse - (lambda_weight / (n**2)) * (torch.sum(diff))**2\n",
    "    return scale_invariant\n",
    "\n",
    "def depth_smoothness_loss(pred, img, alpha=1.0):\n",
    "    depth_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])\n",
    "    depth_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])\n",
    "    img_grad_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), dim=1, keepdim=True)\n",
    "    img_grad_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), dim=1, keepdim=True)\n",
    "    smoothness_x = depth_grad_x * torch.exp(-alpha * img_grad_x)\n",
    "    smoothness_y = depth_grad_y * torch.exp(-alpha * img_grad_y)\n",
    "    return smoothness_x.mean() + smoothness_y.mean()\n",
    "\n",
    "\n",
    "def inv_huber_loss(pred, target, delta=0.1):\n",
    "    \"\"\"\n",
    "    Inverse Huber loss for depth prediction.\n",
    "    Args:\n",
    "        pred (Tensor): Predicted depth map.\n",
    "        target (Tensor): Ground truth depth map.\n",
    "        delta (float): Threshold for switching between quadratic and linear terms.\n",
    "    Returns:\n",
    "        Tensor: Inverse Huber loss.\n",
    "    \"\"\"\n",
    "    abs_diff = torch.abs(pred - target)\n",
    "    delta_tensor = torch.tensor(delta, dtype=abs_diff.dtype, device=abs_diff.device)  # Convert delta to tensor\n",
    "    quadratic = torch.minimum(abs_diff, delta_tensor)\n",
    "    linear = abs_diff - quadratic\n",
    "    return (0.5 * quadratic**2 + delta_tensor * linear).mean()\n",
    "\n",
    "\n",
    "def mean_iou(pred, target, num_classes):\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    intersection = torch.logical_and(pred == target, target != 255).float()  # Ignore class 255\n",
    "    union = torch.logical_or(pred == target, target != 255).float()\n",
    "    iou = torch.sum(intersection) / torch.sum(union)\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def contrastive_loss(pred, target, margin=1.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss to ensure the depth map predictions are closer to the target.\n",
    "    \"\"\"\n",
    "    # Flatten the tensors for element-wise operations\n",
    "    pred_flat = pred.view(pred.size(0), -1)  # Flatten except for the batch dimension\n",
    "    target_flat = target.view(target.size(0), -1)  # Flatten except for the batch dimension\n",
    "\n",
    "    # Compute the pairwise distances\n",
    "    distances = torch.sqrt(torch.sum((pred_flat - target_flat) ** 2, dim=1))  # Batch-wise distances\n",
    "\n",
    "    # Create labels for contrastive loss\n",
    "    labels = (torch.abs(pred_flat - target_flat).mean(dim=1) < margin).float()  # Batch-wise labels\n",
    "\n",
    "    # Calculate contrastive loss\n",
    "    similar_loss = labels * distances**2\n",
    "    dissimilar_loss = (1 - labels) * torch.clamp(margin - distances, min=0)**2\n",
    "    loss = (similar_loss + dissimilar_loss).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def dice_loss(predictions, targets, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Dice Loss for segmentation.\n",
    "    Args:\n",
    "        predictions (torch.Tensor): The predicted segmentation map (logits or probabilities).\n",
    "                                    Shape: [batch_size, num_classes, height, width]\n",
    "        targets (torch.Tensor): The ground truth segmentation map (one-hot encoded or integer labels).\n",
    "                                Shape: [batch_size, height, width]\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    Returns:\n",
    "        torch.Tensor: Dice Loss (scalar).\n",
    "    \"\"\"\n",
    "    # Convert integer labels to one-hot if needed\n",
    "    if predictions.shape != targets.shape:\n",
    "        targets = F.one_hot(targets, num_classes=predictions.shape[1]).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    # Apply softmax to predictions for multi-class segmentation\n",
    "    predictions = torch.softmax(predictions, dim=1)\n",
    "    \n",
    "    # Flatten tensors to calculate intersection and union\n",
    "    predictions_flat = predictions.view(predictions.shape[0], predictions.shape[1], -1)\n",
    "    targets_flat = targets.view(targets.shape[0], targets.shape[1], -1)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = (predictions_flat * targets_flat).sum(dim=-1)\n",
    "    union = predictions_flat.sum(dim=-1) + targets_flat.sum(dim=-1)\n",
    "    \n",
    "    # Calculate Dice Coefficient\n",
    "    dice_coeff = (2 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # Dice Loss\n",
    "    return 1 - dice_coeff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f8ec23d0-1fad-4dfe-8b94-6de4c2579d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, valid_losses, save_dir):\n",
    "    epochs = range(1, len(train_losses[\"seg\"]) + 1)\n",
    "\n",
    "    # Plot Segmentation Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses[\"seg\"], label=\"Train Seg Loss\")\n",
    "    plt.plot(epochs, valid_losses[\"seg\"], label=\"Valid Seg Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Segmentation Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Segmentation Loss Over Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir, \"segmentation_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Depth Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses[\"depth\"], label=\"Train Depth Loss\")\n",
    "    plt.plot(epochs, valid_losses[\"depth\"], label=\"Valid Depth Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Depth Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Depth Loss Over Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir, \"depth_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Combined Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses[\"combined\"], label=\"Train Combined Loss\")\n",
    "    plt.plot(epochs, valid_losses[\"combined\"], label=\"Valid Combined Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Combined Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Combined Loss Over Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir, \"combined_loss.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8b8fe187-44fc-45aa-bf13-87b9b28936af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels):\n",
    "    inputs = inputs.detach().cpu()\n",
    "    seg_output = torch.argmax(seg_output, dim=1).detach().cpu()\n",
    "    depth_output = depth_output.detach().cpu()\n",
    "    seg_labels = seg_labels.detach().cpu()\n",
    "    depth_labels = depth_labels.detach().cpu()\n",
    "    \n",
    "#     inputs_rgb = (inputs - inputs.min()) / (inputs.max() - inputs.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "    \n",
    "#     # Normalize depth maps for visualization\n",
    "#     depth_labels_vis = (depth_labels - depth_labels.min()) / (depth_labels.max() - depth_labels.min() + 1e-5)\n",
    "#     depth_preds_vis = (depth_output - depth_output.min()) / (depth_output.max() - depth_output.min() + 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = min(4, inputs.size(0))  # Limit to 4 samples for visualization\n",
    "    fig, axes = plt.subplots(batch_size, 5, figsize=(15, 4 * batch_size))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        inputs_temp = inputs[i]\n",
    "        # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "        inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "        \n",
    "        depth_labels_vis = (depth_labels[i] - depth_labels[i].min()) / (depth_labels[i].max() - depth_labels[i].min() + 1e-5)\n",
    "        depth_preds = depth_output[i]\n",
    "        depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5)\n",
    "        # print(f\"depth_labels_vis: {depth_labels_vis.shape}\")\n",
    "        # print(f\"depth_preds_vis: {depth_preds_vis.shape}\")\n",
    "\n",
    "    \n",
    "        \n",
    "        # Row 1: Ground truth\n",
    "        axes[i, 0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "        axes[i, 0].set_title(\"RGB Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(seg_labels[i], cmap=\"tab20\")\n",
    "        axes[i, 1].set_title(\"GT Segmentation\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(depth_labels_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 2].set_title(\"GT Depth\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "        # Row 2: Predictions\n",
    "        axes[i, 3].imshow(seg_output[i], cmap=\"tab20\")\n",
    "        axes[i, 3].set_title(\"Generated Segmentation\")\n",
    "        axes[i, 3].axis(\"off\")\n",
    "\n",
    "        axes[i, 4].imshow(depth_preds_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 4].set_title(\"Generated Depth\")\n",
    "        axes[i, 4].axis(\"off\")\n",
    "        \n",
    "    # Remove axes for cleaner visualization\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    fig.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # # Save current epoch as an image for GIF\n",
    "    # epoch_img_path = os.path.join(gif_path, f\"epoch_{epoch}.png\")\n",
    "    # os.makedirs(gif_path, exist_ok=True)\n",
    "    # plt.savefig(epoch_img_path)\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    # return epoch_img_path\n",
    "    frame = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)  # Updated to buffer_rgba\n",
    "    frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (4,))  # RGBA has 4 channels\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Convert to PIL.Image for GIF\n",
    "    frame_rgb = frame[:, :, :3] \n",
    "\n",
    "    # Return as PIL.Image for GIF creation\n",
    "    # return Image.fromarray(frame)\n",
    "    return Image.fromarray(frame_rgb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55d241-89e1-47d7-9552-60deb4df0251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d01be9a0-180e-44bf-a549-ba003cbf2bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Define the ResBlock\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "# Define the CRPBlock\n",
    "class CRPBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, n_stages=4, groups=False):\n",
    "        super(CRPBlock, self).__init__()\n",
    "        self.n_stages = n_stages\n",
    "        groups = in_chans if groups else 1\n",
    "        self.mini_blocks = nn.ModuleList()\n",
    "        for _ in range(n_stages):\n",
    "            self.mini_blocks.append(nn.MaxPool2d(kernel_size=5, stride=1, padding=2))\n",
    "            self.mini_blocks.append(nn.Conv2d(in_chans, out_chans, kernel_size=1, bias=False, groups=groups))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for block in self.mini_blocks:\n",
    "            out = block(out)\n",
    "            x = x + out\n",
    "        return x\n",
    "\n",
    "class ResNetBackbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, feature_dim=256):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        base_model = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Freeze pre-trained layers\n",
    "        for param in base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Extract ResNet layers and modify strides/pooling to preserve spatial dimensions\n",
    "        layers = list(base_model.children())[:-2]  # Remove FC and AvgPool layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                layer.stride = (1, 1)  # Set stride to 1\n",
    "            elif isinstance(layer, nn.MaxPool2d) or isinstance(layer, nn.AvgPool2d):\n",
    "                layer.stride = (1, 1)  # Avoid reducing dimensions with pooling layers\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # Adjust final feature dimension using a 1x1 convolution\n",
    "        self.feature_dim = feature_dim\n",
    "        self.adjust_channels = nn.Conv2d(base_model.fc.in_features, feature_dim, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Extract features without changing spatial dimensions\n",
    "        x = self.adjust_channels(x)  # Adjust feature channels\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "70ae93ff-1a2e-46c9-92cf-57767c645dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CityScapesDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, split='train', label_map='id', crop=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "        self.crop = crop\n",
    "\n",
    "        self.left_paths = glob(os.path.join(root, 'leftImg8bit', split, '**/*.png'))\n",
    "        self.mask_paths = glob(os.path.join(root, 'gtFine', split, '**/*gtFine_labelIds.png'))\n",
    "        self.depth_paths = glob(os.path.join(root, 'crestereo_depth2', split, '**/*.npy'))\n",
    "\n",
    "        sorted(self.left_paths)\n",
    "        sorted(self.mask_paths)\n",
    "        sorted(self.depth_paths)\n",
    "\n",
    "        # get label mappings\n",
    "        self.id_2_train = {}\n",
    "        self.id_2_cat = {}\n",
    "        self.train_2_id = {}\n",
    "        self.id_2_name = {-1 : 'unlabeled'}\n",
    "        self.trainid_2_name = {19 : 'unlabeled'} # {255 : 'unlabeled', -1 : 'unlabeled'}\n",
    "\n",
    "        for lbl in labels:\n",
    "            self.id_2_train.update({lbl.id : lbl.trainId})\n",
    "            self.id_2_cat.update({lbl.id : lbl.categoryId})\n",
    "            if lbl.trainId != 19: # (lbl.trainId > 0) and (lbl.trainId != 255):\n",
    "                self.trainid_2_name.update({lbl.trainId : lbl.name})\n",
    "                self.train_2_id.update({lbl.trainId : lbl.id})\n",
    "            if lbl.id > 0:\n",
    "                self.id_2_name.update({lbl.id : lbl.name})\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left = cv2.cvtColor(cv2.imread(self.left_paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_UNCHANGED).astype(np.uint8)\n",
    "        depth = np.load(self.depth_paths[idx]) # data is type float16\n",
    "\n",
    "        if self.crop:\n",
    "            left = left[:800, :, :]\n",
    "            mask = mask[:800, :]\n",
    "            depth = depth[:800, :]\n",
    "\n",
    "        # get label id\n",
    "        if self.label_map == 'id':\n",
    "            mask[mask==-1] == 0\n",
    "        elif self.label_map == 'trainId':\n",
    "            for _id, train_id in self.id_2_train.items():\n",
    "                mask[mask==_id] = train_id\n",
    "        elif self.label_map == 'categoryId':\n",
    "            for _id, train_id in self.id_2_cat.items():\n",
    "                mask[mask==_id] = train_id\n",
    "\n",
    "        sample = {'left' : left, 'mask' : mask, 'depth' : depth}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # ensure that no depth values are less than 0\n",
    "        depth[depth < 0] = 0\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        print(f\"Number of RGB images: {len(self.left_paths)}\")\n",
    "        print(f\"Number of Mask images: {len(self.mask_paths)}\")\n",
    "        print(f\"Number of depth images: {len(self.depth_paths)}\")\n",
    "        return len(self.left_paths)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c92a35c2-3d5c-4f87-87ac-4b189c154bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OG_W, OG_H = 2048, 800 # OG width and height after crop\n",
    "W, H = OG_W//4, OG_H//4 # resize w,h for training\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    RandomCrop(H, W),\n",
    "    # ElasticTransform(alpha=100.0, sigma=25.0, prob=0.5),\n",
    "    AddColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    RandomHorizontalFlip(0.5),\n",
    "    RandomVerticalFlip(0.2),\n",
    "    # RandomRotate((-30, 30)),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Rescale(H, W),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CityScapesDataset(root, transform=transform, split='train', label_map='trainId') # 'trainId')\n",
    "train_subset = Subset(train_dataset, indices=range(2968)) #2968\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n",
    "\n",
    "\n",
    "valid_dataset = CityScapesDataset(root, transform=valid_transform, split='val', label_map='trainId')\n",
    "val_subset = Subset(valid_dataset, indices=range(496)) #496 \n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=False)\n",
    "valid_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4612e4a-638e-4b58-9cd1-7c6f59edf9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b96a8e6-8ceb-4ce9-b268-73df64cc0c12",
   "metadata": {},
   "source": [
    "# shared Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a1dc8478-39f9-41f3-bb56-02ffccc724bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SharedGenerator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         Shared Generator for both tasks.\n",
    "#         Contains shared layers for skip connection processing and refinement.\n",
    "#         \"\"\"\n",
    "#         super(SharedGenerator, self).__init__()\n",
    "#         # Shared convolution layers to process each skip connection\n",
    "#         self.shared_conv1 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l11_out (1/32)\n",
    "#         self.shared_conv2 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l7_out (1/16)\n",
    "#         self.shared_conv3 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l3_out (1/8)\n",
    "#         self.shared_conv4 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l1_out (1/4)\n",
    "\n",
    "#         # Shared CRP blocks for refinement\n",
    "#         self.shared_crp1 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/32\n",
    "#         self.shared_crp2 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/16\n",
    "#         self.shared_crp3 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/8\n",
    "#         self.shared_crp4 = CRPBlock(256, 256, n_stages=4)  # CRP for 1/4\n",
    "\n",
    "#     def forward(self, skips):\n",
    "#         \"\"\"\n",
    "#         Process skips with shared layers for task-specific generation.\n",
    "#         Args:\n",
    "#             skips (dict): Skip connections from the encoder.\n",
    "#         Returns:\n",
    "#             dict: Processed skip connections.\n",
    "#         \"\"\"\n",
    "#         x1 = self.shared_crp1(self.shared_conv1(skips[\"l11_out\"]))\n",
    "#         x2 = self.shared_crp2(self.shared_conv2(skips[\"l7_out\"]))\n",
    "#         x3 = self.shared_crp3(self.shared_conv3(skips[\"l3_out\"]))\n",
    "#         x4 = self.shared_crp4(self.shared_conv4(skips[\"l1_out\"]))\n",
    "\n",
    "#         return {\"x1\": x1, \"x2\": x2, \"x3\": x3, \"x4\": x4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e4d36-98bd-4506-ad2f-3220cd3661f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb487e-4641-481a-9b15-56d1ce9a38ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e74154-b3ec-4e4c-aa36-aa2e9f5545fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "fa913d77-fd04-4079-a039-6caf557329a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SharedPix2PixGenerator(nn.Module):\n",
    "#     def __init__(self, seg_output_channels=20, depth_output_channels=1):\n",
    "#         \"\"\"\n",
    "#         Shared Pix2Pix Generator for Segmentation and Depth tasks.\n",
    "#         Args:\n",
    "#             seg_output_channels (int): Number of output channels for segmentation.\n",
    "#             depth_output_channels (int): Number of output channels for depth estimation.\n",
    "#         \"\"\"\n",
    "#         super(SharedPix2PixGenerator, self).__init__()\n",
    "#         self.shared_generator = SharedGenerator()\n",
    "#         self.seg_output_layer = TaskOutputLayer(output_channels=seg_output_channels)\n",
    "#         self.depth_output_layer = TaskOutputLayer(output_channels=depth_output_channels)\n",
    "\n",
    "#     def forward(self, skips, input_size):\n",
    "#         \"\"\"\n",
    "#         Forward pass for both tasks.\n",
    "#         Args:\n",
    "#             skips (dict): Skip connections from the encoder.\n",
    "#             input_size (tuple): Original input size (H, W).\n",
    "#         Returns:\n",
    "#             dict: Outputs for segmentation and depth tasks.\n",
    "#         \"\"\"\n",
    "#         shared_features = self.shared_generator(skips)\n",
    "\n",
    "#         # Task-specific outputs\n",
    "#         seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "#         depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "#         return {\n",
    "#             \"seg_output\": seg_output,\n",
    "#             \"depth_output\": depth_output\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80b7b6-15b9-47dd-bdea-af6fa22b055b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59b8dc73-e69d-4f80-b1e4-09507649a504",
   "metadata": {},
   "source": [
    "# Saving batch gif code And function to plot al losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "36faf655-f78f-4cc8-99af-8de9a1ecabcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels):\n",
    "    inputs = inputs.detach().cpu()\n",
    "    seg_output = torch.argmax(seg_output, dim=1).detach().cpu()\n",
    "    depth_output = depth_output.detach().cpu()\n",
    "    seg_labels = seg_labels.detach().cpu()\n",
    "    depth_labels = depth_labels.detach().cpu()\n",
    "    \n",
    "#     inputs_rgb = (inputs - inputs.min()) / (inputs.max() - inputs.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "    \n",
    "#     # Normalize depth maps for visualization\n",
    "#     depth_labels_vis = (depth_labels - depth_labels.min()) / (depth_labels.max() - depth_labels.min() + 1e-5)\n",
    "#     depth_preds_vis = (depth_output - depth_output.min()) / (depth_output.max() - depth_output.min() + 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "    batch_size = min(4, inputs.size(0))  # Limit to 4 samples for visualization\n",
    "    fig, axes = plt.subplots(batch_size, 5, figsize=(15, 4 * batch_size))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        inputs_temp = inputs[i]\n",
    "        # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "        inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "        \n",
    "        depth_labels_vis = (depth_labels[i] - depth_labels[i].min()) / (depth_labels[i].max() - depth_labels[i].min() + 1e-5)\n",
    "        depth_preds = depth_output[i]\n",
    "        depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5)\n",
    "        # print(f\"depth_labels_vis: {depth_labels_vis.shape}\")\n",
    "        # print(f\"depth_preds_vis: {depth_preds_vis.shape}\")\n",
    "\n",
    "    \n",
    "        \n",
    "        # Row 1: Ground truth\n",
    "        axes[i, 0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "        axes[i, 0].set_title(\"RGB Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(seg_labels[i], cmap=\"tab20\")\n",
    "        axes[i, 1].set_title(\"GT Segmentation\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(depth_labels_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 2].set_title(\"GT Depth\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "        # Row 2: Predictions\n",
    "        axes[i, 3].imshow(seg_output[i], cmap=\"tab20\")\n",
    "        axes[i, 3].set_title(\"Generated Segmentation\")\n",
    "        axes[i, 3].axis(\"off\")\n",
    "\n",
    "        axes[i, 4].imshow(depth_preds_vis.squeeze(), cmap=\"inferno\")\n",
    "        axes[i, 4].set_title(\"Generated Depth\")\n",
    "        axes[i, 4].axis(\"off\")\n",
    "        \n",
    "    # Remove axes for cleaner visualization\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    fig.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # # Save current epoch as an image for GIF\n",
    "    # epoch_img_path = os.path.join(gif_path, f\"epoch_{epoch}.png\")\n",
    "    # os.makedirs(gif_path, exist_ok=True)\n",
    "    # plt.savefig(epoch_img_path)\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "    # return epoch_img_path\n",
    "    frame = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)  # Updated to buffer_rgba\n",
    "    frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (4,))  # RGBA has 4 channels\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Convert to PIL.Image for GIF\n",
    "    frame_rgb = frame[:, :, :3] \n",
    "\n",
    "    # Return as PIL.Image for GIF creation\n",
    "    # return Image.fromarray(frame)\n",
    "    return Image.fromarray(frame_rgb)\n",
    "\n",
    "def plot_all_losses(train_losses,valid_losses,save_dir):\n",
    "    # Plot training and validation losses\n",
    "    for key in train_losses.keys():\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses[key], label=f\"Train {key}\")\n",
    "        plt.plot(valid_losses[key], label=f\"Valid {key}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(key.replace(\"_\", \" \").title())\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, f\"{key}_loss.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f670c-bfdb-49d7-90f9-b1f42c9a3152",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "57b9043f-4f47-410c-8ab6-bb8b3082de8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_adversarial_losses(\n",
    "    discriminator_model,  # Pass the actual model\n",
    "    discriminator_real, discriminator_fake, hinge_loss=True, gradient_penalty=True, real_inputs=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes adversarial losses for WGAN-GP and Hinge Loss.\n",
    "    \"\"\"\n",
    "    # Generator Loss\n",
    "    generator_loss = -torch.mean(discriminator_fake)\n",
    "\n",
    "    # Discriminator Loss\n",
    "    if hinge_loss:\n",
    "        discriminator_real_loss = torch.mean(torch.relu(1.0 - discriminator_real))\n",
    "        discriminator_fake_loss = torch.mean(torch.relu(1.0 + discriminator_fake))\n",
    "        discriminator_loss = discriminator_real_loss + discriminator_fake_loss\n",
    "    else:\n",
    "        discriminator_loss = torch.mean(discriminator_fake) - torch.mean(discriminator_real)\n",
    "\n",
    "    # Gradient Penalty (WGAN-GP)\n",
    "    if gradient_penalty and not hinge_loss:\n",
    "        alpha = torch.rand(real_inputs.size(0), 1, 1, 1, device=real_inputs.device)\n",
    "        interpolates = alpha * real_inputs + (1 - alpha) * real_inputs.detach()\n",
    "        interpolates.requires_grad_(True)\n",
    "        # interpolates_output = discriminator_real(interpolates)\n",
    "        \n",
    "        # Use the actual discriminator model for gradient penalty computation\n",
    "        interpolates_output = discriminator_model(interpolates)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interpolates_output,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones_like(interpolates_output),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        # gradient_penalty_term = torch.mean((gradients.view(gradients.size(0), -1).norm(2, dim=1) - 1) ** 2)\n",
    "        gradient_penalty_term = torch.mean((gradients.flatten(start_dim=1).norm(2, dim=1) - 1) ** 2)\n",
    "\n",
    "        discriminator_loss += 10.0 * gradient_penalty_term\n",
    "\n",
    "    return generator_loss, discriminator_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e338594d-fd9d-4121-aeef-288680c916cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"vgg16\", layers=[\"relu3_3\"], device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Perceptual loss class.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model (str): Pretrained model to use (e.g., \"vgg16\").\n",
    "            layers (list of str): Layers to extract features from.\n",
    "            device (str): Device to load the pretrained model on (\"cuda\" or \"cpu\").\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained model\n",
    "        if pretrained_model == \"vgg16\":\n",
    "            vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pretrained model: {pretrained_model}\")\n",
    "\n",
    "        # Freeze the parameters\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Select layers\n",
    "        self.layers = layers\n",
    "        self.feature_extractor = nn.ModuleDict({\n",
    "            layer: vgg[:i] for i, layer in enumerate(vgg._modules.keys()) if layer in self.layers\n",
    "        })\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        \"\"\"\n",
    "        Compute perceptual loss between generated and target images.\n",
    "\n",
    "        Args:\n",
    "            generated (torch.Tensor): Generated image batch.\n",
    "            target (torch.Tensor): Target image batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: MSE loss between extracted features.\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        for layer_name, extractor in self.feature_extractor.items():\n",
    "            gen_features = extractor(generated)\n",
    "            target_features = extractor(target)\n",
    "            loss += F.mse_loss(gen_features, target_features)\n",
    "        return loss\n",
    "\n",
    "def scale_invariant_depth_loss(pred, target, lambda_weight=0.1):\n",
    "    if pred.shape != target.shape:\n",
    "        pred = F.interpolate(pred, size=target.shape[1:], mode='bilinear', align_corners=False)\n",
    "    \n",
    "    diff = pred - target\n",
    "    n = diff.numel()\n",
    "    mse = torch.sum(diff**2) / n\n",
    "    scale_invariant = mse - (lambda_weight / (n**2)) * (torch.sum(diff))**2\n",
    "    return scale_invariant\n",
    "\n",
    "def depth_smoothness_loss(pred, img, alpha=1.0):\n",
    "    depth_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])\n",
    "    depth_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])\n",
    "    img_grad_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), dim=1, keepdim=True)\n",
    "    img_grad_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), dim=1, keepdim=True)\n",
    "    smoothness_x = depth_grad_x * torch.exp(-alpha * img_grad_x)\n",
    "    smoothness_y = depth_grad_y * torch.exp(-alpha * img_grad_y)\n",
    "    return smoothness_x.mean() + smoothness_y.mean()\n",
    "\n",
    "\n",
    "def inv_huber_loss(pred, target, delta=0.1):\n",
    "    \"\"\"\n",
    "    Inverse Huber loss for depth prediction.\n",
    "    Args:\n",
    "        pred (Tensor): Predicted depth map.\n",
    "        target (Tensor): Ground truth depth map.\n",
    "        delta (float): Threshold for switching between quadratic and linear terms.\n",
    "    Returns:\n",
    "        Tensor: Inverse Huber loss.\n",
    "    \"\"\"\n",
    "    abs_diff = torch.abs(pred - target)\n",
    "    delta_tensor = torch.tensor(delta, dtype=abs_diff.dtype, device=abs_diff.device)  # Convert delta to tensor\n",
    "    quadratic = torch.minimum(abs_diff, delta_tensor)\n",
    "    linear = abs_diff - quadratic\n",
    "    return (0.5 * quadratic**2 + delta_tensor * linear).mean()\n",
    "\n",
    "\n",
    "def mean_iou(pred, target, num_classes):\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    intersection = torch.logical_and(pred == target, target != 255).float()  # Ignore class 255\n",
    "    union = torch.logical_or(pred == target, target != 255).float()\n",
    "    iou = torch.sum(intersection) / torch.sum(union)\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def contrastive_loss(pred, target, margin=1.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss to ensure the depth map predictions are closer to the target.\n",
    "    \"\"\"\n",
    "    # Flatten the tensors for element-wise operations\n",
    "    pred_flat = pred.view(pred.size(0), -1)  # Flatten except for the batch dimension\n",
    "    target_flat = target.view(target.size(0), -1)  # Flatten except for the batch dimension\n",
    "\n",
    "    # Compute the pairwise distances\n",
    "    distances = torch.sqrt(torch.sum((pred_flat - target_flat) ** 2, dim=1))  # Batch-wise distances\n",
    "\n",
    "    # Create labels for contrastive loss\n",
    "    labels = (torch.abs(pred_flat - target_flat).mean(dim=1) < margin).float()  # Batch-wise labels\n",
    "\n",
    "    # Calculate contrastive loss\n",
    "    similar_loss = labels * distances**2\n",
    "    dissimilar_loss = (1 - labels) * torch.clamp(margin - distances, min=0)**2\n",
    "    loss = (similar_loss + dissimilar_loss).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def dice_loss(predictions, targets, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Dice Loss for segmentation.\n",
    "    Args:\n",
    "        predictions (torch.Tensor): The predicted segmentation map (logits or probabilities).\n",
    "                                    Shape: [batch_size, num_classes, height, width]\n",
    "        targets (torch.Tensor): The ground truth segmentation map (one-hot encoded or integer labels).\n",
    "                                Shape: [batch_size, height, width]\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    Returns:\n",
    "        torch.Tensor: Dice Loss (scalar).\n",
    "    \"\"\"\n",
    "    # Convert integer labels to one-hot if needed\n",
    "    if predictions.shape != targets.shape:\n",
    "        targets = F.one_hot(targets, num_classes=predictions.shape[1]).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    # Apply softmax to predictions for multi-class segmentation\n",
    "    predictions = torch.softmax(predictions, dim=1)\n",
    "    \n",
    "    # Flatten tensors to calculate intersection and union\n",
    "    predictions_flat = predictions.view(predictions.shape[0], predictions.shape[1], -1)\n",
    "    targets_flat = targets.view(targets.shape[0], targets.shape[1], -1)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = (predictions_flat * targets_flat).sum(dim=-1)\n",
    "    union = predictions_flat.sum(dim=-1) + targets_flat.sum(dim=-1)\n",
    "    \n",
    "    # Calculate Dice Coefficient\n",
    "    dice_coeff = (2 * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # Dice Loss\n",
    "    return 1 - dice_coeff.mean()\n",
    "\n",
    "\n",
    "def initialize_optimizers_and_schedulers(model, lr_gen=1e-4, lr_disc=1e-4, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Initialize optimizers and schedulers for all generators and discriminators.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): MultiTaskModel instance.\n",
    "        lr_gen (float): Learning rate for generators.\n",
    "        lr_disc (float): Learning rate for discriminators.\n",
    "        weight_decay (float): Weight decay for optimizers.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Optimizers and schedulers for generators and discriminators.\n",
    "    \"\"\"\n",
    "    # Optimizers for shared generator\n",
    "    optimizer_shared_gen = torch.optim.AdamW(\n",
    "        model.feature_generator.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_shared_gen = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_shared_gen, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # Optimizer and scheduler for the shared generator's refinement layer\n",
    "    optimizer_shared_refine = torch.optim.AdamW(\n",
    "        model.shared_generator.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_shared_refine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_shared_refine, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    # Optimizers and schedulers for task-specific generators\n",
    "    optimizer_seg_gen = torch.optim.AdamW(\n",
    "        model.seg_output_layer.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_seg_gen = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_seg_gen, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    optimizer_depth_gen = torch.optim.AdamW(\n",
    "        model.depth_output_layer.parameters(),\n",
    "        lr=lr_gen,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_depth_gen = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_depth_gen, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "\n",
    "    # Optimizers and schedulers for task-specific discriminators\n",
    "    optimizer_seg_disc = torch.optim.AdamW(\n",
    "        model.seg_discriminator.parameters(),\n",
    "        lr=lr_disc,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_seg_disc = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_seg_disc, step_size=20, gamma=0.1\n",
    "    )\n",
    "\n",
    "    optimizer_depth_disc = torch.optim.AdamW(\n",
    "        model.depth_discriminator.parameters(),\n",
    "        lr=lr_disc,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_depth_disc = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_depth_disc, step_size=20, gamma=0.1\n",
    "    )\n",
    "\n",
    "    # Optimizer and scheduler for the multi-task discriminator\n",
    "    optimizer_multi_task_disc = torch.optim.AdamW(\n",
    "        model.multi_task_discriminator.parameters(),\n",
    "        lr=lr_disc,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler_multi_task_disc = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer_multi_task_disc, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"optimizers\": {\n",
    "            \"shared_gen\": optimizer_shared_gen,\n",
    "            \"shared_refine\": optimizer_shared_refine,\n",
    "            \"seg_gen\": optimizer_seg_gen,\n",
    "            \"depth_gen\": optimizer_depth_gen,\n",
    "            \"seg_disc\": optimizer_seg_disc,\n",
    "            \"depth_disc\": optimizer_depth_disc,\n",
    "            \"multi_task_disc\": optimizer_multi_task_disc\n",
    "        },\n",
    "        \"schedulers\": {\n",
    "            \"shared_gen\": scheduler_shared_gen,\n",
    "            \"shared_refine\": scheduler_shared_refine,\n",
    "            \"seg_gen\": scheduler_seg_gen,\n",
    "            \"depth_gen\": scheduler_depth_gen,\n",
    "            \"seg_disc\": scheduler_seg_disc,\n",
    "            \"depth_disc\": scheduler_depth_disc,\n",
    "            \"multi_task_disc\": scheduler_multi_task_disc\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b91ad4-6f1c-4f07-b1ee-098fb0b0b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13bfe288-9700-45ec-8835-9795a3d51277",
   "metadata": {},
   "source": [
    "## MultiTaskModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb51d0-879f-40b1-9092-731e2d247b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "70ad8610-be7e-460c-93f2-cb1f579d6b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MobileNetV3Backbone(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.proj_l1 = nn.Conv2d(16, 576, kernel_size=1, bias=False)   # For l1_out (1/4 resolution)\n",
    "        self.proj_l3 = nn.Conv2d(24, 576, kernel_size=1, bias=False)  # For l3_out (1/8 resolution)\n",
    "        self.proj_l7 = nn.Conv2d(48, 576, kernel_size=1, bias=False)  # For l7_out (1/16 resolution)\n",
    "        self.proj_l11 = nn.Conv2d(96, 576, kernel_size=1, bias=False) # For l11_out (1/32 resolution)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Passes input theough MobileNetV3 backbone feature extraction layers\n",
    "            layers to add connections to (0 indexed)\n",
    "                - 1:  1/4 res\n",
    "                - 3:  1/8 res\n",
    "                - 7, 8:  1/16 res\n",
    "                - 10, 11: 1/32 res\n",
    "           \"\"\"\n",
    "        # skips = nn.ParameterDict()\n",
    "        # for i in range(len(self.backbone) - 1):\n",
    "        #     x = self.backbone[i](x)\n",
    "        #     # add skip connection outputs\n",
    "        #     if i in [1, 3, 7, 11]:\n",
    "        #         skips.update({f\"l{i}_out\" : x})\n",
    "\n",
    "        # return skips\n",
    "        skips = {}  # Dictionary to store skip connections\n",
    "\n",
    "        for i, layer in enumerate(self.backbone):\n",
    "            x = layer(x)\n",
    "            # Add skip connections for specific layers\n",
    "            if i == 1:\n",
    "                skips[\"l1_out\"] = self.proj_l1(x)  # Project l1_out\n",
    "            elif i == 3:\n",
    "                skips[\"l3_out\"] = self.proj_l3(x)  # Project l3_out\n",
    "            elif i == 7:\n",
    "                skips[\"l7_out\"] = self.proj_l7(x)  # Project l7_out\n",
    "            elif i == 11:\n",
    "                skips[\"l11_out\"] = self.proj_l11(x)  # Project l11_out\n",
    "\n",
    "        return skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f9bf6050-8f9c-439c-b125-bc1b9955a4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnhancedSharedGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Enhanced Shared Generator for Segmentation and Depth tasks.\n",
    "        Includes additional refinement layers for better generalization.\n",
    "        \"\"\"\n",
    "        super(EnhancedSharedGenerator, self).__init__()\n",
    "        # Shared convolution layers to process each skip connection\n",
    "        self.shared_conv1 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l11_out (1/32)\n",
    "        self.shared_conv2 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l7_out (1/16)\n",
    "        self.shared_conv3 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l3_out (1/8)\n",
    "        self.shared_conv4 = nn.Conv2d(576, 256, kernel_size=1, bias=False)  # Process l1_out (1/4)\n",
    "\n",
    "        # CRP blocks for refinement\n",
    "        self.shared_crp1 = CRPBlock(256, 256, n_stages=4)\n",
    "        self.shared_crp2 = CRPBlock(256, 256, n_stages=4)\n",
    "        self.shared_crp3 = CRPBlock(256, 256, n_stages=4)\n",
    "        self.shared_crp4 = CRPBlock(256, 256, n_stages=4)\n",
    "\n",
    "        # Additional refinement layers\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, skips):\n",
    "        \"\"\"\n",
    "        Process skips with shared layers for task-specific generation.\n",
    "        Args:\n",
    "            skips (dict): Skip connections from the encoder.\n",
    "        Returns:\n",
    "            dict: Processed skip connections.\n",
    "        \"\"\"\n",
    "        x1 = self.shared_crp1(self.shared_conv1(skips[\"l11_out\"]))\n",
    "        x1 = self.refine(x1)  # Extra refinement\n",
    "\n",
    "        x2 = self.shared_crp2(self.shared_conv2(skips[\"l7_out\"]))\n",
    "        x2 = self.refine(x2)\n",
    "\n",
    "        x3 = self.shared_crp3(self.shared_conv3(skips[\"l3_out\"]))\n",
    "        x3 = self.refine(x3)\n",
    "\n",
    "        x4 = self.shared_crp4(self.shared_conv4(skips[\"l1_out\"]))\n",
    "        x4 = self.refine(x4)\n",
    "\n",
    "        return {\"x1\": x1, \"x2\": x2, \"x3\": x3, \"x4\": x4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2783c167-438d-4093-9b90-256011e88b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaskOutputLayer(nn.Module):\n",
    "    def __init__(self, output_channels):\n",
    "        \"\"\"\n",
    "        Task-specific output layers for generating final predictions.\n",
    "        Args:\n",
    "            output_channels (int): Number of output channels (e.g., 20 for segmentation, 1 for depth).\n",
    "        \"\"\"\n",
    "        super(TaskOutputLayer, self).__init__()\n",
    "        self.final_conv = nn.Conv2d(256, output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, input_size):\n",
    "        \"\"\"\n",
    "        Generate task-specific output.\n",
    "        Args:\n",
    "            x (Tensor): Input feature map.\n",
    "            input_size (tuple): Original input size (H, W).\n",
    "        Returns:\n",
    "            Tensor: Task-specific output.\n",
    "        \"\"\"\n",
    "        x = self.final_conv(x)\n",
    "        return nn.functional.interpolate(x, size=input_size, mode=\"bilinear\", align_corners=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ac107db7-0257-45ec-b7af-e09ff552760d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaskSpecificDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(TaskSpecificDiscriminator, self).__init__()\n",
    "        self.adapt_conv = nn.Conv2d(input_channels+input_channels, input_channels, kernel_size=1, bias=False)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, task_output, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the discriminator.\n",
    "\n",
    "        Args:\n",
    "            task_output (Tensor): Output from the generator (e.g., seg_output or depth_output).\n",
    "            labels (Tensor, optional): Ground truth labels. If provided, aligns channels with task_output.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Discriminator's prediction.\n",
    "        \"\"\"\n",
    "        if labels is not None:\n",
    "            # Ensure labels match the shape of task_output\n",
    "            if labels.dim() < task_output.dim():\n",
    "                labels = labels.unsqueeze(1)  # Add channel dimension if needed\n",
    "            if labels.size(1) != task_output.size(1):\n",
    "                labels = torch.nn.functional.one_hot(labels.squeeze(1), num_classes=task_output.size(1))\n",
    "                labels = labels.permute(0, 3, 1, 2).float().to(task_output.device)\n",
    "            combined = torch.cat([task_output, labels], dim=1)\n",
    "            combined = self.adapt_conv(combined)\n",
    "        else:\n",
    "            combined = task_output\n",
    "\n",
    "        return self.model(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "dc64fce4-dfa5-4884-addd-2d46b50dba0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiTaskDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        \"\"\"\n",
    "        Multi-Task Discriminator for evaluating all task-specific outputs.\n",
    "        Args:\n",
    "            input_channels (int): Number of input channels for concatenated features and outputs.\n",
    "        \"\"\"\n",
    "        super(MultiTaskDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Evaluate input image and task-specific outputs.\n",
    "        Args:\n",
    "            inputs (Tensor): Input image.\n",
    "            outputs (list[Tensor]): List of task-specific outputs.\n",
    "        Returns:\n",
    "            Tensor: Discriminator output.\n",
    "        \"\"\"\n",
    "        # combined = torch.cat([inputs] + outputs, dim=1)\n",
    "        return self.model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "cd983afc-6f6f-4d21-ab3e-9d45f90e78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone, num_seg_classes=20, depth_channels=1):\n",
    "        \"\"\"\n",
    "        Multi-task model with shared Pix2Pix Generator, task-specific discriminators,\n",
    "        and a multi-task discriminator.\n",
    "        Args:\n",
    "            backbone (nn.Module): Encoder backbone for feature extraction.\n",
    "            num_seg_classes (int): Number of segmentation classes.\n",
    "            depth_channels (int): Number of output channels for depth.\n",
    "        \"\"\"\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.feature_generator = MobileNetV3Backbone(backbone)\n",
    "        self.shared_generator = EnhancedSharedGenerator()\n",
    "        self.seg_output_layer = TaskOutputLayer(output_channels=num_seg_classes)\n",
    "        self.depth_output_layer = TaskOutputLayer(output_channels=depth_channels)\n",
    "\n",
    "        # Task-specific discriminators\n",
    "        self.seg_discriminator = TaskSpecificDiscriminator(input_channels=num_seg_classes)\n",
    "        self.depth_discriminator = TaskSpecificDiscriminator(input_channels=depth_channels)\n",
    "\n",
    "        # Multi-task discriminator\n",
    "        self.multi_task_discriminator = MultiTaskDiscriminator(input_channels= num_seg_classes + depth_channels)\n",
    "        \n",
    "    def forward(self, inputs, input_size, seg_labels=None, depth_labels=None, return_discriminator_outputs=False):\n",
    "        # Extract features from the encoder\n",
    "        skips = self.feature_generator(inputs)\n",
    "        shared_features = self.shared_generator(skips)\n",
    "\n",
    "        # Task-specific outputs\n",
    "        seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "        depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "        output_dict = {\n",
    "            \"seg_output\": seg_output,\n",
    "            \"depth_output\": depth_output,\n",
    "        }\n",
    "\n",
    "        if return_discriminator_outputs:\n",
    "            \n",
    "            # Detach outputs to prevent discriminator backward from interfering with the generator\n",
    "            seg_output_detached = seg_output.detach()\n",
    "            depth_output_detached = depth_output.detach()\n",
    "            \n",
    "            # Adversarial feedback from task-specific discriminators\n",
    "            seg_real_disc = self.seg_discriminator(seg_output_detached, seg_labels) if seg_labels is not None else None\n",
    "            seg_fake_disc = self.seg_discriminator(seg_output_detached, None)\n",
    "\n",
    "            depth_real_disc = self.depth_discriminator(depth_output_detached, depth_labels) if depth_labels is not None else None\n",
    "            depth_fake_disc = self.depth_discriminator(depth_output_detached, None)\n",
    "\n",
    "            # Multi-task discriminator feedback\n",
    "            combined_real_input = torch.cat([ seg_labels, depth_labels], dim=1) if seg_labels is not None and depth_labels is not None else None\n",
    "            combined_fake_input = torch.cat([ seg_output, depth_output], dim=1)\n",
    "\n",
    "            combined_real_disc = self.multi_task_discriminator(combined_real_input) if combined_real_input is not None else None\n",
    "            combined_fake_disc = self.multi_task_discriminator(combined_fake_input.detach())\n",
    "\n",
    "            output_dict.update({\n",
    "                \"seg_real_disc\": seg_real_disc,\n",
    "                \"seg_fake_disc\": seg_fake_disc,\n",
    "                \"depth_real_disc\": depth_real_disc,\n",
    "                \"depth_fake_disc\": depth_fake_disc,\n",
    "                \"combined_real_disc\": combined_real_disc,\n",
    "                \"combined_fake_disc\": combined_fake_disc,\n",
    "            })\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "#     def forward(self, inputs, input_size, seg_labels=None, depth_labels=None, return_discriminator_outputs=False):\n",
    "#         \"\"\"\n",
    "#         Forward pass for multi-task model.\n",
    "#         Args:\n",
    "#             inputs (Tensor): Input images.\n",
    "#             input_size (tuple): Original input size.\n",
    "#             seg_labels (Tensor, optional): Ground truth segmentation labels. Required for discriminator feedback.\n",
    "#             depth_labels (Tensor, optional): Ground truth depth labels. Required for discriminator feedback.\n",
    "#             return_discriminator_outputs (bool): If True, returns discriminator outputs for adversarial loss.\n",
    "#         Returns:\n",
    "#             dict: Outputs for segmentation and depth tasks, and optionally discriminator outputs.\n",
    "#         \"\"\"\n",
    "#         skips = self.feature_generator(inputs)\n",
    "#         shared_features = self.shared_generator(skips)\n",
    "\n",
    "#         # Task-specific outputs\n",
    "#         seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "#         depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "#         if return_discriminator_outputs:\n",
    "#             # Adversarial feedback from task-specific discriminators\n",
    "#             seg_real_disc = self.seg_discriminator(seg_output, seg_labels) if seg_labels is not None else None\n",
    "#             depth_real_disc = self.depth_discriminator(depth_output, depth_labels) if depth_labels is not None else None\n",
    "\n",
    "#             # Multi-task discriminator feedback\n",
    "#             combined_real_disc = self.multi_task_discriminator(inputs, [seg_output, depth_output])\n",
    "\n",
    "#             return {\n",
    "#                 \"seg_output\": seg_output,\n",
    "#                 \"depth_output\": depth_output,\n",
    "#                 \"seg_real_disc\": seg_real_disc,\n",
    "#                 \"depth_real_disc\": depth_real_disc,\n",
    "#                 \"combined_real_disc\": combined_real_disc\n",
    "#             }\n",
    "\n",
    "#         return {\n",
    "#             \"seg_output\": seg_output,\n",
    "#             \"depth_output\": depth_output\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "adc322fb-dbb7-4cd2-a934-33118521696b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageSequence\n",
    "import os\n",
    "\n",
    "def combine_training_gifs(model_dir, save_dir2, output_path):\n",
    "    \"\"\"\n",
    "    Combine two training visualization GIFs into one.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Directory containing the first training GIF.\n",
    "        save_dir2: Directory containing the second training GIF.\n",
    "        output_path: Path to save the combined GIF.\n",
    "    \"\"\"\n",
    "    # Find the GIF files\n",
    "    model_dir_gif = [file for file in os.listdir(model_dir) if file.startswith(\"training_visualization\") and file.endswith(\".gif\")]\n",
    "    save_dir2_gif = [file for file in os.listdir(save_dir2) if file.startswith(\"training_visualization\") and file.endswith(\".gif\")]\n",
    "    \n",
    "    if not model_dir_gif or not save_dir2_gif:\n",
    "        raise FileNotFoundError(\"Could not find training_visualization_*.gif in one of the directories.\")\n",
    "    \n",
    "    model_dir_gif_path = os.path.join(model_dir, model_dir_gif[0])\n",
    "    save_dir2_gif_path = os.path.join(save_dir2, save_dir2_gif[0])\n",
    "\n",
    "    # Open the GIFs\n",
    "    gif1 = Image.open(model_dir_gif_path)\n",
    "    gif2 = Image.open(save_dir2_gif_path)\n",
    "\n",
    "    # Collect all frames from both GIFs\n",
    "    combined_frames = []\n",
    "    for frame in ImageSequence.Iterator(gif1):\n",
    "        combined_frames.append(frame.copy())\n",
    "    for frame in ImageSequence.Iterator(gif2):\n",
    "        combined_frames.append(frame.copy())\n",
    "\n",
    "    # Save the combined GIF\n",
    "    combined_frames[0].save(\n",
    "        output_path,\n",
    "        save_all=True,\n",
    "        append_images=combined_frames[1:],\n",
    "        duration=gif1.info.get(\"duration\", 500),  # Use duration from the first GIF\n",
    "        loop=0\n",
    "    )\n",
    "\n",
    "    print(f\"Combined GIF saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b6e25-1715-412c-b906-714e85c0e266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dce8dd65-e71e-4d5b-86b0-a28e02fdaa23",
   "metadata": {},
   "source": [
    "# Saving loss charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ce716570-bcca-40b6-91be-72d6713ff523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_all_losses(epoch, train_losses,valid_losses,save_dir):\n",
    "    # Plot training and validation losses\n",
    "    for key in train_losses.keys():\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses[key], label=f\"Train {key}\")\n",
    "        plt.plot(valid_losses[key], label=f\"Valid {key}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(key.replace(\"_\", \" \").title())\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, f\"{key}_loss_after_epoch_{epoch}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11165c9f-97c8-4240-9985-956b229c2656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d7f380-f308-45f2-a57e-7cfaa4dd9b28",
   "metadata": {},
   "source": [
    "# saving checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "cf7ada56-870b-4926-b793-a9e3468afcaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save checkpoint including model, optimizer, and scheduler states\n",
    "def save_checkpoint(model, opt_sched, save_path, epoch, best_loss):\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_states\": {name: opt.state_dict() for name, opt in opt_sched[\"optimizers\"].items()},\n",
    "        \"scheduler_states\": {name: sched.state_dict() for name, sched in opt_sched[\"schedulers\"].items()},\n",
    "        \"epoch\": epoch,\n",
    "        \"best_loss\": best_loss\n",
    "    }\n",
    "    torch.save(checkpoint, save_path, _use_new_zipfile_serialization=True)\n",
    "    \n",
    "def load_checkpoint(model, opt_sched, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "    for name, opt in opt_sched[\"optimizers\"].items():\n",
    "        if name in checkpoint[\"optimizer_states\"]:\n",
    "            opt.load_state_dict(checkpoint[\"optimizer_states\"][name])\n",
    "            \n",
    "    for name, sched in opt_sched[\"schedulers\"].items():\n",
    "        if name in checkpoint[\"scheduler_states\"]:\n",
    "            sched.load_state_dict(checkpoint[\"scheduler_states\"][name])\n",
    "            \n",
    "    # return checkpoint[\"epoch\"], checkpoint[\"best_loss\"]\n",
    "    return checkpoint.get(\"epoch\", 0), checkpoint.get(\"best_loss\", float(\"inf\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "43d19b7e-72cb-4674-9d20-b86afb22c7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "def combine_and_plot_loss_data(model_dir, save_dir2, combined_save_dir=\"all_data_from_prev_curr_epoch\"):\n",
    "    \"\"\"\n",
    "    Combines loss data from previous and current training sessions and plots combined graphs.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Path to the directory containing the previous loss-tracking CSV.\n",
    "        save_dir2: Path to the directory containing the current loss-tracking CSV.\n",
    "        combined_save_dir: Path to save the combined data and plots.\n",
    "\n",
    "    Returns:\n",
    "        combined_df: A pandas DataFrame containing the combined loss data.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    combined_save_dir = os.path.join(save_dir2, combined_save_dir)\n",
    "    os.makedirs(combined_save_dir, exist_ok=True)\n",
    "\n",
    "    # Locate CSV files\n",
    "    previous_csv = os.path.join(model_dir, [file for file in os.listdir(model_dir) if file.endswith(\".csv\")][0])\n",
    "    current_csv = os.path.join(save_dir2, [file for file in os.listdir(save_dir2) if file.endswith(\".csv\")][0])\n",
    "\n",
    "    # Load data into pandas DataFrames\n",
    "    previous_df = pd.read_csv(previous_csv)\n",
    "    current_df = pd.read_csv(current_csv)\n",
    "\n",
    "    # Update epoch numbers in the current DataFrame\n",
    "    max_prev_epoch = previous_df[\"epoch\"].max()\n",
    "    current_df[\"epoch\"] += max_prev_epoch\n",
    "\n",
    "    # Combine the DataFrames\n",
    "    combined_df = pd.concat([previous_df, current_df], ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame\n",
    "    combined_csv_path = os.path.join(combined_save_dir, \"combined_loss_tracking.csv\")\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Combined loss data saved to {combined_csv_path}\")\n",
    "\n",
    "    # # Generate plots for each loss type\n",
    "    # loss_columns = [\"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\", \"train_adv_loss\",\n",
    "    #                 \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\", \"valid_adv_loss\"]\n",
    "    # for col in loss_columns:\n",
    "    #     plt.figure()\n",
    "    #     plt.plot(combined_df[\"epoch\"], combined_df[col], label=col)\n",
    "    #     plt.xlabel(\"Epoch\")\n",
    "    #     plt.ylabel(\"Loss\")\n",
    "    #     plt.title(f\"{col.replace('_', ' ').title()} Over Epochs\")\n",
    "    #     plt.legend()\n",
    "    #     plot_path = os.path.join(combined_save_dir, f\"{col}_plot.png\")\n",
    "    #     plt.savefig(plot_path)\n",
    "    #     plt.close()\n",
    "    #     print(f\"Plot saved to {plot_path}\")\n",
    "    # Generate combined plots for train and valid losses\n",
    "    \n",
    "    loss_types = [\"seg_loss\", \"depth_loss\", \"combined_loss\", \"adv_loss\"]\n",
    "    for loss_type in loss_types:\n",
    "        train_loss_col = f\"train_{loss_type}\"\n",
    "        valid_loss_col = f\"valid_{loss_type}\"\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(combined_df[\"epoch\"], combined_df[train_loss_col], label=f\"Train {loss_type.capitalize()}\")\n",
    "        plt.plot(combined_df[\"epoch\"], combined_df[valid_loss_col], label=f\"Valid {loss_type.capitalize()}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"{loss_type.replace('_', ' ').capitalize()} Over Epochs\")\n",
    "        plt.legend()\n",
    "        plot_path = os.path.join(combined_save_dir, f\"{loss_type}_train_valid_plot.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Combined plot saved to {plot_path}\")\n",
    "\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e0a88c39-c135-4bcc-9995-732df53562bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "bf9c5647-e263-48ba-a1c5-f2d1898da89c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes',\n",
       " 'results_test8')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd(),'results_test8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6cf32-b092-4ee0-b628-f4b9ab3cf990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb691baa-d9fc-4d44-9abb-305a026d5847",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "238ab35f-3458-488c-b170-a1def0a1b714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_with_adversarial_loss_tracking(\n",
    "    model, train_loader, valid_loader, num_epochs, device, opt_sched, save_dir=\"results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a multi-task model with adversarial feedback and tracks losses.\n",
    "    \n",
    "    Args:\n",
    "        model: Multi-task model with integrated generators and discriminators.\n",
    "        train_loader: DataLoader for training data.\n",
    "        valid_loader: DataLoader for validation data.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        device: Device for training (\"cuda\" or \"cpu\").\n",
    "        opt_sched: Dictionary of optimizers and schedulers for generators and discriminators.\n",
    "        save_dir: Directory to save results.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, valid_losses: Lists of losses for training and validation.\n",
    "    \"\"\"\n",
    "    # Create directories for saving results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = os.path.join(save_dir, timestamp)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare CSV file for loss tracking\n",
    "    csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "    gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "    \n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "            \"train_adv_loss\", \n",
    "            # \"train_seg_iou\",\n",
    "            \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "            \"valid_adv_loss\", \n",
    "            # \"valid_seg_iou\"\n",
    "        ])\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    best_combined_loss = float(\"inf\")\n",
    "    gif_frames =[]\n",
    "    perceptual_loss_fn = PerceptualLoss(pretrained_model=\"vgg16\").to(device)\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "        num_batches = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", unit=\"batch\") as pbar:\n",
    "            for batch in train_loader:\n",
    "                inputs, seg_labels, depth_labels = (\n",
    "                    batch[\"left\"].to(device),\n",
    "                    batch[\"mask\"].to(device),\n",
    "                    batch[\"depth\"].to(device),\n",
    "                )\n",
    "                input_size = inputs.size()[-2:]\n",
    "\n",
    "                # Preprocess seg_labels to one-hot encoding\n",
    "                if seg_labels.size(1) == 1:  # If class indices are given\n",
    "                    seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "                    seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)  # Convert to [B, C, H, W]\n",
    "\n",
    "                # Ensure depth_labels has correct dimensions\n",
    "                if depth_labels.dim() == 5:  # If depth_labels has extra dimensions\n",
    "                    depth_labels = depth_labels.squeeze(2)\n",
    "\n",
    "                # Zero gradients\n",
    "                for optimizer in opt_sched[\"optimizers\"].values():\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass with discriminator outputs\n",
    "                outputs = model(\n",
    "                    inputs,\n",
    "                    input_size=input_size,\n",
    "                    seg_labels=seg_labels,\n",
    "                    depth_labels=depth_labels,\n",
    "                    return_discriminator_outputs=True,\n",
    "                )\n",
    "\n",
    "                # Generator losses\n",
    "                seg_loss = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + \\\n",
    "                           dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "                depth_loss = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                             inv_huber_loss(outputs[\"depth_output\"], depth_labels) \n",
    "                # + \\\n",
    "                #              depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "                \n",
    "                seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "                depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "                \n",
    "                seg_loss = seg_loss + 0.1 * seg_perceptual_loss\n",
    "                depth_loss = depth_loss + 0.1 * depth_perceptual_loss\n",
    "                \n",
    "                \n",
    "                # Adversarial Losses\n",
    "                gen_adv_loss_seg, disc_loss_seg = compute_adversarial_losses(\n",
    "                    model.seg_discriminator,\n",
    "                    outputs[\"seg_real_disc\"],\n",
    "                    outputs[\"seg_fake_disc\"].detach(),\n",
    "                    hinge_loss=False,  # Switch to False for WGAN-GP\n",
    "                    gradient_penalty=True,\n",
    "                    real_inputs=seg_labels,  # Use only for WGAN-GP\n",
    "                )\n",
    "\n",
    "                gen_adv_loss_depth, disc_loss_depth = compute_adversarial_losses(\n",
    "                    model.depth_discriminator,\n",
    "                    outputs[\"depth_real_disc\"],\n",
    "                    outputs[\"depth_fake_disc\"].detach(),\n",
    "                    hinge_loss=False,\n",
    "                    gradient_penalty=True,\n",
    "                    real_inputs=depth_labels,\n",
    "                )\n",
    "\n",
    "                gen_adv_loss_combined, disc_loss_combined = compute_adversarial_losses(\n",
    "                    model.multi_task_discriminator,\n",
    "                    outputs[\"combined_real_disc\"],\n",
    "                    outputs[\"combined_fake_disc\"].detach(),\n",
    "                    hinge_loss=False,\n",
    "                    gradient_penalty=True,\n",
    "                    real_inputs=torch.cat([seg_labels, depth_labels], dim=1) if seg_labels is not None else None,\n",
    "                )\n",
    "                \n",
    "\n",
    "                # adv_loss = -(\n",
    "                #     torch.mean(outputs[\"seg_real_disc\"]) +\n",
    "                #     torch.mean(outputs[\"depth_real_disc\"]) +\n",
    "                #     torch.mean(outputs[\"combined_real_disc\"])\n",
    "                # )\n",
    "                # Total Generator Loss\n",
    "                gen_adv_loss =1.5 * gen_adv_loss_seg + gen_adv_loss_depth + gen_adv_loss_combined\n",
    "                combined_loss = 2 * seg_loss + depth_loss +  0.1 * gen_adv_loss\n",
    "\n",
    "                # combined_loss = seg_loss + depth_loss + 0.01 * adv_loss\n",
    "\n",
    "                # Backpropagation for generators\n",
    "                combined_loss.backward(retain_graph=True)\n",
    "                # opt_sched[\"optimizers\"][\"generator\"].step()\n",
    "                opt_sched[\"optimizers\"][\"shared_gen\"].step()\n",
    "                opt_sched[\"optimizers\"][\"shared_refine\"].step()\n",
    "                opt_sched[\"optimizers\"][\"seg_gen\"].step()\n",
    "                opt_sched[\"optimizers\"][\"depth_gen\"].step()\n",
    "\n",
    "\n",
    "                # # Update task-specific discriminators\n",
    "                # for task, disc_optimizer in [\n",
    "                #     (\"seg\", \"seg_disc\"),\n",
    "                #     (\"depth\", \"depth_disc\"),\n",
    "                # ]:\n",
    "                #     opt_sched[\"optimizers\"][disc_optimizer].zero_grad()\n",
    "                #     real_disc_loss = torch.mean(\n",
    "                #         (outputs[f\"{task}_real_disc\"] - 1) ** 2\n",
    "                #     )\n",
    "                #     fake_disc_loss = torch.mean(\n",
    "                #         (outputs[f\"{task}_fake_disc\"].detach()) ** 2\n",
    "                #     )\n",
    "                #     disc_loss = (real_disc_loss + fake_disc_loss) / 2\n",
    "                #     disc_loss.backward()\n",
    "                #     opt_sched[\"optimizers\"][disc_optimizer].step()\n",
    "                \n",
    "                # Backpropagation for discriminators\n",
    "                for task, disc_loss in [(\"seg\", disc_loss_seg), (\"depth\", disc_loss_depth)]:\n",
    "                    opt_sched[\"optimizers\"][f\"{task}_disc\"].zero_grad()\n",
    "                    disc_loss.backward()\n",
    "                    opt_sched[\"optimizers\"][f\"{task}_disc\"].step()\n",
    "\n",
    "                # Update multi-task discriminator\n",
    "                opt_sched[\"optimizers\"][\"multi_task_disc\"].zero_grad()\n",
    "                disc_loss_combined.backward()\n",
    "#                 real_combined_loss = torch.mean(\n",
    "#                     (outputs[\"combined_real_disc\"] - 1) ** 2\n",
    "#                 )\n",
    "#                 fake_combined_loss = torch.mean(\n",
    "#                     (outputs[\"combined_fake_disc\"].detach()) ** 2\n",
    "#                 )\n",
    "                \n",
    "#                 combined_disc_loss = (real_combined_loss + fake_combined_loss) / 2\n",
    "#                 combined_disc_loss.backward()\n",
    "                opt_sched[\"optimizers\"][\"multi_task_disc\"].step()\n",
    "\n",
    "                # Update training metrics\n",
    "                epoch_train[\"seg\"] += seg_loss.item()\n",
    "                epoch_train[\"depth\"] += depth_loss.item()\n",
    "                epoch_train[\"combined\"] += combined_loss.item()\n",
    "                epoch_train[\"adv\"] += gen_adv_loss.item()\n",
    "                # epoch_train[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "                num_batches += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Average training metrics\n",
    "            for key in epoch_train.keys():\n",
    "                train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "            num_valid_batches = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    inputs, seg_labels, depth_labels = (\n",
    "                        batch[\"left\"].to(device),\n",
    "                        batch[\"mask\"].to(device),\n",
    "                        batch[\"depth\"].to(device),\n",
    "                    )\n",
    "                    input_size = inputs.size()[-2:]\n",
    "\n",
    "                    # Preprocess seg_labels to one-hot encoding\n",
    "                    if seg_labels.size(1) == 1:\n",
    "                        seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "                        seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "                    # Ensure depth_labels has correct dimensions\n",
    "                    if depth_labels.dim() == 5:\n",
    "                        depth_labels = depth_labels.squeeze(2)\n",
    "\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(\n",
    "                        inputs,\n",
    "                        input_size=input_size,\n",
    "                        seg_labels=seg_labels,\n",
    "                        depth_labels=depth_labels,\n",
    "                        return_discriminator_outputs=True,\n",
    "                    )\n",
    "\n",
    "                    # Validation loss calculations\n",
    "                    seg_loss = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + \\\n",
    "                               dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "                    depth_loss = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels) + \\\n",
    "                                 inv_huber_loss(outputs[\"depth_output\"], depth_labels) \n",
    "                    # + \\\n",
    "                    #              depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "                    \n",
    "                    seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "                    depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "\n",
    "                    seg_loss = seg_loss + 0.1 * seg_perceptual_loss\n",
    "                    depth_loss = depth_loss + 0.1 * depth_perceptual_loss\n",
    "                    \n",
    "                    # Adversarial Losses (No Backpropagation)\n",
    "                    gen_adv_loss_seg, _ = compute_adversarial_losses(\n",
    "                        model.seg_discriminator,\n",
    "                        outputs[\"seg_real_disc\"], outputs[\"seg_fake_disc\"], hinge_loss=False,gradient_penalty=False,\n",
    "                    )\n",
    "                    gen_adv_loss_depth, _ = compute_adversarial_losses(\n",
    "                        model.depth_discriminator,\n",
    "                        outputs[\"depth_real_disc\"], outputs[\"depth_fake_disc\"], hinge_loss=False,gradient_penalty=False,\n",
    "                    )\n",
    "                    gen_adv_loss_combined, _ = compute_adversarial_losses(\n",
    "                        model.multi_task_discriminator,\n",
    "                        outputs[\"combined_real_disc\"], outputs[\"combined_fake_disc\"], hinge_loss=False,gradient_penalty=False,\n",
    "                        real_inputs=torch.cat([seg_labels, depth_labels], dim=1) if seg_labels is not None else None,\n",
    "                    )\n",
    "\n",
    "                    gen_adv_loss = 1.5 * gen_adv_loss_seg + gen_adv_loss_depth + gen_adv_loss_combined\n",
    "                    combined_loss = 2 * seg_loss + depth_loss + 0.1 * gen_adv_loss\n",
    "\n",
    "\n",
    "#                     adv_loss = -(\n",
    "#                         torch.mean(outputs[\"seg_real_disc\"]) +\n",
    "#                         torch.mean(outputs[\"depth_real_disc\"]) +\n",
    "#                         torch.mean(outputs[\"combined_real_disc\"])\n",
    "#                     )\n",
    "\n",
    "#                     combined_loss = seg_loss + depth_loss + 0.01 * adv_loss\n",
    "\n",
    "                    # Update validation metrics\n",
    "                    epoch_valid[\"seg\"] += seg_loss.item()\n",
    "                    epoch_valid[\"depth\"] += depth_loss.item()\n",
    "                    epoch_valid[\"combined\"] += combined_loss.item()\n",
    "                    epoch_valid[\"adv\"] += gen_adv_loss.item()\n",
    "                    # epoch_valid[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "                    num_valid_batches += 1\n",
    "\n",
    "        # Average validation metrics\n",
    "        for key in epoch_valid.keys():\n",
    "            valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "        # Save best model\n",
    "        valid_combined_loss = (epoch_valid[\"combined\"] / num_valid_batches) + 0.01 * (epoch_valid[\"adv\"] / num_valid_batches)\n",
    "        if valid_combined_loss < best_combined_loss:\n",
    "            best_combined_loss = valid_combined_loss\n",
    "            # torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "            checkpoint_path = os.path.join(save_dir, \"best_model_checkpoint.pth\")\n",
    "            save_checkpoint(model, opt_sched, checkpoint_path, epoch + 1, best_combined_loss)\n",
    "            print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "        frame = save_training_visualization_as_gif2(epoch, inputs, outputs[\"seg_output\"], outputs[\"depth_output\"], torch.argmax(seg_labels, dim=1), depth_labels)\n",
    "        gif_frames.append(frame)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Append metrics to CSV\n",
    "        with open(csv_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                epoch + 1,\n",
    "                epoch_train[\"seg\"] / num_batches,\n",
    "                epoch_train[\"depth\"] / num_batches,\n",
    "                epoch_train[\"combined\"] / num_batches,\n",
    "                epoch_train[\"adv\"] / num_batches,\n",
    "                # epoch_train[\"iou\"] / num_batches,\n",
    "                epoch_valid[\"seg\"] / num_valid_batches,\n",
    "                epoch_valid[\"depth\"] / num_valid_batches,\n",
    "                epoch_valid[\"combined\"] / num_valid_batches,\n",
    "                epoch_valid[\"adv\"] / num_valid_batches,\n",
    "                # epoch_valid[\"iou\"] / num_valid_batches,\n",
    "            ])\n",
    "            \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Results:\")\n",
    "\n",
    "        # Print training losses\n",
    "        print(f\"  Train Losses - Segmentation: {epoch_train['seg']/num_batches:.4f}, Depth: {epoch_train['depth']/num_batches:.4f}, \"\n",
    "              f\"Combined: {epoch_train['combined']/num_batches:.4f}, Adversarial: {epoch_train['adv']/num_batches:.4f}\")\n",
    "\n",
    "        # Print validation losses\n",
    "        print(f\"  Valid Losses - Segmentation: {epoch_valid['seg']/ num_valid_batches:.4f}, Depth: {epoch_valid['depth']/ num_valid_batches:.4f}, \"\n",
    "              f\"Combined: {epoch_valid['combined']/ num_valid_batches:.4f}, Adversarial: {epoch_valid['adv']/ num_valid_batches:.4f}\")\n",
    "\n",
    "\n",
    "        # Update schedulers\n",
    "        for name, scheduler in opt_sched[\"schedulers\"].items():\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                # Pass the appropriate metric to ReduceLROnPlateau\n",
    "                scheduler.step(valid_losses[\"combined\"][-1])  # Use the most recent validation combined loss\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "        if epoch %10 == 0:\n",
    "            gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "            gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "            plot_all_losses(epoch, train_losses,valid_losses,save_dir)\n",
    "            \n",
    "    \n",
    "    gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "    print(f\"Training visualization saved as GIF at {gif_path}\")\n",
    "    plot_all_losses(epoch, train_losses,valid_losses,save_dir)\n",
    "\n",
    "    \n",
    "    return train_losses, valid_losses, save_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bc558fd7-917a-4782-8edf-6f0ec696f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resume_training_with_loss_tracking(\n",
    "    model_class,\n",
    "    model_dir,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_additional_epochs,\n",
    "    device,\n",
    "    opt_sched,\n",
    "    save_dir,\n",
    "):\n",
    "    \"\"\"\n",
    "    Resumes training a multi-task model, appends loss data to the existing CSV file,\n",
    "    and generates graphs for the combined training history.\n",
    "\n",
    "    Args:\n",
    "        model_class: The model class to instantiate.\n",
    "        model_dir: Path to the directory containing the saved model and loss CSV file.\n",
    "        train_loader: DataLoader for training data.\n",
    "        valid_loader: DataLoader for validation data.\n",
    "        num_additional_epochs: Number of additional epochs to train.\n",
    "        device: Device for training (\"cuda\" or \"cpu\").\n",
    "        opt_sched: Dictionary of optimizers and schedulers.\n",
    "        save_dir: Directory to save the updated results.\n",
    "\n",
    "    Returns:\n",
    "        Updated train and validation losses.\n",
    "    \"\"\"\n",
    "    # # Load the best model\n",
    "    # best_model_path = os.path.join(model_dir, \"best_model.pth\")\n",
    "    # if not os.path.exists(best_model_path):\n",
    "    #     raise FileNotFoundError(f\"Best model not found at {best_model_path}\")\n",
    "        \n",
    "    # Load the checkpoint\n",
    "    checkpoint_path = os.path.join(model_dir, \"best_model_checkpoint.pth\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "    \n",
    "        \n",
    "    model = model_class().to(device)\n",
    "    \n",
    "    start_epoch, best_loss = load_checkpoint(model, opt_sched, checkpoint_path, device)\n",
    "\n",
    "    # model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "    # Locate the existing loss CSV\n",
    "    csv_path = os.path.join(model_dir, [file for file in os.listdir(model_dir) if file.endswith(\".csv\")][0])\n",
    "\n",
    "    # Parse existing CSV data\n",
    "    existing_train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    existing_valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"adv\": []}\n",
    "    current_epoch = 0\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            current_epoch = int(row[0])\n",
    "            existing_train_losses[\"seg\"].append(float(row[1]))\n",
    "            existing_train_losses[\"depth\"].append(float(row[2]))\n",
    "            existing_train_losses[\"combined\"].append(float(row[3]))\n",
    "            existing_train_losses[\"adv\"].append(float(row[4]))\n",
    "            existing_valid_losses[\"seg\"].append(float(row[5]))\n",
    "            existing_valid_losses[\"depth\"].append(float(row[6]))\n",
    "            existing_valid_losses[\"combined\"].append(float(row[7]))\n",
    "            existing_valid_losses[\"adv\"].append(float(row[8]))\n",
    "\n",
    "    # Train for additional epochs\n",
    "    \n",
    "    train_losses, valid_losses,save_dir2 = train_model_with_adversarial_loss_tracking(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        num_epochs=num_additional_epochs,\n",
    "        device=device,\n",
    "        opt_sched=opt_sched,\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "\n",
    "    # Combine the new losses with the existing ones\n",
    "    for key in existing_train_losses.keys():\n",
    "        existing_train_losses[key].extend(train_losses[key])\n",
    "        existing_valid_losses[key].extend(valid_losses[key])\n",
    "        \n",
    "    save_dir3 = os.path.join(save_dir2,\"combined_result\")\n",
    "    os.makedirs(save_dir3, exist_ok=True)\n",
    "    \n",
    "    total_epochs = len(existing_train_losses[\"seg\"])\n",
    "        \n",
    "    updated_csv_path = os.path.join(save_dir3, f\"loss_tracking_updated_{total_epochs}.csv\")\n",
    "    os.makedirs(save_dir3, exist_ok=True)\n",
    "\n",
    "    # Write combined losses to the updated CSV\n",
    "    with open(updated_csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "            \"train_adv_loss\", \n",
    "            \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "            \"valid_adv_loss\", \n",
    "        ])\n",
    "        for epoch in range(len(existing_train_losses[\"seg\"])):\n",
    "            writer.writerow([\n",
    "                epoch + 1,\n",
    "                existing_train_losses[\"seg\"][epoch],\n",
    "                existing_train_losses[\"depth\"][epoch],\n",
    "                existing_train_losses[\"combined\"][epoch],\n",
    "                existing_train_losses[\"adv\"][epoch],\n",
    "                existing_valid_losses[\"seg\"][epoch],\n",
    "                existing_valid_losses[\"depth\"][epoch],\n",
    "                existing_valid_losses[\"combined\"][epoch],\n",
    "                existing_valid_losses[\"adv\"][epoch],\n",
    "            ])\n",
    "\n",
    "    # Generate graphs\n",
    "    for key in existing_train_losses.keys():\n",
    "        plt.figure()\n",
    "        plt.plot(range(len(existing_train_losses[key])), existing_train_losses[key], label=f\"Train {key.capitalize()}\")\n",
    "        plt.plot(range(len(existing_valid_losses[key])), existing_valid_losses[key], label=f\"Valid {key.capitalize()}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(f\"{key.capitalize()} Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"{key.capitalize()} Loss Over Epochs\")\n",
    "        plt.savefig(os.path.join(save_dir3, f\"{key}_loss_graph_epoch_{total_epochs}.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    output_path = os.path.join(save_dir3,'combined_results.gif')\n",
    "    combine_training_gifs(model_dir, save_dir2, output_path)\n",
    "\n",
    "    return existing_train_losses, existing_valid_losses,save_dir2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89effbbd-8297-4736-9071-94032be044d0",
   "metadata": {},
   "source": [
    "# for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0bea1522-f3cb-4205-af30-d041364621fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_pth_files(directory):\n",
    "    \"\"\"\n",
    "    Lists all .pth files in a specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory to search.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of .pth file paths.\n",
    "    \"\"\"\n",
    "    pth_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.pth')]\n",
    "    return pth_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a21ad24b-2e29-41c9-9027-4463110ecf28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/test11.pth',\n",
       " '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/test13.pth',\n",
       " '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/test8.pth',\n",
       " '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/test9.pth']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_dir = os.path.join(os.getcwd(),'Best_models')\n",
    "best_model_dir\n",
    "pth_files = list_pth_files(best_model_dir)\n",
    "pth_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f4166b22-9476-4c82-b888-860843733757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of .pth files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test11': '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/test11.pth',\n",
       " 'test13': '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/test13.pth',\n",
       " 'test8': '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/test8.pth',\n",
       " 'test9': '/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/test9.pth'}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"List of .pth files:\")\n",
    "model_map ={}\n",
    "for file in pth_files:\n",
    "    filename = file.split('/')[-1] \n",
    "    name_without_extension = filename.split('.')[0]  # Removes '.pth'\n",
    "    model_map[name_without_extension] = file\n",
    "    \n",
    "model_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "50cf4515-d1f3-46ac-a1ae-49d47ae41592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiTaskModel_old(nn.Module):\n",
    "    def __init__(self, backbone, num_seg_classes=20, depth_channels=1):\n",
    "        \"\"\"\n",
    "        Multi-task model with shared Pix2Pix Generator, task-specific discriminators,\n",
    "        and a multi-task discriminator.\n",
    "        Args:\n",
    "            backbone (nn.Module): Encoder backbone for feature extraction.\n",
    "            num_seg_classes (int): Number of segmentation classes.\n",
    "            depth_channels (int): Number of output channels for depth.\n",
    "        \"\"\"\n",
    "        super(MultiTaskModel_old, self).__init__()\n",
    "        self.feature_generator = MobileNetV3Backbone(backbone)\n",
    "        self.shared_generator = EnhancedSharedGenerator()\n",
    "        self.seg_output_layer = TaskOutputLayer(output_channels=num_seg_classes)\n",
    "        self.depth_output_layer = TaskOutputLayer(output_channels=depth_channels)\n",
    "\n",
    "        # Task-specific discriminators\n",
    "        self.seg_discriminator = TaskSpecificDiscriminator(input_channels=num_seg_classes)\n",
    "        self.depth_discriminator = TaskSpecificDiscriminator(input_channels=depth_channels)\n",
    "\n",
    "        # Multi-task discriminator\n",
    "        self.multi_task_discriminator = MultiTaskDiscriminator(input_channels=3 + num_seg_classes + depth_channels)\n",
    "        \n",
    "    def forward(self, inputs, input_size, seg_labels=None, depth_labels=None, return_discriminator_outputs=False):\n",
    "        # Extract features from the encoder\n",
    "        skips = self.feature_generator(inputs)\n",
    "        shared_features = self.shared_generator(skips)\n",
    "\n",
    "        # Task-specific outputs\n",
    "        seg_output = self.seg_output_layer(shared_features[\"x4\"], input_size)\n",
    "        depth_output = self.depth_output_layer(shared_features[\"x4\"], input_size)\n",
    "\n",
    "        output_dict = {\n",
    "            \"seg_output\": seg_output,\n",
    "            \"depth_output\": depth_output,\n",
    "        }\n",
    "\n",
    "        if return_discriminator_outputs:\n",
    "            \n",
    "            # Detach outputs to prevent discriminator backward from interfering with the generator\n",
    "            seg_output_detached = seg_output.detach()\n",
    "            depth_output_detached = depth_output.detach()\n",
    "            \n",
    "            # Adversarial feedback from task-specific discriminators\n",
    "            seg_real_disc = self.seg_discriminator(seg_output_detached, seg_labels) if seg_labels is not None else None\n",
    "            seg_fake_disc = self.seg_discriminator(seg_output_detached, None)\n",
    "\n",
    "            depth_real_disc = self.depth_discriminator(depth_output_detached, depth_labels) if depth_labels is not None else None\n",
    "            depth_fake_disc = self.depth_discriminator(depth_output_detached, None)\n",
    "\n",
    "            # Multi-task discriminator feedback\n",
    "            combined_real_input = torch.cat([inputs, seg_labels, depth_labels], dim=1) if seg_labels is not None and depth_labels is not None else None\n",
    "            combined_fake_input = torch.cat([inputs, seg_output, depth_output], dim=1)\n",
    "\n",
    "            combined_real_disc = self.multi_task_discriminator(combined_real_input) if combined_real_input is not None else None\n",
    "            combined_fake_disc = self.multi_task_discriminator(combined_fake_input.detach())\n",
    "\n",
    "            output_dict.update({\n",
    "                \"seg_real_disc\": seg_real_disc,\n",
    "                \"seg_fake_disc\": seg_fake_disc,\n",
    "                \"depth_real_disc\": depth_real_disc,\n",
    "                \"depth_fake_disc\": depth_fake_disc,\n",
    "                \"combined_real_disc\": combined_real_disc,\n",
    "                \"combined_fake_disc\": combined_fake_disc,\n",
    "            })\n",
    "\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "80a2581d-c683-41dd-9c8a-cccc38981b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def save_visualizations(models, val_loader, device, output_dir):\n",
    "#     \"\"\"\n",
    "#     Save visualizations of segmentation and depth outputs as PNG files.\n",
    "\n",
    "#     Args:\n",
    "#         models (list): List of PyTorch models.\n",
    "#         val_loader: Validation DataLoader.\n",
    "#         device: Device for inference (\"cuda\" or \"cpu\").\n",
    "#         output_dir: Directory to save the visualizations.\n",
    "#     \"\"\"\n",
    "#     # Ensure output directory exists\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Get the 4th element from val_loader\n",
    "#     sample_index = 3\n",
    "#     inputs, seg_labels, depth_labels = None, None, None\n",
    "#     for idx, batch in enumerate(val_loader):\n",
    "#         if idx == sample_index:\n",
    "#             inputs = batch[\"left\"].to(device)\n",
    "#             seg_labels = batch[\"mask\"].to(device)\n",
    "#             depth_labels = batch[\"depth\"].to(device)\n",
    "#             break\n",
    "\n",
    "#     # Ensure the sample was found\n",
    "#     if inputs is None or seg_labels is None or depth_labels is None:\n",
    "#         raise ValueError(f\"Sample with index {sample_index} not found in val_loader.\")\n",
    "\n",
    "#     # Get predictions from all models\n",
    "#     seg_outputs = []\n",
    "#     depth_outputs = []\n",
    "\n",
    "#     for model in models:\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(inputs)\n",
    "#             seg_outputs.append(outputs[\"seg_output\"])  # Segmentation output\n",
    "#             depth_outputs.append(outputs[\"depth_output\"])  # Depth output\n",
    "\n",
    "#     # Convert segmentation labels to one-hot if necessary\n",
    "#     if seg_labels.size(1) == 1:  # If labels are class indices\n",
    "#         seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "#         seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "#     # Save segmentation visualization\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "\n",
    "#     # Original Image\n",
    "#     plt.subplot(1, len(models) + 2, 1)\n",
    "#     plt.imshow(inputs[0].permute(1, 2, 0).cpu().numpy())\n",
    "#     plt.title(\"Original Image\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     # Original Segmentation Labels\n",
    "#     plt.subplot(1, len(models) + 2, 2)\n",
    "#     plt.imshow(torch.argmax(seg_labels[0], dim=0).cpu().numpy(), cmap=\"jet\")\n",
    "#     plt.title(\"Segmentation Labels\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     # Model Outputs for Segmentation\n",
    "#     for i, seg_output in enumerate(seg_outputs):\n",
    "#         plt.subplot(1, len(models) + 2, i + 3)\n",
    "#         plt.imshow(torch.argmax(seg_output[0], dim=0).cpu().numpy(), cmap=\"jet\")\n",
    "#         plt.title(f\"Model {i + 1}\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#     seg_output_path = os.path.join(output_dir, \"segmentation_visualization.png\")\n",
    "#     plt.savefig(seg_output_path)\n",
    "#     plt.close()\n",
    "#     print(f\"Segmentation visualization saved at: {seg_output_path}\")\n",
    "\n",
    "#     # Save depth visualization\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "\n",
    "#     # Original Image\n",
    "#     plt.subplot(1, len(models) + 2, 1)\n",
    "#     plt.imshow(inputs[0].permute(1, 2, 0).cpu().numpy())\n",
    "#     plt.title(\"Original Image\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     # Depth Labels\n",
    "#     plt.subplot(1, len(models) + 2, 2)\n",
    "#     plt.imshow(depth_labels[0].squeeze(0).cpu().numpy(), cmap=\"plasma\")\n",
    "#     plt.title(\"Depth Labels\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     # Model Outputs for Depth\n",
    "#     for i, depth_output in enumerate(depth_outputs):\n",
    "#         plt.subplot(1, len(models) + 2, i + 3)\n",
    "#         plt.imshow(depth_output[0].squeeze(0).cpu().numpy(), cmap=\"plasma\")\n",
    "#         plt.title(f\"Model {i + 1}\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#     depth_output_path = os.path.join(output_dir, \"depth_visualization.png\")\n",
    "#     plt.savefig(depth_output_path)\n",
    "#     plt.close()\n",
    "#     print(f\"Depth visualization saved at: {depth_output_path}\")\n",
    "\n",
    "# # def save_visualizations(models, val_loader, device, output_dir):\n",
    "# #     \"\"\"\n",
    "# #     Save visualizations of segmentation and depth outputs as PNG files.\n",
    "\n",
    "# #     Args:\n",
    "# #         models (list): List of PyTorch models.\n",
    "# #         val_loader: Validation DataLoader.\n",
    "# #         device: Device for inference (\"cuda\" or \"cpu\").\n",
    "# #         output_dir: Directory to save the visualizations.\n",
    "# #     \"\"\"\n",
    "# #     # Ensure output directory exists\n",
    "# #     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# #     # Get the 4th element from val_loader\n",
    "# #     sample_index = 3\n",
    "# #     for idx, batch in enumerate(val_loader):\n",
    "# #         if idx == sample_index:\n",
    "# #             inputs = batch[\"left\"].to(device)\n",
    "# #             seg_labels = batch[\"mask\"].to(device)\n",
    "# #             depth_labels = batch[\"depth\"].to(device)\n",
    "# #             break\n",
    "\n",
    "# #     # Get predictions from all models\n",
    "# #     seg_outputs = []\n",
    "# #     depth_outputs = []\n",
    "\n",
    "# #     for model in models:\n",
    "# #         model.eval()\n",
    "# #         with torch.no_grad():\n",
    "# #             outputs = model(inputs)\n",
    "# #             seg_outputs.append(outputs[\"seg_output\"])  # Segmentation output\n",
    "# #             depth_outputs.append(outputs[\"depth_output\"])  # Depth output\n",
    "\n",
    "# #     # Convert segmentation labels to one-hot if necessary\n",
    "# #     if seg_labels.size(1) == 1:  # If labels are class indices\n",
    "# #         seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "# #         seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "# #     # Save segmentation visualization\n",
    "# #     plt.figure(figsize=(15, 5))\n",
    "\n",
    "# #     # Original Image\n",
    "# #     plt.subplot(1, len(models) + 2, 1)\n",
    "# #     plt.imshow(inputs[0].permute(1, 2, 0).cpu().numpy())\n",
    "# #     plt.title(\"Original Image\")\n",
    "# #     plt.axis(\"off\")\n",
    "\n",
    "# #     # Original Segmentation Labels\n",
    "# #     plt.subplot(1, len(models) + 2, 2)\n",
    "# #     plt.imshow(torch.argmax(seg_labels[0], dim=0).cpu().numpy(), cmap=\"jet\")\n",
    "# #     plt.title(\"Segmentation Labels\")\n",
    "# #     plt.axis(\"off\")\n",
    "\n",
    "# #     # Model Outputs for Segmentation\n",
    "# #     for i, seg_output in enumerate(seg_outputs):\n",
    "# #         plt.subplot(1, len(models) + 2, i + 3)\n",
    "# #         plt.imshow(torch.argmax(seg_output[0], dim=0).cpu().numpy(), cmap=\"jet\")\n",
    "# #         plt.title(f\"Model {i + 1}\")\n",
    "# #         plt.axis(\"off\")\n",
    "\n",
    "# #     seg_output_path = os.path.join(output_dir, \"segmentation_visualization.png\")\n",
    "# #     plt.savefig(seg_output_path)\n",
    "# #     plt.close()\n",
    "# #     print(f\"Segmentation visualization saved at: {seg_output_path}\")\n",
    "\n",
    "# #     # Save depth visualization\n",
    "# #     plt.figure(figsize=(15, 5))\n",
    "\n",
    "# #     # Original Image\n",
    "# #     plt.subplot(1, len(models) + 2, 1)\n",
    "# #     plt.imshow(inputs[0].permute(1, 2, 0).cpu().numpy())\n",
    "# #     plt.title(\"Original Image\")\n",
    "# #     plt.axis(\"off\")\n",
    "\n",
    "# #     # Depth Labels\n",
    "# #     plt.subplot(1, len(models) + 2, 2)\n",
    "# #     plt.imshow(depth_labels[0].squeeze(0).cpu().numpy(), cmap=\"plasma\")\n",
    "# #     plt.title(\"Depth Labels\")\n",
    "# #     plt.axis(\"off\")\n",
    "\n",
    "# #     # Model Outputs for Depth\n",
    "# #     for i, depth_output in enumerate(depth_outputs):\n",
    "# #         plt.subplot(1, len(models) + 2, i + 3)\n",
    "# #         plt.imshow(depth_output[0].squeeze(0).cpu().numpy(), cmap=\"plasma\")\n",
    "# #         plt.title(f\"Model {i + 1}\")\n",
    "# #         plt.axis(\"off\")\n",
    "\n",
    "# #     depth_output_path = os.path.join(output_dir, \"depth_visualization.png\")\n",
    "# #     plt.savefig(depth_output_path)\n",
    "# #     plt.close()\n",
    "# #     print(f\"Depth visualization saved at: {depth_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "888a619b-ce2e-46b2-abac-721054561831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def save_batch_visualizations(models, val_loader, device, output_dir, batch_index=0):\n",
    "#     \"\"\"\n",
    "#     Save visualizations of segmentation and depth outputs for an entire batch as PNG files.\n",
    "\n",
    "#     Args:\n",
    "#         models (list): List of PyTorch models.\n",
    "#         val_loader: Validation DataLoader.\n",
    "#         device: Device for inference (\"cuda\" or \"cpu\").\n",
    "#         output_dir: Directory to save the visualizations.\n",
    "#         batch_index: Index of the batch to visualize.\n",
    "#     \"\"\"\n",
    "#     # Ensure output directory exists\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Get the specified batch\n",
    "#     for idx, batch in enumerate(val_loader):\n",
    "#         if idx == batch_index:\n",
    "#             inputs = batch[\"left\"].to(device)\n",
    "#             seg_labels = batch[\"mask\"].to(device)\n",
    "#             depth_labels = batch[\"depth\"].to(device)\n",
    "#             input_size = inputs.size()[-2:]\n",
    "#             break\n",
    "\n",
    "#     # Ensure the batch was found\n",
    "#     if inputs is None or seg_labels is None or depth_labels is None:\n",
    "#         raise ValueError(f\"Batch with index {batch_index} not found in val_loader.\")\n",
    "        \n",
    "#     # Move models to the correct device\n",
    "#     for model in models:\n",
    "#         model.to(device)\n",
    "\n",
    "#     # Get predictions from all models\n",
    "#     seg_outputs = []\n",
    "#     depth_outputs = []\n",
    "\n",
    "#     for model in models:\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(inputs,input_size=input_size)\n",
    "#             seg_outputs.append(outputs[\"seg_output\"])  # Segmentation output\n",
    "#             depth_outputs.append(outputs[\"depth_output\"])  # Depth output\n",
    "\n",
    "#     # Iterate over the batch\n",
    "#     batch_size = inputs.size(0)\n",
    "\n",
    "#     for i in range(batch_size):\n",
    "#         # Convert segmentation labels to one-hot if necessary\n",
    "#         if seg_labels.size(1) == 1:  # If labels are class indices\n",
    "#             single_seg_label = torch.nn.functional.one_hot(seg_labels[i].squeeze(0), num_classes=20)\n",
    "#             single_seg_label = single_seg_label.permute(2, 0, 1).float().to(device)\n",
    "#         else:\n",
    "#             single_seg_label = seg_labels[i]\n",
    "\n",
    "#         single_depth_label = depth_labels[i]\n",
    "\n",
    "#         # Save segmentation visualization for this image\n",
    "#         plt.figure(figsize=(15, 5))\n",
    "\n",
    "#         # Original Image\n",
    "#         plt.subplot(1, len(models) + 2, 1)\n",
    "#         plt.imshow(inputs[i].permute(1, 2, 0).cpu().numpy())\n",
    "#         plt.title(\"Original Image\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         # Original Segmentation Labels\n",
    "#         plt.subplot(1, len(models) + 2, 2)\n",
    "#         plt.imshow(torch.argmax(single_seg_label, dim=0).cpu().numpy(), cmap=\"jet\")\n",
    "#         plt.title(\"Segmentation Labels\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         # Model Outputs for Segmentation\n",
    "#         for j, seg_output in enumerate(seg_outputs):\n",
    "#             plt.subplot(1, len(models) + 2, j + 3)\n",
    "#             plt.imshow(torch.argmax(seg_output[i], dim=0).cpu().numpy(), cmap=\"jet\")\n",
    "#             plt.title(f\"Model {j + 1}\")\n",
    "#             plt.axis(\"off\")\n",
    "\n",
    "#         seg_output_path = os.path.join(output_dir, f\"segmentation_visualization_image_{i}.png\")\n",
    "#         plt.savefig(seg_output_path)\n",
    "#         plt.close()\n",
    "#         print(f\"Segmentation visualization for image {i} saved at: {seg_output_path}\")\n",
    "\n",
    "#         # Save depth visualization for this image\n",
    "#         plt.figure(figsize=(15, 5))\n",
    "\n",
    "#         # Original Image\n",
    "#         plt.subplot(1, len(models) + 2, 1)\n",
    "#         plt.imshow(inputs[i].permute(1, 2, 0).cpu().numpy())\n",
    "#         plt.title(\"Original Image\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         # Depth Labels\n",
    "#         plt.subplot(1, len(models) + 2, 2)\n",
    "#         plt.imshow(single_depth_label.squeeze(0).cpu().numpy(), cmap=\"plasma\")\n",
    "#         plt.title(\"Depth Labels\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         # Model Outputs for Depth\n",
    "#         for j, depth_output in enumerate(depth_outputs):\n",
    "#             plt.subplot(1, len(models) + 2, j + 3)\n",
    "#             plt.imshow(depth_output[i].squeeze(0).cpu().numpy(), cmap=\"plasma\")\n",
    "#             plt.title(f\"Model {j + 1}\")\n",
    "#             plt.axis(\"off\")\n",
    "\n",
    "#         depth_output_path = os.path.join(output_dir, f\"depth_visualization_image_{i}.png\")\n",
    "#         plt.savefig(depth_output_path)\n",
    "#         plt.close()\n",
    "#         print(f\"Depth visualization for image {i} saved at: {depth_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3736407a-f541-4120-aeba-3d33598c47c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_batch_visualizations(models, val_loader, device, output_dir, batch_index=0):\n",
    "    \"\"\"\n",
    "    Save visualizations of segmentation and depth outputs for an entire batch as separate PNG files.\n",
    "\n",
    "    Args:\n",
    "        models (list): List of PyTorch models.\n",
    "        val_loader: Validation DataLoader.\n",
    "        device: Device for inference (\"cuda\" or \"cpu\").\n",
    "        output_dir: Directory to save the visualizations.\n",
    "        batch_index: Index of the batch to visualize.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get the specified batch\n",
    "    inputs, seg_labels, depth_labels = None, None, None\n",
    "    for idx, batch in enumerate(val_loader):\n",
    "        if idx == batch_index:\n",
    "            inputs = batch[\"left\"].detach().cpu()\n",
    "            seg_labels = batch[\"mask\"].detach().cpu()\n",
    "            depth_labels = batch[\"depth\"].detach().cpu()\n",
    "            \n",
    "            # Preprocess seg_labels to one-hot encoding\n",
    "            if seg_labels.size(1) == 1:\n",
    "                seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "                seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "            # Ensure depth_labels has correct dimensions\n",
    "            if depth_labels.dim() == 5:\n",
    "                depth_labels = depth_labels.squeeze(2)\n",
    "                \n",
    "            seg_labels = torch.argmax(seg_labels, dim=1)\n",
    "            break\n",
    "\n",
    "    # Ensure the batch was found\n",
    "    if inputs is None or seg_labels is None or depth_labels is None:\n",
    "        raise ValueError(f\"Batch with index {batch_index} not found in val_loader.\")\n",
    "\n",
    "    # Move models to the correct device\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "\n",
    "    # Get predictions from all models\n",
    "    seg_outputs = []\n",
    "    depth_outputs = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, input_size=inputs.shape[-2:])\n",
    "            seg_outputs.append(outputs[\"seg_output\"].detach().cpu())  # Segmentation output\n",
    "            depth_outputs.append(outputs[\"depth_output\"].detach().cpu())  # Depth output\n",
    "    \n",
    "    batch_size = min(4, inputs.size(0))\n",
    "    \n",
    "    # Visualize segmentation\n",
    "    for i in range(inputs.size(0)):\n",
    "        inputs_temp = inputs[i].detach().cpu()\n",
    "        inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)\n",
    "\n",
    "        seg_label = seg_labels[i].detach().cpu()\n",
    "        seg_outputs_vis = [torch.argmax(output[i], dim=0).detach().cpu() for output in seg_outputs]\n",
    "\n",
    "        # fig, axes = plt.subplots(1, len(models) + 2, figsize=(15, 5))\n",
    "        fig, axes = plt.subplots(batch_size, len(models) + 2, figsize=(15, 4 * batch_size))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "        \n",
    "            inputs_temp = inputs[i]\n",
    "            # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "            inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "\n",
    "            # depth_labels_vis = (depth_labels[i] - depth_labels[i].min()) / (depth_labels[i].max() - depth_labels[i].min() + 1e-5)\n",
    "            # depth_preds = depth_output[i]\n",
    "            # depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5) \n",
    "\n",
    "            # Original Image\n",
    "            axes[i,0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "            axes[i,0].set_title(\"Original Image (RGB)\")\n",
    "            axes[i,0].axis(\"off\")\n",
    "\n",
    "            # Segmentation Labels\n",
    "            axes[i,1].imshow(seg_label, cmap=\"tab20\")\n",
    "            axes[i,1].set_title(\"Original Segmentation Label\")\n",
    "            axes[i,1].axis(\"off\")\n",
    "\n",
    "            # Model Segmentation Outputs\n",
    "            for j, seg_output in enumerate(seg_outputs_vis):\n",
    "                axes[i,j + 2].imshow(seg_output, cmap=\"tab20\")\n",
    "                axes[i,j + 2].set_title(f\"Model {j + 1} Segmentation Output\")\n",
    "                axes[i,j + 2].axis(\"off\")\n",
    "\n",
    "            seg_output_path = os.path.join(output_dir, f\"segmentation_visualization_image_{i}.png\")\n",
    "            plt.savefig(seg_output_path)\n",
    "            plt.close()\n",
    "            print(f\"Segmentation visualization for image {i} saved at: {seg_output_path}\")\n",
    "\n",
    "        for i in range(batch_size):\n",
    "        \n",
    "            inputs_temp = inputs[i]\n",
    "            # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "            inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "\n",
    "            depth_labels_vis = (depth_labels[i] - depth_labels[i].min()) / (depth_labels[i].max() - depth_labels[i].min() + 1e-5)\n",
    "            depth_preds = depth_outputs[i]\n",
    "            depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5) \n",
    "\n",
    "            # Original Image\n",
    "            axes[i,0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "            axes[i,0].set_title(\"Original Image (RGB)\")\n",
    "            axes[i,0].axis(\"off\")\n",
    "\n",
    "            # Segmentation Labels\n",
    "            axes[i,1].imshow(depth_labels_vis.squeeze(), cmap=\"tab20\")\n",
    "            axes[i,1].set_title(\"Original Depth Map\")\n",
    "            axes[i,1].axis(\"off\")\n",
    "\n",
    "            # Model Depth Outputs\n",
    "            for j, depth_output in enumerate(depth_preds_vis):\n",
    "                axes[i,j + 2].imshow(depth_output.squeeze(), cmap=\"inferno\")\n",
    "                axes[i,j + 2].set_title(f\"Model {j + 1} Depth Output\")\n",
    "                axes[i,j + 2].axis(\"off\")\n",
    "\n",
    "            depth_output_path = os.path.join(output_dir, f\"depth_visualization_image_{i}.png\")\n",
    "            plt.savefig(depth_output_path)\n",
    "            plt.close()\n",
    "            print(f\"Depth visualization for image {i} saved at: {depth_output_path}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df2a8c-a43e-44db-b635-7e15b49c6fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9605702b-1523-4d7a-9598-ad76aaefb122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device =  'cpu'\n",
    "\n",
    "# # Initialize the model class and loaders\n",
    "# mobilenet_backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "# model_class = lambda: MultiTaskModel(backbone=mobilenet_backbone.features, num_seg_classes=20, depth_channels=1)\n",
    "\n",
    "# # # mobilenet_backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "# model_class2 = lambda: MultiTaskModel_old(backbone=mobilenet_backbone.features, num_seg_classes=20, depth_channels=1)\n",
    "\n",
    "# # opt_sched = initialize_optimizers_and_schedulers(model)\n",
    "\n",
    "# # Initialize optimizers and schedulers\n",
    "# model1 = model_class2()\n",
    "# opt_sched1 = initialize_optimizers_and_schedulers(model1)\n",
    "# start_epoch, best_loss = load_checkpoint(model1, opt_sched1, model_map['test8'], device)\n",
    "\n",
    "# model2 = model_class()\n",
    "# opt_sched2 = initialize_optimizers_and_schedulers(model2)\n",
    "# start_epoch, best_loss = load_checkpoint(model2, opt_sched2, model_map['test9'], device)\n",
    "\n",
    "# model3 = model_class()\n",
    "# opt_sched3 = initialize_optimizers_and_schedulers(model3)\n",
    "# start_epoch, best_loss = load_checkpoint(model3, opt_sched3, model_map['test11'], device)\n",
    "\n",
    "# model4 = model_class()\n",
    "# opt_sched4 = initialize_optimizers_and_schedulers(model4)\n",
    "# start_epoch, best_loss = load_checkpoint(model4, opt_sched4, model_map['test13'], device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "26a68199-3258-4b93-b960-36b549896b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_batch_data(valid_loader, batch_index):\n",
    "    # Get the specified batch\n",
    "    inputs, seg_labels, depth_labels = None, None, None\n",
    "    for idx, batch in enumerate(valid_loader):\n",
    "        print(idx)\n",
    "        if idx == batch_index:\n",
    "            inputs = batch[\"left\"]\n",
    "            seg_labels = batch[\"mask\"]\n",
    "            depth_labels = batch[\"depth\"]\n",
    "            \n",
    "            input_size = inputs.size()[-2:]\n",
    "            \n",
    "            # Preprocess seg_labels to one-hot encoding\n",
    "            if seg_labels.size(1) == 1:\n",
    "                seg_labels = torch.nn.functional.one_hot(seg_labels.squeeze(1), num_classes=20)\n",
    "                seg_labels = seg_labels.permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "            # Ensure depth_labels has correct dimensions\n",
    "            if depth_labels.dim() == 5:\n",
    "                depth_labels = depth_labels.squeeze(2)\n",
    "                \n",
    "            # seg_labels = torch.argmax(seg_labels, dim=1)\n",
    "            break\n",
    "    print(\"shape of data in extract_batch_data\")\n",
    "    print(inputs.shape,seg_labels.shape,depth_labels.shape)\n",
    "    return inputs.detach().cpu(),input_size,seg_labels.detach().cpu(),depth_labels.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "65d54f51-1a22-4ffb-9a46-33b34586df1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_seg_depth_output(model,inputs,input_size,seg_labels,depth_labels):\n",
    "    outputs = model(\n",
    "                    inputs,\n",
    "                    input_size=input_size,\n",
    "                    seg_labels=seg_labels,\n",
    "                    depth_labels=depth_labels,\n",
    "                    return_discriminator_outputs=False,\n",
    "                    )\n",
    "    print(\"inside get_seg_depth_output\")\n",
    "    print(outputs[\"seg_output\"].shape,outputs[\"depth_output\"].shape)\n",
    "    return outputs[\"seg_output\"],outputs[\"depth_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085f98c-dc02-4d8d-946e-5272c1514e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d81c2c-fd0c-4f11-b5ad-d959b39fa6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "dcb1071f-ebea-4a2d-b85d-34416fe92b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch_visualization3(inputs,input_size,seg_outputs,depth_outputs,seg_labels, depth_labels,best_model_dir):\n",
    "    inputs = inputs.detach().cpu()\n",
    "    seg_labels = seg_labels.detach().cpu()\n",
    "    depth_labels = depth_labels.detach().cpu()\n",
    "    \n",
    "    batch_size = min(4, inputs.size(0))  # Limit to 4 samples for visualization\n",
    "    fig, axes = plt.subplots(batch_size, 5, figsize=(15, 4 * batch_size))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        inputs_temp = inputs[i]\n",
    "        # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "        inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "        \n",
    "        # Row 1: Ground truth\n",
    "        axes[i, 0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "        axes[i, 0].set_title(\"RGB Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(seg_labels[i], cmap=\"tab20\")\n",
    "        axes[i, 1].set_title(\"GT Segmentation\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "        \n",
    "        j=2\n",
    "        for index,seg_out in enumerate(seg_outputs):\n",
    "            seg_output = torch.argmax(seg_output, dim=1).detach().cpu()\n",
    "            axes[i, j].imshow(seg_output[i], cmap=\"tab20\")\n",
    "            axes[i, j].set_title(f\"Model_{index}\")\n",
    "            axes[i, j].axis(\"off\")\n",
    "            j+=1\n",
    "            \n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "    fig.tight_layout() \n",
    "    \n",
    "    seg_output_path = os.path.join(best_model_dir, \"segmentation_visualization_image.png\")\n",
    "    plt.savefig(seg_output_path)\n",
    "    plt.close()\n",
    "    print(f\"Segmentation visualization for image {i} saved at: {seg_output_path}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c24b87fa-ec55-4d48-997b-2b65ffc1c2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_batch_visualizations2(models, valid_loader, device, best_model_dir, batch_index=0):\n",
    "    seg_outputs = []\n",
    "    depth_outputs = []\n",
    "    \n",
    "    inputs,input_size,seg_labels,depth_labels = extract_batch_data(valid_loader,batch_index)\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        seg_output, depth_output= get_seg_depth_output(model,inputs,input_size,seg_labels,depth_labels)\n",
    "        seg_outputs.append(seg_output)\n",
    "        depth_outputs.append(depth_output)\n",
    "        \n",
    "    print(\"save_batch_visualizations2\")\n",
    "    print(type(seg_output),type(seg_outputs),len(seg_outputs))\n",
    "    \n",
    "    save_batch_visualization3(inputs,input_size,seg_outputs,depth_outputs,torch.argmax(seg_labels, dim=1), depth_labels,best_model_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bbbdb4bf-e5c4-4445-9463-b2eb174456a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.1311431/ipykernel_1118662/725688548.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "shape of data in extract_batch_data\n",
      "torch.Size([8, 3, 200, 512]) torch.Size([8, 20, 200, 512]) torch.Size([8, 1, 200, 512])\n",
      "inside get_seg_depth_output\n",
      "torch.Size([8, 20, 200, 512]) torch.Size([8, 1, 200, 512])\n",
      "inside get_seg_depth_output\n",
      "torch.Size([8, 20, 200, 512]) torch.Size([8, 1, 200, 512])\n",
      "inside get_seg_depth_output\n",
      "torch.Size([8, 20, 200, 512]) torch.Size([8, 1, 200, 512])\n",
      "inside get_seg_depth_output\n",
      "torch.Size([8, 20, 200, 512]) torch.Size([8, 1, 200, 512])\n",
      "save_batch_visualizations2\n",
      "<class 'torch.Tensor'> <class 'list'> 4\n"
     ]
    }
   ],
   "source": [
    "device =  'cpu'\n",
    "\n",
    "# Initialize the model class and loaders\n",
    "mobilenet_backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model_class = lambda: MultiTaskModel(backbone=mobilenet_backbone.features, num_seg_classes=20, depth_channels=1)\n",
    "\n",
    "# # mobilenet_backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model_class2 = lambda: MultiTaskModel_old(backbone=mobilenet_backbone.features, num_seg_classes=20, depth_channels=1)\n",
    "\n",
    "# opt_sched = initialize_optimizers_and_schedulers(model)\n",
    "\n",
    "# Initialize optimizers and schedulers\n",
    "model1 = model_class2()\n",
    "opt_sched1 = initialize_optimizers_and_schedulers(model1)\n",
    "start_epoch, best_loss = load_checkpoint(model1, opt_sched1, model_map['test8'], device)\n",
    "\n",
    "model2 = model_class()\n",
    "opt_sched2 = initialize_optimizers_and_schedulers(model2)\n",
    "start_epoch, best_loss = load_checkpoint(model2, opt_sched2, model_map['test9'], device)\n",
    "\n",
    "model3 = model_class()\n",
    "opt_sched3 = initialize_optimizers_and_schedulers(model3)\n",
    "start_epoch, best_loss = load_checkpoint(model3, opt_sched3, model_map['test11'], device)\n",
    "\n",
    "model4 = model_class()\n",
    "opt_sched4 = initialize_optimizers_and_schedulers(model4)\n",
    "start_epoch, best_loss = load_checkpoint(model4, opt_sched4, model_map['test13'], device)\n",
    "\n",
    "\n",
    "models = [model1,model2,model3,model4]\n",
    "# Visualize outputs\n",
    "# visualize_model_outputs(models, val_loader, device)\n",
    "# Save visualizations\n",
    "# save_visualizations(models, valid_loader, device, best_model_dir)\n",
    "# save_batch_visualizations2(models, valid_loader, device, best_model_dir, batch_index=0)\n",
    "\n",
    "seg_outputs = []\n",
    "depth_outputs = []\n",
    "batch_index = 12 \n",
    "inputs,input_size,seg_labels,depth_labels = extract_batch_data(valid_loader,batch_index)\n",
    "\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    seg_output, depth_output= get_seg_depth_output(model,inputs,input_size,seg_labels,depth_labels)\n",
    "    seg_outputs.append(seg_output.detach().cpu())\n",
    "    depth_outputs.append(depth_output.detach().cpu())\n",
    "\n",
    "print(\"save_batch_visualizations2\")\n",
    "print(type(seg_output),type(seg_outputs),len(seg_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4731d29c-b6ec-495a-806b-561ef6fefe2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = inputs.detach().cpu()\n",
    "seg_labels = seg_labels.detach().cpu()\n",
    "seg_labels =torch.argmax(seg_labels, dim=1)\n",
    "depth_labels = depth_labels.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9e6955f1-7ac1-4136-a676-e68a79426df4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "print(seg_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c2325d3c-6b0e-4429-87b1-119f611680e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "torch.Size([20, 200, 512])\n",
      "Segmentation visualization for images at: /home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/segmentation_visualization_image_12.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = min(4, inputs.size(0))  # Limit to 4 samples for visualization\n",
    "fig, axes = plt.subplots(batch_size, 6, figsize=(15, 6 ))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    inputs_temp = inputs[i]\n",
    "    # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "    inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "\n",
    "    # Row 1: Ground truth\n",
    "    axes[i, 0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "    axes[i, 0].set_title(\"RGB Image\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(seg_labels[i], cmap=\"tab20\")\n",
    "    axes[i, 1].set_title(\"GT Segmentation\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    j=2\n",
    "    for index,seg_output in enumerate(seg_outputs):\n",
    "        seg_output = torch.argmax(seg_output, dim=0).detach().cpu()\n",
    "        print(seg_output.shape)\n",
    "        axes[i, j].imshow(seg_output[i], cmap=\"tab20\")\n",
    "        axes[i, j].set_title(f\"Model_{index}\")\n",
    "        axes[i, j].axis(\"off\")\n",
    "        j+=1\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.tight_layout() \n",
    "\n",
    "seg_output_path = os.path.join(best_model_dir, f\"segmentation_visualization_image_{batch_index}.png\")\n",
    "plt.savefig(seg_output_path)\n",
    "plt.close()\n",
    "print(f\"Segmentation visualization for images at: {seg_output_path}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "58849dd3-024b-4570-a42d-d6af59c1547e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "torch.Size([1, 200, 512]) torch.Size([200, 512])\n",
      "Depth visualization for images saved at: /home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/Best_models/depth_visualization_image_12.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = min(8, inputs.size(0))  # Limit to 4 samples for visualization\n",
    "fig, axes = plt.subplots(batch_size, 6, figsize=(15, 8 ))\n",
    "\n",
    "# depth_labels_vis = (depth_labels - depth_labels.min()) / (depth_labels.max() - depth_labels.min() + 1e-5)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    inputs_temp = inputs[i]\n",
    "    # print(f\"inputs_temp: {inputs_temp.shape}\")\n",
    "    inputs_rgb = (inputs_temp - inputs_temp.min()) / (inputs_temp.max() - inputs_temp.min() + 1e-5)  # Normalize inputs to [0, 1]\n",
    "    \n",
    "    depth_labels_vis = (depth_labels[i] - depth_labels[i].min()) / (depth_labels[i].max() - depth_labels[i].min() + 1e-5)\n",
    "    # depth_preds = depth_output[i]\n",
    "    # depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5)\n",
    "\n",
    "    # Row 1: Ground truth\n",
    "    axes[i, 0].imshow(inputs_rgb.permute(1, 2, 0))\n",
    "    axes[i, 0].set_title(\"RGB Image\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(depth_labels_vis.squeeze(), cmap=\"inferno\")\n",
    "    axes[i, 1].set_title(\"GT Depth\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    j=2\n",
    "    for index,depth_output in enumerate(depth_outputs):\n",
    "        depth_preds = depth_output[i]\n",
    "        depth_preds_vis = (depth_preds - depth_preds.min()) / (depth_preds.max() - depth_preds.min() + 1e-5)\n",
    "        print(depth_preds_vis.shape,depth_preds_vis.squeeze(0).shape)\n",
    "        axes[i, j].imshow(depth_preds_vis.squeeze(0), cmap=\"inferno\")\n",
    "        axes[i, j].set_title(f\"Model_{index}\")\n",
    "        axes[i, j].axis(\"off\")\n",
    "        j+=1\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.tight_layout() \n",
    "\n",
    "depth_output_path = os.path.join(best_model_dir, f\"depth_visualization_image_{batch_index}.png\")\n",
    "plt.savefig(depth_output_path)\n",
    "plt.close()\n",
    "print(f\"Depth visualization for images saved at: {depth_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaaa29f-6a55-445c-a6d7-0baf155838e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231469e2-cb80-40cd-be83-4a70eab622d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7fa7c-bca6-4b10-b38e-e8a489333fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2af4671-a619-4c13-80b8-2106ddba4e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test12:Train FFF, Valid-FFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccde2f-05c7-4332-a489-ee2cf3529147",
   "metadata": {},
   "source": [
    "# For first instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aef8c474-94e3-4753-8718-9aa32bef85a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Training:   0%|          | 0/125 [02:56<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 1 with combined loss 2.7408\n",
      "Epoch 1/100 Results:\n",
      "  Train Losses - Segmentation: 2.6150, Depth: 0.0706, Combined: 2.6842, Adversarial: -0.1380\n",
      "  Valid Losses - Segmentation: 2.6908, Depth: 0.0518, Combined: 2.7417, Adversarial: -0.0856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 2 with combined loss 2.4960\n",
      "Epoch 2/100 Results:\n",
      "  Train Losses - Segmentation: 2.2913, Depth: 0.0360, Combined: 2.3263, Adversarial: -0.0984\n",
      "  Valid Losses - Segmentation: 2.4602, Depth: 0.0351, Combined: 2.4957, Adversarial: 0.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 3 with combined loss 2.3005\n",
      "Epoch 3/100 Results:\n",
      "  Train Losses - Segmentation: 2.2053, Depth: 0.0360, Combined: 2.2420, Adversarial: 0.0671\n",
      "  Valid Losses - Segmentation: 2.2584, Depth: 0.0439, Combined: 2.3014, Adversarial: -0.0897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 Results:\n",
      "  Train Losses - Segmentation: 2.1675, Depth: 0.0340, Combined: 2.2000, Adversarial: -0.1528\n",
      "  Valid Losses - Segmentation: 2.4102, Depth: 0.0448, Combined: 2.4532, Adversarial: -0.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 Results:\n",
      "  Train Losses - Segmentation: 2.1272, Depth: 0.0345, Combined: 2.1597, Adversarial: -0.1904\n",
      "  Valid Losses - Segmentation: 2.4627, Depth: 0.0722, Combined: 2.5328, Adversarial: -0.2148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Training:   0%|          | 0/125 [02:49<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 6 with combined loss 2.2204\n",
      "Epoch 6/100 Results:\n",
      "  Train Losses - Segmentation: 2.0647, Depth: 0.0359, Combined: 2.0983, Adversarial: -0.2359\n",
      "  Valid Losses - Segmentation: 2.1868, Depth: 0.0388, Combined: 2.2230, Adversarial: -0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Training:   0%|          | 0/125 [02:49<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 Results:\n",
      "  Train Losses - Segmentation: 2.0296, Depth: 0.0314, Combined: 2.0578, Adversarial: -0.3109\n",
      "  Valid Losses - Segmentation: 2.6012, Depth: 0.0542, Combined: 2.6515, Adversarial: -0.3802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 Results:\n",
      "  Train Losses - Segmentation: 2.0570, Depth: 0.0317, Combined: 2.0857, Adversarial: -0.3009\n",
      "  Valid Losses - Segmentation: 2.3027, Depth: 0.0483, Combined: 2.3477, Adversarial: -0.3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 Results:\n",
      "  Train Losses - Segmentation: 2.0521, Depth: 0.0332, Combined: 2.0818, Adversarial: -0.3465\n",
      "  Valid Losses - Segmentation: 2.2114, Depth: 0.1791, Combined: 2.3872, Adversarial: -0.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 10 with combined loss 2.2176\n",
      "Epoch 10/100 Results:\n",
      "  Train Losses - Segmentation: 1.9860, Depth: 0.0331, Combined: 2.0153, Adversarial: -0.3863\n",
      "  Valid Losses - Segmentation: 2.1724, Depth: 0.0534, Combined: 2.2217, Adversarial: -0.4115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 11 with combined loss 2.2058\n",
      "Epoch 11/100 Results:\n",
      "  Train Losses - Segmentation: 2.0062, Depth: 0.0294, Combined: 2.0312, Adversarial: -0.4377\n",
      "  Valid Losses - Segmentation: 2.1751, Depth: 0.0398, Combined: 2.2103, Adversarial: -0.4591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 Results:\n",
      "  Train Losses - Segmentation: 1.9893, Depth: 0.0323, Combined: 2.0168, Adversarial: -0.4823\n",
      "  Valid Losses - Segmentation: 2.2646, Depth: 0.0618, Combined: 2.3214, Adversarial: -0.4944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 Results:\n",
      "  Train Losses - Segmentation: 1.9949, Depth: 0.0314, Combined: 2.0213, Adversarial: -0.5002\n",
      "  Valid Losses - Segmentation: 2.1810, Depth: 0.0433, Combined: 2.2194, Adversarial: -0.4907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 - Training:   0%|          | 0/125 [02:49<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 Results:\n",
      "  Train Losses - Segmentation: 1.9433, Depth: 0.0287, Combined: 1.9667, Adversarial: -0.5385\n",
      "  Valid Losses - Segmentation: 2.2179, Depth: 0.0742, Combined: 2.2866, Adversarial: -0.5486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 Results:\n",
      "  Train Losses - Segmentation: 1.9483, Depth: 0.0297, Combined: 1.9721, Adversarial: -0.5870\n",
      "  Valid Losses - Segmentation: 2.2561, Depth: 0.0633, Combined: 2.3133, Adversarial: -0.6085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 Results:\n",
      "  Train Losses - Segmentation: 1.9182, Depth: 0.0276, Combined: 1.9394, Adversarial: -0.6385\n",
      "  Valid Losses - Segmentation: 2.1791, Depth: 0.0834, Combined: 2.2563, Adversarial: -0.6160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 17 with combined loss 2.1961\n",
      "Epoch 17/100 Results:\n",
      "  Train Losses - Segmentation: 1.8744, Depth: 0.0269, Combined: 1.8949, Adversarial: -0.6390\n",
      "  Valid Losses - Segmentation: 2.1659, Depth: 0.0439, Combined: 2.2029, Adversarial: -0.6859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 18 with combined loss 2.1160\n",
      "Epoch 18/100 Results:\n",
      "  Train Losses - Segmentation: 1.9095, Depth: 0.0269, Combined: 1.9293, Adversarial: -0.7126\n",
      "  Valid Losses - Segmentation: 2.0674, Depth: 0.0624, Combined: 2.1229, Adversarial: -0.6921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 Results:\n",
      "  Train Losses - Segmentation: 1.8914, Depth: 0.0292, Combined: 1.9132, Adversarial: -0.7366\n",
      "  Valid Losses - Segmentation: 2.2280, Depth: 0.0439, Combined: 2.2645, Adversarial: -0.7346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 Results:\n",
      "  Train Losses - Segmentation: 1.8988, Depth: 0.0281, Combined: 1.9192, Adversarial: -0.7750\n",
      "  Valid Losses - Segmentation: 2.1467, Depth: 0.0525, Combined: 2.1915, Adversarial: -0.7707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 Results:\n",
      "  Train Losses - Segmentation: 1.8841, Depth: 0.0263, Combined: 1.9023, Adversarial: -0.8129\n",
      "  Valid Losses - Segmentation: 2.2782, Depth: 0.0588, Combined: 2.3294, Adversarial: -0.7634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 - Training:   0%|          | 0/125 [02:56<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 Results:\n",
      "  Train Losses - Segmentation: 1.8712, Depth: 0.0259, Combined: 1.8894, Adversarial: -0.7703\n",
      "  Valid Losses - Segmentation: 2.1423, Depth: 0.0572, Combined: 2.1912, Adversarial: -0.8244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 - Training:   0%|          | 0/125 [02:56<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 Results:\n",
      "  Train Losses - Segmentation: 1.8614, Depth: 0.0260, Combined: 1.8789, Adversarial: -0.8418\n",
      "  Valid Losses - Segmentation: 2.2147, Depth: 0.0675, Combined: 2.2736, Adversarial: -0.8560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 Results:\n",
      "  Train Losses - Segmentation: 1.8697, Depth: 0.0256, Combined: 1.8867, Adversarial: -0.8536\n",
      "  Valid Losses - Segmentation: 2.1454, Depth: 0.0429, Combined: 2.1796, Adversarial: -0.8691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 Results:\n",
      "  Train Losses - Segmentation: 1.8666, Depth: 0.0267, Combined: 1.8848, Adversarial: -0.8465\n",
      "  Valid Losses - Segmentation: 2.1969, Depth: 0.0433, Combined: 2.2316, Adversarial: -0.8555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 - Training:   0%|          | 0/125 [02:57<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 Results:\n",
      "  Train Losses - Segmentation: 1.8471, Depth: 0.0251, Combined: 1.8636, Adversarial: -0.8647\n",
      "  Valid Losses - Segmentation: 2.1772, Depth: 0.0612, Combined: 2.2298, Adversarial: -0.8579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 - Training:   0%|          | 0/125 [02:56<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 Results:\n",
      "  Train Losses - Segmentation: 1.8551, Depth: 0.0248, Combined: 1.8712, Adversarial: -0.8698\n",
      "  Valid Losses - Segmentation: 2.1224, Depth: 0.0455, Combined: 2.1594, Adversarial: -0.8522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 Results:\n",
      "  Train Losses - Segmentation: 1.8327, Depth: 0.0256, Combined: 1.8497, Adversarial: -0.8563\n",
      "  Valid Losses - Segmentation: 2.1522, Depth: 0.0400, Combined: 2.1836, Adversarial: -0.8703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 29 with combined loss 2.0468\n",
      "Epoch 29/100 Results:\n",
      "  Train Losses - Segmentation: 1.8214, Depth: 0.0260, Combined: 1.8386, Adversarial: -0.8842\n",
      "  Valid Losses - Segmentation: 2.0364, Depth: 0.0337, Combined: 2.0584, Adversarial: -1.1626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 Results:\n",
      "  Train Losses - Segmentation: 1.8219, Depth: 0.0241, Combined: 1.8369, Adversarial: -0.9122\n",
      "  Valid Losses - Segmentation: 2.1400, Depth: 0.0812, Combined: 2.2123, Adversarial: -0.8973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 Results:\n",
      "  Train Losses - Segmentation: 1.8102, Depth: 0.0250, Combined: 1.8262, Adversarial: -0.9082\n",
      "  Valid Losses - Segmentation: 2.0956, Depth: 0.0750, Combined: 2.1614, Adversarial: -0.9197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 Results:\n",
      "  Train Losses - Segmentation: 1.7997, Depth: 0.0250, Combined: 1.8157, Adversarial: -0.9006\n",
      "  Valid Losses - Segmentation: 2.1175, Depth: 0.0517, Combined: 2.1603, Adversarial: -0.8903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 Results:\n",
      "  Train Losses - Segmentation: 1.8017, Depth: 0.0227, Combined: 1.8154, Adversarial: -0.9036\n",
      "  Valid Losses - Segmentation: 2.1378, Depth: 0.0404, Combined: 2.1692, Adversarial: -0.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 Results:\n",
      "  Train Losses - Segmentation: 1.7755, Depth: 0.0237, Combined: 1.7901, Adversarial: -0.9088\n",
      "  Valid Losses - Segmentation: 2.0855, Depth: 0.0562, Combined: 2.1328, Adversarial: -0.8882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 - Training:   0%|          | 0/125 [02:47<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 Results:\n",
      "  Train Losses - Segmentation: 1.7619, Depth: 0.0240, Combined: 1.7767, Adversarial: -0.9189\n",
      "  Valid Losses - Segmentation: 2.0954, Depth: 0.0464, Combined: 2.1325, Adversarial: -0.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 Results:\n",
      "  Train Losses - Segmentation: 1.7920, Depth: 0.0226, Combined: 1.8054, Adversarial: -0.9186\n",
      "  Valid Losses - Segmentation: 2.1290, Depth: 0.0389, Combined: 2.1587, Adversarial: -0.9179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 Results:\n",
      "  Train Losses - Segmentation: 1.7925, Depth: 0.0228, Combined: 1.8062, Adversarial: -0.9056\n",
      "  Valid Losses - Segmentation: 2.0595, Depth: 0.0324, Combined: 2.0828, Adversarial: -0.9034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 Results:\n",
      "  Train Losses - Segmentation: 1.7756, Depth: 0.0226, Combined: 1.7893, Adversarial: -0.8990\n",
      "  Valid Losses - Segmentation: 2.1205, Depth: 0.0345, Combined: 2.1459, Adversarial: -0.9106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 Results:\n",
      "  Train Losses - Segmentation: 1.7627, Depth: 0.0224, Combined: 1.7758, Adversarial: -0.9256\n",
      "  Valid Losses - Segmentation: 2.1022, Depth: 0.0381, Combined: 2.1309, Adversarial: -0.9379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 Results:\n",
      "  Train Losses - Segmentation: 1.7724, Depth: 0.0224, Combined: 1.7854, Adversarial: -0.9423\n",
      "  Valid Losses - Segmentation: 2.0764, Depth: 0.0398, Combined: 2.1068, Adversarial: -0.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 Results:\n",
      "  Train Losses - Segmentation: 1.7514, Depth: 0.0220, Combined: 1.7639, Adversarial: -0.9427\n",
      "  Valid Losses - Segmentation: 2.0222, Depth: 0.0508, Combined: 2.0636, Adversarial: -0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 Results:\n",
      "  Train Losses - Segmentation: 1.7491, Depth: 0.0222, Combined: 1.7618, Adversarial: -0.9419\n",
      "  Valid Losses - Segmentation: 2.0888, Depth: 0.0439, Combined: 2.1232, Adversarial: -0.9394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 - Training:   0%|          | 0/125 [02:59<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 Results:\n",
      "  Train Losses - Segmentation: 1.7708, Depth: 0.0221, Combined: 1.7834, Adversarial: -0.9420\n",
      "  Valid Losses - Segmentation: 2.0865, Depth: 0.0458, Combined: 2.1228, Adversarial: -0.9448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 Results:\n",
      "  Train Losses - Segmentation: 1.7467, Depth: 0.0210, Combined: 1.7582, Adversarial: -0.9515\n",
      "  Valid Losses - Segmentation: 2.0731, Depth: 0.0421, Combined: 2.1057, Adversarial: -0.9572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 Results:\n",
      "  Train Losses - Segmentation: 1.7504, Depth: 0.0216, Combined: 1.7624, Adversarial: -0.9552\n",
      "  Valid Losses - Segmentation: 2.0851, Depth: 0.0472, Combined: 2.1228, Adversarial: -0.9521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 Results:\n",
      "  Train Losses - Segmentation: 1.7406, Depth: 0.0213, Combined: 1.7524, Adversarial: -0.9553\n",
      "  Valid Losses - Segmentation: 2.0568, Depth: 0.0418, Combined: 2.0890, Adversarial: -0.9559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 Results:\n",
      "  Train Losses - Segmentation: 1.7545, Depth: 0.0219, Combined: 1.7668, Adversarial: -0.9534\n",
      "  Valid Losses - Segmentation: 2.0638, Depth: 0.0428, Combined: 2.0970, Adversarial: -0.9560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 Results:\n",
      "  Train Losses - Segmentation: 1.7108, Depth: 0.0213, Combined: 1.7225, Adversarial: -0.9570\n",
      "  Valid Losses - Segmentation: 2.0568, Depth: 0.0443, Combined: 2.0915, Adversarial: -0.9595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 Results:\n",
      "  Train Losses - Segmentation: 1.7526, Depth: 0.0217, Combined: 1.7647, Adversarial: -0.9616\n",
      "  Valid Losses - Segmentation: 2.0744, Depth: 0.0456, Combined: 2.1104, Adversarial: -0.9620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 Results:\n",
      "  Train Losses - Segmentation: 1.7629, Depth: 0.0218, Combined: 1.7751, Adversarial: -0.9631\n",
      "  Valid Losses - Segmentation: 2.0692, Depth: 0.0440, Combined: 2.1036, Adversarial: -0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 Results:\n",
      "  Train Losses - Segmentation: 1.7550, Depth: 0.0214, Combined: 1.7668, Adversarial: -0.9629\n",
      "  Valid Losses - Segmentation: 2.0772, Depth: 0.0402, Combined: 2.1078, Adversarial: -0.9640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 Results:\n",
      "  Train Losses - Segmentation: 1.7402, Depth: 0.0212, Combined: 1.7517, Adversarial: -0.9621\n",
      "  Valid Losses - Segmentation: 2.0711, Depth: 0.0433, Combined: 2.1048, Adversarial: -0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 Results:\n",
      "  Train Losses - Segmentation: 1.7235, Depth: 0.0218, Combined: 1.7357, Adversarial: -0.9624\n",
      "  Valid Losses - Segmentation: 2.0881, Depth: 0.0444, Combined: 2.1229, Adversarial: -0.9612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 Results:\n",
      "  Train Losses - Segmentation: 1.7415, Depth: 0.0209, Combined: 1.7528, Adversarial: -0.9645\n",
      "  Valid Losses - Segmentation: 2.0673, Depth: 0.0418, Combined: 2.0995, Adversarial: -0.9655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 Results:\n",
      "  Train Losses - Segmentation: 1.7696, Depth: 0.0216, Combined: 1.7816, Adversarial: -0.9625\n",
      "  Valid Losses - Segmentation: 2.0558, Depth: 0.0448, Combined: 2.0911, Adversarial: -0.9610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 Results:\n",
      "  Train Losses - Segmentation: 1.7609, Depth: 0.0216, Combined: 1.7728, Adversarial: -0.9627\n",
      "  Valid Losses - Segmentation: 2.0739, Depth: 0.0483, Combined: 2.1125, Adversarial: -0.9678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 Results:\n",
      "  Train Losses - Segmentation: 1.7404, Depth: 0.0215, Combined: 1.7522, Adversarial: -0.9670\n",
      "  Valid Losses - Segmentation: 2.0578, Depth: 0.0497, Combined: 2.0978, Adversarial: -0.9676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 Results:\n",
      "  Train Losses - Segmentation: 1.7229, Depth: 0.0215, Combined: 1.7347, Adversarial: -0.9681\n",
      "  Valid Losses - Segmentation: 2.1173, Depth: 0.0529, Combined: 2.1605, Adversarial: -0.9669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 Results:\n",
      "  Train Losses - Segmentation: 1.7357, Depth: 0.0213, Combined: 1.7474, Adversarial: -0.9575\n",
      "  Valid Losses - Segmentation: 2.1280, Depth: 0.0508, Combined: 2.1688, Adversarial: -0.9912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 Results:\n",
      "  Train Losses - Segmentation: 1.7565, Depth: 0.0227, Combined: 1.7698, Adversarial: -0.9441\n",
      "  Valid Losses - Segmentation: 2.0624, Depth: 0.0403, Combined: 2.0933, Adversarial: -0.9437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 Results:\n",
      "  Train Losses - Segmentation: 1.7715, Depth: 0.0217, Combined: 1.7835, Adversarial: -0.9737\n",
      "  Valid Losses - Segmentation: 2.0956, Depth: 0.0417, Combined: 2.1277, Adversarial: -0.9670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 Results:\n",
      "  Train Losses - Segmentation: 1.7410, Depth: 0.0219, Combined: 1.7532, Adversarial: -0.9744\n",
      "  Valid Losses - Segmentation: 2.0988, Depth: 0.0395, Combined: 2.1284, Adversarial: -0.9898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 Results:\n",
      "  Train Losses - Segmentation: 1.7375, Depth: 0.0216, Combined: 1.7491, Adversarial: -1.0019\n",
      "  Valid Losses - Segmentation: 2.0806, Depth: 0.0524, Combined: 2.1228, Adversarial: -1.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 Results:\n",
      "  Train Losses - Segmentation: 1.7486, Depth: 0.0222, Combined: 1.7612, Adversarial: -0.9562\n",
      "  Valid Losses - Segmentation: 2.0663, Depth: 0.0568, Combined: 2.1129, Adversarial: -1.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 Results:\n",
      "  Train Losses - Segmentation: 1.7643, Depth: 0.0222, Combined: 1.7762, Adversarial: -1.0374\n",
      "  Valid Losses - Segmentation: 2.0642, Depth: 0.0332, Combined: 2.0881, Adversarial: -0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 Results:\n",
      "  Train Losses - Segmentation: 1.7519, Depth: 0.0222, Combined: 1.7641, Adversarial: -1.0042\n",
      "  Valid Losses - Segmentation: 2.1443, Depth: 0.0402, Combined: 2.1703, Adversarial: -1.4194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 - Training:   0%|          | 0/125 [02:49<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 Results:\n",
      "  Train Losses - Segmentation: 1.7454, Depth: 0.0226, Combined: 1.7579, Adversarial: -1.0085\n",
      "  Valid Losses - Segmentation: 2.0982, Depth: 0.0375, Combined: 2.1254, Adversarial: -1.0345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 Results:\n",
      "  Train Losses - Segmentation: 1.7637, Depth: 0.0231, Combined: 1.7768, Adversarial: -1.0016\n",
      "  Valid Losses - Segmentation: 2.0261, Depth: 0.0491, Combined: 2.0658, Adversarial: -0.9470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 Results:\n",
      "  Train Losses - Segmentation: 1.7657, Depth: 0.0234, Combined: 1.7796, Adversarial: -0.9505\n",
      "  Valid Losses - Segmentation: 2.0412, Depth: 0.0307, Combined: 2.0631, Adversarial: -0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 Results:\n",
      "  Train Losses - Segmentation: 1.7841, Depth: 0.0239, Combined: 1.7983, Adversarial: -0.9748\n",
      "  Valid Losses - Segmentation: 2.0410, Depth: 0.0382, Combined: 2.0689, Adversarial: -1.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 Results:\n",
      "  Train Losses - Segmentation: 1.7391, Depth: 0.0220, Combined: 1.7513, Adversarial: -0.9859\n",
      "  Valid Losses - Segmentation: 2.0632, Depth: 0.0443, Combined: 2.0976, Adversarial: -0.9948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 Results:\n",
      "  Train Losses - Segmentation: 1.7797, Depth: 0.0238, Combined: 1.7932, Adversarial: -1.0305\n",
      "  Valid Losses - Segmentation: 2.0909, Depth: 0.0424, Combined: 2.1242, Adversarial: -0.9133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 - Training:   0%|          | 0/125 [02:49<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 Results:\n",
      "  Train Losses - Segmentation: 1.7831, Depth: 0.0240, Combined: 1.7974, Adversarial: -0.9797\n",
      "  Valid Losses - Segmentation: 2.0376, Depth: 0.0305, Combined: 2.0577, Adversarial: -1.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 - Training:   0%|          | 0/125 [02:49<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 Results:\n",
      "  Train Losses - Segmentation: 1.7762, Depth: 0.0235, Combined: 1.7892, Adversarial: -1.0451\n",
      "  Valid Losses - Segmentation: 2.2511, Depth: 0.0891, Combined: 2.3294, Adversarial: -1.0860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 Results:\n",
      "  Train Losses - Segmentation: 1.7778, Depth: 0.0230, Combined: 1.7901, Adversarial: -1.0709\n",
      "  Valid Losses - Segmentation: 2.0475, Depth: 0.0448, Combined: 2.0816, Adversarial: -1.0660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 Results:\n",
      "  Train Losses - Segmentation: 1.7810, Depth: 0.0241, Combined: 1.7946, Adversarial: -1.0513\n",
      "  Valid Losses - Segmentation: 2.0962, Depth: 0.0346, Combined: 2.1207, Adversarial: -1.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 Results:\n",
      "  Train Losses - Segmentation: 1.7599, Depth: 0.0232, Combined: 1.7721, Adversarial: -1.1022\n",
      "  Valid Losses - Segmentation: 2.0749, Depth: 0.0574, Combined: 2.1221, Adversarial: -1.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 Results:\n",
      "  Train Losses - Segmentation: 1.8006, Depth: 0.0237, Combined: 1.8143, Adversarial: -0.9912\n",
      "  Valid Losses - Segmentation: 2.0648, Depth: 0.0604, Combined: 2.1160, Adversarial: -0.9213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 Results:\n",
      "  Train Losses - Segmentation: 1.7682, Depth: 0.0245, Combined: 1.7824, Adversarial: -1.0326\n",
      "  Valid Losses - Segmentation: 2.0481, Depth: 0.0830, Combined: 2.1239, Adversarial: -0.7139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 Results:\n",
      "  Train Losses - Segmentation: 1.7780, Depth: 0.0268, Combined: 1.7941, Adversarial: -1.0657\n",
      "  Valid Losses - Segmentation: 2.1529, Depth: 0.0514, Combined: 2.1934, Adversarial: -1.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 Results:\n",
      "  Train Losses - Segmentation: 1.7838, Depth: 0.0241, Combined: 1.7971, Adversarial: -1.0747\n",
      "  Valid Losses - Segmentation: 2.1563, Depth: 0.0361, Combined: 2.1823, Adversarial: -1.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100 Results:\n",
      "  Train Losses - Segmentation: 1.7473, Depth: 0.0252, Combined: 1.7618, Adversarial: -1.0705\n",
      "  Valid Losses - Segmentation: 2.1386, Depth: 0.0430, Combined: 2.1714, Adversarial: -1.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 83 with combined loss 2.0152\n",
      "Epoch 83/100 Results:\n",
      "  Train Losses - Segmentation: 1.7534, Depth: 0.0236, Combined: 1.7659, Adversarial: -1.1074\n",
      "  Valid Losses - Segmentation: 2.0024, Depth: 0.0354, Combined: 2.0265, Adversarial: -1.1271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100 Results:\n",
      "  Train Losses - Segmentation: 1.7822, Depth: 0.0238, Combined: 1.7945, Adversarial: -1.1516\n",
      "  Valid Losses - Segmentation: 2.2594, Depth: 0.0393, Combined: 2.2871, Adversarial: -1.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100 Results:\n",
      "  Train Losses - Segmentation: 1.7967, Depth: 0.0246, Combined: 1.8097, Adversarial: -1.1545\n",
      "  Valid Losses - Segmentation: 2.1953, Depth: 0.0344, Combined: 2.2177, Adversarial: -1.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/100 - Training:   0%|          | 0/125 [02:55<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 86 with combined loss 2.0068\n",
      "Epoch 86/100 Results:\n",
      "  Train Losses - Segmentation: 1.8043, Depth: 0.0249, Combined: 1.8181, Adversarial: -1.1117\n",
      "  Valid Losses - Segmentation: 1.9948, Depth: 0.0288, Combined: 2.0152, Adversarial: -0.8425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 Results:\n",
      "  Train Losses - Segmentation: 1.7761, Depth: 0.0251, Combined: 1.7892, Adversarial: -1.2052\n",
      "  Valid Losses - Segmentation: 2.0116, Depth: 0.0397, Combined: 2.0391, Adversarial: -1.2246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100 Results:\n",
      "  Train Losses - Segmentation: 1.7745, Depth: 0.0244, Combined: 1.7862, Adversarial: -1.2660\n",
      "  Valid Losses - Segmentation: 1.9988, Depth: 0.0384, Combined: 2.0254, Adversarial: -1.1842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100 Results:\n",
      "  Train Losses - Segmentation: 1.7858, Depth: 0.0238, Combined: 1.7972, Adversarial: -1.2402\n",
      "  Valid Losses - Segmentation: 2.1381, Depth: 0.0451, Combined: 2.1706, Adversarial: -1.2621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 Results:\n",
      "  Train Losses - Segmentation: 1.7677, Depth: 0.0247, Combined: 1.7803, Adversarial: -1.1998\n",
      "  Valid Losses - Segmentation: 2.0033, Depth: 0.0736, Combined: 2.0643, Adversarial: -1.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 - Training:   0%|          | 0/125 [02:50<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 Results:\n",
      "  Train Losses - Segmentation: 1.7997, Depth: 0.0271, Combined: 1.8144, Adversarial: -1.2458\n",
      "  Valid Losses - Segmentation: 2.1587, Depth: 0.0360, Combined: 2.1830, Adversarial: -1.1754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 - Training:   0%|          | 0/125 [02:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 Results:\n",
      "  Train Losses - Segmentation: 1.7589, Depth: 0.0244, Combined: 1.7706, Adversarial: -1.2675\n",
      "  Valid Losses - Segmentation: 2.0181, Depth: 0.0544, Combined: 2.0583, Adversarial: -1.4157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/100 - Training:   0%|          | 0/125 [02:52<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100 Results:\n",
      "  Train Losses - Segmentation: 1.7702, Depth: 0.0250, Combined: 1.7825, Adversarial: -1.2707\n",
      "  Valid Losses - Segmentation: 2.0902, Depth: 0.0403, Combined: 2.1181, Adversarial: -1.2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100 Results:\n",
      "  Train Losses - Segmentation: 1.8032, Depth: 0.0252, Combined: 1.8162, Adversarial: -1.2214\n",
      "  Valid Losses - Segmentation: 2.4374, Depth: 0.0338, Combined: 2.4580, Adversarial: -1.3257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/100 - Training:   0%|          | 0/125 [02:56<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100 Results:\n",
      "  Train Losses - Segmentation: 1.7755, Depth: 0.0258, Combined: 1.7882, Adversarial: -1.3132\n",
      "  Valid Losses - Segmentation: 2.0545, Depth: 0.0360, Combined: 2.0781, Adversarial: -1.2402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/100 - Training:   0%|          | 0/125 [02:58<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100 Results:\n",
      "  Train Losses - Segmentation: 1.7972, Depth: 0.0244, Combined: 1.8091, Adversarial: -1.2513\n",
      "  Valid Losses - Segmentation: 2.3038, Depth: 0.0606, Combined: 2.3509, Adversarial: -1.3549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/100 - Training:   0%|          | 0/125 [02:56<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100 Results:\n",
      "  Train Losses - Segmentation: 1.7732, Depth: 0.0257, Combined: 1.7836, Adversarial: -1.5392\n",
      "  Valid Losses - Segmentation: 2.1059, Depth: 0.0473, Combined: 2.1325, Adversarial: -2.0689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 Results:\n",
      "  Train Losses - Segmentation: 1.7531, Depth: 0.0249, Combined: 1.7651, Adversarial: -1.2852\n",
      "  Valid Losses - Segmentation: 2.2048, Depth: 0.0976, Combined: 2.2886, Adversarial: -1.3782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/100 - Training:   0%|          | 0/125 [02:53<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100 Results:\n",
      "  Train Losses - Segmentation: 1.7714, Depth: 0.0248, Combined: 1.7829, Adversarial: -1.3315\n",
      "  Valid Losses - Segmentation: 2.0458, Depth: 0.0572, Combined: 2.0898, Adversarial: -1.3195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 - Training:   0%|          | 0/125 [02:54<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 Results:\n",
      "  Train Losses - Segmentation: 1.7919, Depth: 0.0262, Combined: 1.8045, Adversarial: -1.3633\n",
      "  Valid Losses - Segmentation: 2.0056, Depth: 0.0539, Combined: 2.0454, Adversarial: -1.4092\n",
      "Training visualization saved as GIF at results_test12_final3_MTL_disc_channel_21/20241203_042841/training_visualization_20241203_042841.gif\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "# Instantiate Models\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "mobilenet_backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "# encoder = MobileNetV3Backbone(mobilenet_backbone.features)\n",
    "model = MultiTaskModel(backbone=mobilenet_backbone.features, num_seg_classes=20, depth_channels=1)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Initialize optimizers and schedulers\n",
    "opt_sched = initialize_optimizers_and_schedulers(model)\n",
    "\n",
    "# Access optimizers\n",
    "optimizers = opt_sched[\"optimizers\"]\n",
    "schedulers = opt_sched[\"schedulers\"]\n",
    "\n",
    "# Prepare Data Loaders (Ensure train_loader and valid_loader are ready)\n",
    "train_losses, valid_losses,save_dir2 = train_model_with_adversarial_loss_tracking(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    num_epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    opt_sched=opt_sched,\n",
    "    save_dir=\"results_test12_final3_MTL_disc_channel_21\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03941dac-8ce7-4c0c-b9c4-1aefdf36a9ee",
   "metadata": {},
   "source": [
    "# For second instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f5b697a1-5cdc-4683-aaea-c6c46840e916",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/results_test12_final3_MTL_disc_channel_21/20241203_042841'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_save_dir = os.path.join(os.getcwd(),'results_test12_final3_MTL_disc_channel_21')\n",
    "model_dir =  os.path.join(root_save_dir,'20241203_042841')\n",
    "# model_dir = os.path.join(model_dir,'combined_result')\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b1f08f35-de94-4973-acd6-0ae47637544f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.1311431/ipykernel_1118662/725688548.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "Epoch 1/70 - Training:   0%|          | 0/371 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 22.62 MiB is free. Including non-PyTorch memory, this process has 3.94 GiB memory in use. Process 1124940 has 11.49 GiB memory in use. Of the allocated memory 3.51 GiB is allocated by PyTorch, and 65.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m opt_sched \u001b[38;5;241m=\u001b[39m initialize_optimizers_and_schedulers(model)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Call the resume training function\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m resume_training_with_loss_tracking(\n\u001b[1;32m     17\u001b[0m     model_class\u001b[38;5;241m=\u001b[39mmodel_class,\n\u001b[1;32m     18\u001b[0m     model_dir\u001b[38;5;241m=\u001b[39mmodel_dir,\n\u001b[1;32m     19\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     20\u001b[0m     valid_loader\u001b[38;5;241m=\u001b[39mvalid_loader,\n\u001b[1;32m     21\u001b[0m     num_additional_epochs\u001b[38;5;241m=\u001b[39mnum_additional_epochs,\n\u001b[1;32m     22\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     23\u001b[0m     opt_sched\u001b[38;5;241m=\u001b[39mopt_sched,\n\u001b[1;32m     24\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_test8_final3_MTL_disc_channel_21\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m root_save_dir,model_dir\n",
      "Cell \u001b[0;32mIn[259], line 68\u001b[0m, in \u001b[0;36mresume_training_with_loss_tracking\u001b[0;34m(model_class, model_dir, train_loader, valid_loader, num_additional_epochs, device, opt_sched, save_dir)\u001b[0m\n\u001b[1;32m     64\u001b[0m         existing_valid_losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madv\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;241m8\u001b[39m]))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Train for additional epochs\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m train_losses, valid_losses,save_dir2 \u001b[38;5;241m=\u001b[39m train_model_with_adversarial_loss_tracking(\n\u001b[1;32m     69\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     70\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     71\u001b[0m     valid_loader\u001b[38;5;241m=\u001b[39mvalid_loader,\n\u001b[1;32m     72\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_additional_epochs,\n\u001b[1;32m     73\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     74\u001b[0m     opt_sched\u001b[38;5;241m=\u001b[39mopt_sched,\n\u001b[1;32m     75\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39msave_dir,\n\u001b[1;32m     76\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Combine the new losses with the existing ones\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m existing_train_losses\u001b[38;5;241m.\u001b[39mkeys():\n",
      "Cell \u001b[0;32mIn[258], line 119\u001b[0m, in \u001b[0;36mtrain_model_with_adversarial_loss_tracking\u001b[0;34m(model, train_loader, valid_loader, num_epochs, device, opt_sched, save_dir)\u001b[0m\n\u001b[1;32m    101\u001b[0m gen_adv_loss_seg, disc_loss_seg \u001b[38;5;241m=\u001b[39m compute_adversarial_losses(\n\u001b[1;32m    102\u001b[0m     model\u001b[38;5;241m.\u001b[39mseg_discriminator,\n\u001b[1;32m    103\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseg_real_disc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     real_inputs\u001b[38;5;241m=\u001b[39mseg_labels,  \u001b[38;5;66;03m# Use only for WGAN-GP\u001b[39;00m\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m gen_adv_loss_depth, disc_loss_depth \u001b[38;5;241m=\u001b[39m compute_adversarial_losses(\n\u001b[1;32m    111\u001b[0m     model\u001b[38;5;241m.\u001b[39mdepth_discriminator,\n\u001b[1;32m    112\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_real_disc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     real_inputs\u001b[38;5;241m=\u001b[39mdepth_labels,\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 119\u001b[0m gen_adv_loss_combined, disc_loss_combined \u001b[38;5;241m=\u001b[39m compute_adversarial_losses(\n\u001b[1;32m    120\u001b[0m     model\u001b[38;5;241m.\u001b[39mmulti_task_discriminator,\n\u001b[1;32m    121\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_real_disc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    122\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_fake_disc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach(),\n\u001b[1;32m    123\u001b[0m     hinge_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    124\u001b[0m     gradient_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m     real_inputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat([seg_labels, depth_labels], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m seg_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# adv_loss = -(\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#     torch.mean(outputs[\"seg_real_disc\"]) +\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#     torch.mean(outputs[\"depth_real_disc\"]) +\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#     torch.mean(outputs[\"combined_real_disc\"])\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Total Generator Loss\u001b[39;00m\n\u001b[1;32m    135\u001b[0m gen_adv_loss \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m \u001b[38;5;241m*\u001b[39m gen_adv_loss_seg \u001b[38;5;241m+\u001b[39m gen_adv_loss_depth \u001b[38;5;241m+\u001b[39m gen_adv_loss_combined\n",
      "Cell \u001b[0;32mIn[240], line 22\u001b[0m, in \u001b[0;36mcompute_adversarial_losses\u001b[0;34m(discriminator_model, discriminator_real, discriminator_fake, hinge_loss, gradient_penalty, real_inputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_penalty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hinge_loss:\n\u001b[1;32m     21\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(real_inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mreal_inputs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 22\u001b[0m     interpolates \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m real_inputs \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m real_inputs\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     23\u001b[0m     interpolates\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# interpolates_output = discriminator_real(interpolates)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Use the actual discriminator model for gradient penalty computation\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 22.62 MiB is free. Including non-PyTorch memory, this process has 3.94 GiB memory in use. Process 1124940 has 11.49 GiB memory in use. Of the allocated memory 3.51 GiB is allocated by PyTorch, and 65.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# root_save_dir = os.path.join(os.getcwd(),'results_test8_final2')\n",
    "# model_dir =  os.path.join(root_save_dir,'20241129_030455')\n",
    "num_additional_epochs = 70  # Number of epochs to continue training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the model class and loaders\n",
    "mobilenet_backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "model_class = lambda: MultiTaskModel(backbone=mobilenet_backbone.features, num_seg_classes=20, depth_channels=1)\n",
    "\n",
    "# Initialize optimizers and schedulers\n",
    "model = model_class()\n",
    "opt_sched = initialize_optimizers_and_schedulers(model)\n",
    "\n",
    "\n",
    "# Call the resume training function\n",
    "resume_training_with_loss_tracking(\n",
    "    model_class=model_class,\n",
    "    model_dir=model_dir,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    num_additional_epochs=num_additional_epochs,\n",
    "    device=device,\n",
    "    opt_sched=opt_sched,\n",
    "    save_dir=\"results_test8_final3_MTL_disc_channel_21\"\n",
    ")\n",
    "\n",
    "root_save_dir,model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7a8fa-2148-4685-b4cd-9d6222949210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input(\"Enter timestamp folder value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8dd9190-d327-476b-a992-0f3d1a86f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f871a15-59f2-4357-a525-263a4012dff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78220e94-129e-457c-8865-ecfa98445b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ed5e4-2134-4aee-8a93-4aa65cb10b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64331738-e822-49ee-a26a-71b5a36bd9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cfffa11-a02d-4156-b088-88a3f4830818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined GIF saved to /home/rmajumd/2024/ML_in_image_synthesis/Cityscapes/Cityscapes/results_test8_final/20241129_024547/combined_results.gif\n"
     ]
    }
   ],
   "source": [
    "# root = os.path.join(os.getcwd(), 'results_test8_final' )\n",
    "# model_dir = os.path.join(root,'20241129_015944')\n",
    "# save_dir2 = os.path.join(root,'20241129_024547')\n",
    "\n",
    "# output_path = os.path.join(save_dir2,'combined_results.gif')\n",
    "# combine_training_gifs(model_dir, save_dir2, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b24c6-0d86-4c55-9e9f-51782eb1b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with perpetual loss later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec784a8-16ae-4ce3-bfee-c676246a3c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# from datetime import datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.utils import save_image\n",
    "\n",
    "# def train_model_with_loss_tracking(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, opt_sched, save_dir=\"results\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Trains a multi-task model with Conditional GANs, structural consistency, and perceptual loss.\n",
    "\n",
    "#     Args:\n",
    "#         model: The multi-task model to train.\n",
    "#         train_loader: DataLoader for training data.\n",
    "#         valid_loader: DataLoader for validation data.\n",
    "#         num_epochs: Number of epochs to train.\n",
    "#         device: Device for training (\"cuda\" or \"cpu\").\n",
    "#         opt_sched: Dictionary of optimizers and schedulers.\n",
    "#         save_dir: Directory to save results.\n",
    "\n",
    "#     Returns:\n",
    "#         train_losses, valid_losses: Lists of losses for training and validation.\n",
    "#     \"\"\"\n",
    "#     # Create directories for saving results\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     # Prepare CSV file\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_smooth\", \"train_seg_iou\", \"train_seg_perceptual_loss\",\n",
    "#             \"train_depth_perceptual_loss\", \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_smooth\", \"valid_seg_iou\", \"valid_seg_perceptual_loss\",\n",
    "#             \"valid_depth_perceptual_loss\"\n",
    "#         ])\n",
    "\n",
    "#     # Initialize tracking variables\n",
    "#     train_losses = {\n",
    "#         \"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": [],\n",
    "#         \"seg_perceptual\": [], \"depth_perceptual\": []\n",
    "#     }\n",
    "#     valid_losses = {\n",
    "#         \"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": [],\n",
    "#         \"seg_perceptual\": [], \"depth_perceptual\": []\n",
    "#     }\n",
    "#     best_combined_loss = float(\"inf\")\n",
    "#     gif_frames = []\n",
    "\n",
    "#     # Perceptual Loss (example using VGG features)\n",
    "#     perceptual_loss_fn = PerceptualLoss(pretrained_model=\"vgg16\").to(device)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = (\n",
    "#                 batch[\"left\"].to(device),\n",
    "#                 batch[\"mask\"].to(device),\n",
    "#                 batch[\"depth\"].to(device)\n",
    "#             )\n",
    "#             latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#             # Zero gradients\n",
    "#             for optimizer in opt_sched[\"optimizers\"].values():\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(inputs, input_size=inputs.size()[-2:])\n",
    "\n",
    "#             # Loss calculations\n",
    "#             seg_loss_task = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "#             seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "#             seg_loss = seg_loss_task + 0.1 * seg_perceptual_loss\n",
    "\n",
    "#             depth_loss_sidl = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#             depth_loss_huber = inv_huber_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#             depth_loss_smooth = depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "#             depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "#             depth_loss = depth_loss_sidl + depth_loss_huber + depth_loss_smooth + 0.1 * depth_perceptual_loss\n",
    "\n",
    "#             combined_loss = seg_loss + depth_loss\n",
    "\n",
    "#             # Backpropagation\n",
    "#             combined_loss.backward()\n",
    "#             for optimizer in opt_sched[\"optimizers\"].values():\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             # Update training metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += depth_loss.item()\n",
    "#             epoch_train[\"combined\"] += combined_loss.item()\n",
    "#             epoch_train[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_loss_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_loss_smooth.item()\n",
    "#             epoch_train[\"seg_perceptual\"] += seg_perceptual_loss.item()\n",
    "#             epoch_train[\"depth_perceptual\"] += depth_perceptual_loss.item()\n",
    "#             num_batches += 1\n",
    "\n",
    "#         # Average training metrics\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         # Validation loop\n",
    "#         model.eval()\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 inputs, seg_labels, depth_labels = (\n",
    "#                     batch[\"left\"].to(device),\n",
    "#                     batch[\"mask\"].to(device),\n",
    "#                     batch[\"depth\"].to(device)\n",
    "#                 )\n",
    "#                 latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(inputs, input_size=inputs.size()[-2:])\n",
    "\n",
    "#                 # Validation loss calculations\n",
    "#                 seg_loss_task = nn.CrossEntropyLoss()(outputs[\"seg_output\"], seg_labels) + dice_loss(outputs[\"seg_output\"], seg_labels)\n",
    "#                 seg_perceptual_loss = perceptual_loss_fn(outputs[\"seg_output\"], seg_labels.unsqueeze(1))\n",
    "#                 seg_loss = seg_loss_task + 0.1 * seg_perceptual_loss\n",
    "\n",
    "#                 depth_loss_sidl = scale_invariant_depth_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#                 depth_loss_huber = inv_huber_loss(outputs[\"depth_output\"], depth_labels)\n",
    "#                 depth_loss_smooth = depth_smoothness_loss(outputs[\"depth_output\"], inputs)\n",
    "#                 depth_perceptual_loss = perceptual_loss_fn(outputs[\"depth_output\"], depth_labels)\n",
    "#                 depth_loss = depth_loss_sidl + depth_loss_huber + depth_loss_smooth + 0.1 * depth_perceptual_loss\n",
    "\n",
    "#                 combined_loss = seg_loss + depth_loss\n",
    "\n",
    "#                 # Update validation metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += depth_loss.item()\n",
    "#                 epoch_valid[\"combined\"] += combined_loss.item()\n",
    "#                 epoch_valid[\"iou\"] += mean_iou(outputs[\"seg_output\"], seg_labels, num_classes=20).item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_loss_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_loss_smooth.item()\n",
    "#                 epoch_valid[\"seg_perceptual\"] += seg_perceptual_loss.item()\n",
    "#                 epoch_valid[\"depth_perceptual\"] += depth_perceptual_loss.item()\n",
    "#                 num_valid_batches += 1\n",
    "\n",
    "#         # Average validation metrics\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "#         # Save best model\n",
    "#         valid_combined_loss = epoch_valid[\"combined\"] / num_valid_batches\n",
    "#         if valid_combined_loss < best_combined_loss:\n",
    "#             best_combined_loss = valid_combined_loss\n",
    "#             torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "\n",
    "#         # Append metrics to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 epoch_train[\"seg\"] / num_batches,\n",
    "#                 epoch_train[\"depth\"] / num_batches,\n",
    "#                 epoch_train[\"combined\"] / num_batches,\n",
    "#                 epoch_train[\"depth_sidl\"] / num_batches,\n",
    "#                 epoch_train[\"depth_smooth\"] / num_batches,\n",
    "#                 epoch_train[\"iou\"] / num_batches,\n",
    "#                 epoch_train[\"seg_perceptual\"] / num_batches,\n",
    "#                 epoch_train[\"depth_perceptual\"] / num_batches,\n",
    "#                 epoch_valid[\"seg\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"combined\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_sidl\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_smooth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"iou\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"seg_perceptual\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_perceptual\"] / num_valid_batches,\n",
    "#             ])\n",
    "\n",
    "#         # Update schedulers\n",
    "#         for scheduler in opt_sched[\"schedulers\"].values():\n",
    "#             scheduler.step()\n",
    "            \n",
    "#     plot_all_losses(train_losses,valid_losses)\n",
    "\n",
    "    \n",
    "\n",
    "#     return train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1f745-2964-4cc1-8211-49fcf03d216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from datetime import datetime\n",
    "# from torchvision.utils import save_image\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Updated Train Function\n",
    "# def train_model_with_loss_tracking_and_gif(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, save_dir=\"training_output_bicycle_and_pix2pix\"\n",
    "# ):\n",
    "#     # Create directories for saving models and outputs\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "\n",
    "#     # Initialize CSV for saving loss data\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_smooth\", \"train_seg_iou\",\n",
    "#             \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_smooth\", \"valid_seg_iou\"\n",
    "#         ])\n",
    "\n",
    "#     best_combined_loss = float(\"inf\")  # Initialize best combined loss for saving the best model\n",
    "#     train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     gif_frames = []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         model.train()\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         # Training Loop\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "#             latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#             model.optimizer_stage1.zero_grad()\n",
    "#             model.optimizer_stage2.zero_grad()\n",
    "\n",
    "#             # Forward Pass\n",
    "#             outputs = model(inputs, seg_labels, depth_labels, latent_noise)\n",
    "#             seg_output = outputs[\"seg_output\"]\n",
    "#             depth_output = outputs[\"depth_output\"]\n",
    "\n",
    "#             # Loss Calculations\n",
    "#             seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels.squeeze(1))\n",
    "#             seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#             seg_iou = mean_iou(seg_output, seg_labels, num_classes=20)\n",
    "#             seg_loss_total = 0.6 * seg_loss + 0.4 * seg_dice\n",
    "\n",
    "#             depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#             depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#             depth_loss_total = depth_sidl + depth_smooth\n",
    "\n",
    "#             # Combined Loss\n",
    "#             total_loss = seg_loss_total + depth_loss_total\n",
    "#             total_loss.backward()\n",
    "\n",
    "#             # Optimizers Step\n",
    "#             model.optimizer_stage1.step()\n",
    "#             model.optimizer_stage2.step()\n",
    "\n",
    "#             # Accumulate Training Metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += depth_loss_total.item()\n",
    "#             epoch_train[\"combined\"] += total_loss.item()\n",
    "#             epoch_train[\"iou\"] += seg_iou.item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_smooth.item()\n",
    "#             num_batches += 1\n",
    "\n",
    "#             # Save training images for visualization\n",
    "#             if num_batches % 10 == 0:\n",
    "#                 img_grid = torch.cat([inputs[0], seg_output[0].argmax(0, keepdim=True), depth_output[0]], dim=2)\n",
    "#                 save_image(img_grid, os.path.join(save_dir, f\"train_{epoch}_{num_batches}.png\"))\n",
    "#                 gif_frames.append(Image.open(os.path.join(save_dir, f\"train_{epoch}_{num_batches}.png\")))\n",
    "\n",
    "#         model.scheduler_stage1.step()\n",
    "#         model.scheduler_stage2.step(epoch_train[\"combined\"] / num_batches)\n",
    "\n",
    "#         # Average Training Losses\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "#                 latent_noise = torch.randn(inputs.size(0), 3).to(device)\n",
    "\n",
    "#                 outputs = model(inputs, seg_labels, depth_labels, latent_noise)\n",
    "#                 seg_output = outputs[\"seg_output\"]\n",
    "#                 depth_output = outputs[\"depth_output\"]\n",
    "\n",
    "#                 # Validation Loss Calculations\n",
    "#                 seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels.squeeze(1))\n",
    "#                 seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#                 seg_iou = mean_iou(seg_output, seg_labels, num_classes=20)\n",
    "#                 seg_loss_total = 0.6 * seg_loss + 0.4 * seg_dice\n",
    "\n",
    "#                 depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#                 depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#                 depth_loss_total = depth_sidl + depth_smooth\n",
    "\n",
    "#                 # Accumulate Validation Metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += depth_loss_total.item()\n",
    "#                 epoch_valid[\"combined\"] += (seg_loss_total + depth_loss_total).item()\n",
    "#                 epoch_valid[\"iou\"] += seg_iou.item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_smooth.item()\n",
    "#                 num_valid_batches += 1\n",
    "            \n",
    "#             frame = save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels)\n",
    "#             gif_frames.append(frame)\n",
    "              \n",
    "\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "#         # Save Model if Validation Loss Improves\n",
    "#         valid_combined_loss = epoch_valid[\"combined\"] / num_valid_batches\n",
    "#         if valid_combined_loss < best_combined_loss:\n",
    "#             best_combined_loss = valid_combined_loss\n",
    "#             torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "#             print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "#         if epoch%10==0:\n",
    "#             gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "#             gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "            \n",
    "\n",
    "#         # Append Validation Metrics to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 epoch_train[\"seg\"] / num_batches,\n",
    "#                 epoch_train[\"depth\"] / num_batches,\n",
    "#                 epoch_train[\"combined\"] / num_batches,\n",
    "#                 epoch_train[\"depth_sidl\"] / num_batches,\n",
    "#                 epoch_train[\"depth_smooth\"] / num_batches,\n",
    "#                 epoch_train[\"iou\"] / num_batches,\n",
    "#                 epoch_valid[\"seg\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"combined\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_sidl\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"depth_smooth\"] / num_valid_batches,\n",
    "#                 epoch_valid[\"iou\"] / num_valid_batches,\n",
    "#             ])\n",
    "\n",
    "#     # Save GIF\n",
    "#     gif_frames[0].save(\n",
    "#         gif_path, save_all=True, append_images=gif_frames[1:], duration=200, loop=0\n",
    "#     )\n",
    "\n",
    "#     return train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf71573-fe0d-4309-beb5-7e12adf7cbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c01b5-8405-467c-ba4f-731f05cef85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce4c3f7b-a9d8-45e3-8ca2-288c1734d22f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def train_model_with_loss_tracking_and_gif(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, save_dir=\"training_output_bicycle_and_pix2pix\"):\n",
    "#     # Create directory for saving models and outputs\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "\n",
    "#     # Initialize CSV for saving loss data\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_inv_huber\", \"train_depth_contrastive\", \"train_depth_smooth\",\n",
    "#             \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_inv_huber\", \"valid_depth_contrastive\", \"valid_depth_smooth\"\n",
    "#         ])\n",
    "\n",
    "#     best_combined_loss = float(\"inf\")  # Initialize best combined loss for saving the best model\n",
    "\n",
    "#     train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     # , \"depth_inv_huber\": [], \"depth_contrastive\": []\n",
    "\n",
    "#     gif_frames = []\n",
    "#     num_classes = 20\n",
    "#     # Optimizer for latent noise\n",
    "    \n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#         model.train()\n",
    "#         # epoch_train_seg_loss = 0\n",
    "#         # epoch_train_depth_loss = 0\n",
    "#         # epoch_train_iou = 0\n",
    "#         # epoch_train_combined_loss = 0\n",
    "#         # epoch_train_depth_sidl = 0\n",
    "#         # epoch_train_depth_inv_huber = 0\n",
    "#         # epoch_train_depth_contrastive = 0\n",
    "#         # epoch_train_depth_smooth = 0\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         reconstruction_layer = nn.Conv2d(256, 3, kernel_size=1).to(device)\n",
    "        \n",
    "#         # scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#         # Training Loop\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "#             print(inputs.shape,seg_labels.shape,depth_labels.shape) # torch.Size([8, 3, 200, 512]) torch.Size([8, 1, 200, 512]) torch.Size([8, 1, 1, 200, 512])\n",
    "#             return\n",
    "        \n",
    "                       \n",
    "\n",
    "\n",
    "#             # Forward pass\n",
    "#             seg_output, depth_output, backbone_features = model(...)\n",
    "            \n",
    "\n",
    "\n",
    "#             seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#             seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#             seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#             seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "            \n",
    "#             depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#             depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#             depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#             depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "            \n",
    "           \n",
    "           \n",
    "#             # Combined Loss\n",
    "#             total_loss = bicycle_loss + pix2pix_total_loss\n",
    "\n",
    "#             # Single backward pass\n",
    "#             total_loss.backward()\n",
    "\n",
    "#             # Update both optimizers\n",
    "#             model.optimizer_stage1.step()\n",
    "#             model.optimizer_stage2.step()\n",
    "#             latent_optimizer.step()\n",
    "\n",
    "#             # Accumulate Training Metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#             epoch_train[\"combined\"] += total_loss.item()\n",
    "#             epoch_train[\"iou\"] += seg_iou.item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_smooth.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#         model.scheduler_stage1.step()\n",
    "#         model.scheduler_stage2.step(epoch_train[\"combined\"]/num_batches)\n",
    "\n",
    "\n",
    "#         # Average Training Losses\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Train Seg Loss: {epoch_train['seg']:.4f}, \"\n",
    "#             f\"Train Depth Loss: {epoch_train['depth']:.4f}, Train Combined Loss: {epoch_train['combined']:.4f}, \"\n",
    "#             f\"Train mIOU: {epoch_train['iou']:.4f}, Train sidl Loss: {epoch_train['depth_sidl']:.4f}, \"\n",
    "#             f\"Train depth smooth: {epoch_train['depth_smooth']:.4f}\"\n",
    "#     )       \n",
    "\n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 # print(\"inside valid\")\n",
    "#                 inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "\n",
    "#                 # Ensure depth_labels and segmentation labels have correct dimensions\n",
    "                \n",
    "\n",
    "               \n",
    "\n",
    "\n",
    "#                 seg_output_old =seg_output\n",
    "#                 # Resize seg_output to match the spatial dimensions of seg_labels\n",
    "#                 seg_output_resized = F.interpolate(seg_output, size=seg_labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "#                 seg_output = seg_output_resized\n",
    "\n",
    "#                 depth_output_old = depth_output\n",
    "#                 depth_output_resized = F.interpolate(depth_output, size=depth_labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "#                 depth_output =depth_output_resized\n",
    "\n",
    "\n",
    "#                 # Segmentation Loss\n",
    "#                 seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#                 seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#                 seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#                 seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "                \n",
    "#                 depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#                 depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#                 depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#                 depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "\n",
    "#                 pix2pix_loss = seg_loss_total + depth_loss_total\n",
    "\n",
    "#                 # Combined Validation Loss\n",
    "#                 combined_loss = pix2pix_loss\n",
    "\n",
    "#                 # Accumulate Validation Metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#                 epoch_valid[\"combined\"] += combined_loss.item()\n",
    "#                 epoch_valid[\"iou\"] += seg_iou.item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_smooth.item()\n",
    "                \n",
    "#                 num_valid_batches += 1\n",
    "                \n",
    "#             frame = save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels)\n",
    "#             gif_frames.append(frame)\n",
    "                \n",
    "                \n",
    "#         # Calculate epoch averages\n",
    "#         # Average Validation Losses\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Valid Seg Loss: {epoch_valid['seg']:.4f}, \"\n",
    "#             f\"Valid Depth Loss: {epoch_valid['depth']:.4f}, Valid Combined Loss: {epoch_valid['combined']:.4f}, \"\n",
    "#             f\"Valid mIOU: {epoch_valid['iou']:.4f}, Valid sidl Loss: {epoch_valid['depth_sidl']:.4f}, \"\n",
    "#             f\"Valid depth smooth: {epoch_valid['depth_smooth']:.4f}\"\n",
    "#         )\n",
    "\n",
    "#         # Write the losses to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 train_losses[\"seg\"], train_losses[\"depth\"], train_losses[\"combined\"],\n",
    "#                 train_losses[\"depth_sidl\"], 0,0,\n",
    "#                 # avg_train_depth_inv_huber, avg_train_depth_contrastive,\n",
    "#                 train_losses[\"depth_smooth\"],\n",
    "#                 valid_losses[\"seg\"], valid_losses[\"depth\"], valid_losses['combined'],\n",
    "#                 valid_losses[\"depth_sidl\"],0,0,\n",
    "#                 # avg_valid_depth_inv_huber, avg_valid_depth_contrastive, \n",
    "#                 valid_losses[\"depth_smooth\"]\n",
    "#             ])\n",
    "\n",
    "       \n",
    "#         # Save best model\n",
    "#         if valid_losses[\"combined\"][-1] < best_combined_loss:\n",
    "#             best_combined_loss = valid_losses[\"combined\"][-1]\n",
    "#             torch.save(model, os.path.join(save_dir, \"best_model_resnetBackbone.pth\"))\n",
    "#             print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "#         if epoch%10==0:\n",
    "#             gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "#             gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "    \n",
    "    \n",
    "#     plot_loss(train_losses, valid_losses, save_dir)\n",
    "#     gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return train_losses,valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28f06b7e-8a7c-40d1-9ff2-d5bc2219bc74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Create your model instance\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = 'cpu'\n",
    "# model = MultiTaskModel(num_seg_classes=20, feature_channels=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6a0e2e4-241b-4096-ab67-41abda0c3a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Set the number of epochs\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Call the training function\n",
    "# train_losses, valid_losses = train_model_with_loss_tracking_and_gif(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     valid_loader=valid_loader,\n",
    "#     num_epochs=num_epochs,\n",
    "#     device=device,\n",
    "#     save_dir=\"test7_res\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c51a6d-a75d-429c-aef9-8e547a5fe8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789b4aa-a31c-4298-ae2a-c8c13c30da39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d70c33-2426-4327-9b66-208e8c419e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75f3e6-0e5b-4748-a1d0-718357fc8092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3db3d8-d07c-4a04-8615-1f6a59f5d1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e599ee-b582-4b1f-aa67-3b756e18c23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c1039-9a3d-434c-8585-ded47ce50261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720176f-f280-452e-8809-5274dc540273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876589d-4937-4435-ac84-cc235e43a99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64d7f8b2-0558-4745-9647-e6c65811861a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def train_model_with_loss_tracking_and_gif(\n",
    "#     model, train_loader, valid_loader, num_epochs, device, save_dir=\"training_output_bicycle_and_pix2pix\"):\n",
    "#     # Create directory for saving models and outputs\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     save_dir = os.path.join(save_dir, timestamp)\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     csv_path = os.path.join(save_dir, f\"loss_tracking_{timestamp}.csv\")\n",
    "#     gif_path = os.path.join(save_dir, f\"training_visualization_{timestamp}.gif\")\n",
    "\n",
    "#     # Initialize CSV for saving loss data\n",
    "#     with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([\n",
    "#             \"epoch\", \"train_seg_loss\", \"train_depth_loss\", \"train_combined_loss\",\n",
    "#             \"train_depth_sidl\", \"train_depth_inv_huber\", \"train_depth_contrastive\", \"train_depth_smooth\",\n",
    "#             \"valid_seg_loss\", \"valid_depth_loss\", \"valid_combined_loss\",\n",
    "#             \"valid_depth_sidl\", \"valid_depth_inv_huber\", \"valid_depth_contrastive\", \"valid_depth_smooth\"\n",
    "#         ])\n",
    "\n",
    "#     best_combined_loss = float(\"inf\")  # Initialize best combined loss for saving the best model\n",
    "\n",
    "#     train_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     valid_losses = {\"seg\": [], \"depth\": [], \"combined\": [], \"iou\": [], \"depth_sidl\": [], \"depth_smooth\": []}\n",
    "#     # , \"depth_inv_huber\": [], \"depth_contrastive\": []\n",
    "\n",
    "#     gif_frames = []\n",
    "#     num_classes = 20\n",
    "#     # Optimizer for latent noise\n",
    "    \n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "#         model.train()\n",
    "#         # epoch_train_seg_loss = 0\n",
    "#         # epoch_train_depth_loss = 0\n",
    "#         # epoch_train_iou = 0\n",
    "#         # epoch_train_combined_loss = 0\n",
    "#         # epoch_train_depth_sidl = 0\n",
    "#         # epoch_train_depth_inv_huber = 0\n",
    "#         # epoch_train_depth_contrastive = 0\n",
    "#         # epoch_train_depth_smooth = 0\n",
    "#         epoch_train = {key: 0.0 for key in train_losses.keys()}\n",
    "#         num_batches = 0\n",
    "\n",
    "#         reconstruction_layer = nn.Conv2d(256, 3, kernel_size=1).to(device)\n",
    "        \n",
    "#         # scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#         # Training Loop\n",
    "#         for batch in train_loader:\n",
    "#             inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "\n",
    "#             # Ensure depth_labels and segmentation labels have correct dimensions\n",
    "#             if depth_labels.dim() == 5:\n",
    "#                 depth_labels = depth_labels.squeeze(2)\n",
    "#             if seg_labels.dim() == 4 and seg_labels.shape[1] == 1:\n",
    "#                 seg_labels = seg_labels.squeeze(1)\n",
    "\n",
    "#             # Transform depth labels\n",
    "#             # depth_labels = torch.log(depth_labels.flatten(start_dim=1)) / 5\n",
    "#             # depth_labels = depth_labels.view_as(depth_labels)  # Restore shape\n",
    "#             # depth_labels = torch.clamp(depth_labels, min=1e-5) \n",
    "#             # depth_labels = torch.log(depth_labels + 1e-5) / 5  # Avoid log(0)\n",
    "\n",
    "#             # print(f'seg_labels shape : {seg_labels.shape}')\n",
    "#             # print(f'depth_labels shape: {depth_labels.shape}')\n",
    "\n",
    "#             # Start with random noise as latent condition\n",
    "#             if epoch == 0:\n",
    "#                 latent_noise = torch.randn_like(inputs).to(device)\n",
    "#                 # print(f\"latent_noise: {latent_noise.shape}\")\n",
    "#                 latent_noise.requires_grad = True  # Make it trainable\n",
    "#                 latent_optimizer = torch.optim.Adam([latent_noise], lr=1e-3)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "#             # Stage 1: Train BicycleGAN (Backbone Features)\n",
    "            \n",
    "\n",
    "#             # Reset gradients for both optimizers\n",
    "#             model.optimizer_stage1.zero_grad()\n",
    "#             model.optimizer_stage2.zero_grad()\n",
    "#             latent_optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             seg_output, depth_output, backbone_features = model(inputs, latent_noise)\n",
    "#             # print(f'seg_ouput shape : {seg_output.shape}')\n",
    "#             # print(f'depth_output shape: {depth_output.shape}')\n",
    "#             # print(backbone_features.shape)\n",
    "#             # return\n",
    "\n",
    "\n",
    "#             seg_output_old =seg_output\n",
    "#             # Resize seg_output to match the spatial dimensions of seg_labels\n",
    "#             seg_output_resized = F.interpolate(seg_output, size=seg_labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "#             seg_output = seg_output_resized\n",
    "\n",
    "#             # print(f\"depth_output shape before resize: {depth_output.shape}\")\n",
    "#             # print(f\"depth_labels shape: {depth_labels.shape}\")\n",
    "#             # return\n",
    "\n",
    "#             depth_output_old = depth_output\n",
    "#             depth_output_resized = F.interpolate(depth_output, size=depth_labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "#             depth_output = depth_output_resized\n",
    "\n",
    "\n",
    "#             # Pix2Pix Losses\n",
    "#             seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#             seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#             seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#             seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "            \n",
    "#             depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#             depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#             depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#             depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "            \n",
    "#             pix2pix_loss = seg_loss_total + depth_loss_total\n",
    "\n",
    "#             # Reconstruction loss\n",
    "#             # inputs_resized = F.interpolate(inputs, size=(backbone_features.size(2), backbone_features.size(3)))\n",
    "#             # reconstructed_image = reconstruction_layer(backbone_features)\n",
    "#             # recon_loss = nn.L1Loss()(reconstructed_image, inputs_resized)\n",
    "#             # adaptive_weight = 1 / (1 + torch.exp(-recon_loss))\n",
    "#             # adaptive_weight_value = adaptive_weight.item() \n",
    "\n",
    "#             # loss_stage1 = nn.MSELoss()(real_validity, torch.ones_like(real_validity).to(device)) + recon_loss\n",
    "#             # loss_stage1.backward(retain_graph=True)\n",
    "#             # model.optimizer_stage1.step()\n",
    "\n",
    "#             # Pix2Pix Adversarial Losses\n",
    "#             seg_validity = model.segmentation_discriminator(seg_output)\n",
    "#             depth_validity = model.depth_discriminator(depth_output)\n",
    "#             adv_seg_loss = nn.MSELoss()(seg_validity, torch.ones_like(seg_validity))\n",
    "#             adv_depth_loss = nn.MSELoss()(depth_validity, torch.ones_like(depth_validity))\n",
    "#             pix2pix_total_loss = pix2pix_loss + adv_seg_loss + adv_depth_loss\n",
    "\n",
    "\n",
    "#             # BicycleGAN Loss with Pix2Pix Condition\n",
    "#             # real_validity = model.bicycle_discriminator(backbone_features)\n",
    "#             # recon_loss = nn.L1Loss()(backbone_features, inputs)\n",
    "#             # bicycle_loss = nn.MSELoss()(real_validity, torch.ones_like(real_validity)) + recon_loss\n",
    "#             # conditional_bicycle_loss = bicycle_loss + pix2pix_loss\n",
    "#             # conditional_bicycle_loss.backward(retain_graph=True)\n",
    "#             # model.optimizer_stage1.step()\n",
    "\n",
    "#             # BicycleGAN Loss with Pix2Pix Condition\n",
    "#             real_validity = model.bicycle_discriminator(backbone_features,latent_noise)\n",
    "\n",
    "#             # Resize inputs to match backbone_features\n",
    "#             inputs_resized = F.interpolate(inputs, size=backbone_features.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "#             # print(f\"backbone_features shape: {backbone_features.shape}, inputs shape: {inputs.shape}\")\n",
    "#             # print(f\"inputs_resized shape: {inputs_resized.shape}\")\n",
    "#             # recon_loss = nn.L1Loss()(backbone_features, inputs_resized)\n",
    "#             bicycle_loss = adv_seg_loss + adv_depth_loss\n",
    "#             # + recon_loss\n",
    "\n",
    "#             # Combined Loss\n",
    "#             total_loss = bicycle_loss + pix2pix_total_loss\n",
    "\n",
    "#             # Single backward pass\n",
    "#             total_loss.backward()\n",
    "\n",
    "#             # Update both optimizers\n",
    "#             model.optimizer_stage1.step()\n",
    "#             model.optimizer_stage2.step()\n",
    "#             latent_optimizer.step()\n",
    "\n",
    "#             # Accumulate Training Metrics\n",
    "#             epoch_train[\"seg\"] += seg_loss.item()\n",
    "#             epoch_train[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#             epoch_train[\"combined\"] += total_loss.item()\n",
    "#             epoch_train[\"iou\"] += seg_iou.item()\n",
    "#             epoch_train[\"depth_sidl\"] += depth_sidl.item()\n",
    "#             epoch_train[\"depth_smooth\"] += depth_smooth.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#         model.scheduler_stage1.step()\n",
    "#         model.scheduler_stage2.step(epoch_train[\"combined\"]/num_batches)\n",
    "\n",
    "\n",
    "#         # Average Training Losses\n",
    "#         for key in epoch_train.keys():\n",
    "#             train_losses[key].append(epoch_train[key] / num_batches)\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Train Seg Loss: {epoch_train['seg']:.4f}, \"\n",
    "#             f\"Train Depth Loss: {epoch_train['depth']:.4f}, Train Combined Loss: {epoch_train['combined']:.4f}, \"\n",
    "#             f\"Train mIOU: {epoch_train['iou']:.4f}, Train sidl Loss: {epoch_train['depth_sidl']:.4f}, \"\n",
    "#             f\"Train depth smooth: {epoch_train['depth_smooth']:.4f}\"\n",
    "#     )       \n",
    "\n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         # epoch_valid_seg_loss = 0\n",
    "#         # epoch_valid_depth_loss = 0\n",
    "#         # epoch_valid_iou =0\n",
    "#         # epoch_valid_combined_loss = 0\n",
    "#         # epoch_valid_depth_sidl = 0\n",
    "#         # epoch_valid_depth_inv_huber = 0\n",
    "#         # epoch_valid_depth_contrastive = 0\n",
    "#         # epoch_valid_depth_smooth = 0\n",
    "#         epoch_valid = {key: 0.0 for key in valid_losses.keys()}\n",
    "#         num_valid_batches = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in valid_loader:\n",
    "#                 # print(\"inside valid\")\n",
    "#                 inputs, seg_labels, depth_labels = batch[\"left\"].to(device), batch[\"mask\"].to(device), batch[\"depth\"].to(device)\n",
    "\n",
    "#                 # Ensure depth_labels and segmentation labels have correct dimensions\n",
    "#                 if depth_labels.dim() == 5:\n",
    "#                     depth_labels = depth_labels.squeeze(2)\n",
    "#                 if seg_labels.dim() == 4 and seg_labels.shape[1] == 1:\n",
    "#                     seg_labels = seg_labels.squeeze(1)\n",
    "\n",
    "#                 # Transform depth labels\n",
    "#                 # depth_labels = torch.log(depth_labels.flatten(start_dim=1)) / 5\n",
    "#                 # depth_labels = depth_labels.view_as(depth_labels)  # Restore shape\n",
    "#                 # depth_labels = torch.clamp(depth_labels, min=1e-5) \n",
    "#                 # depth_labels = torch.log(depth_labels + 1e-5) / 5  # Avoid log(0)\n",
    "\n",
    "#                 # Latent noise for validation\n",
    "#                 latent_noise = torch.randn_like(inputs).to(device)\n",
    "#                 seg_output, depth_output, backbone_features = model(inputs, latent_noise)\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "#                 seg_output_old =seg_output\n",
    "#                 # Resize seg_output to match the spatial dimensions of seg_labels\n",
    "#                 seg_output_resized = F.interpolate(seg_output, size=seg_labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "#                 seg_output = seg_output_resized\n",
    "\n",
    "#                 depth_output_old = depth_output\n",
    "#                 depth_output_resized = F.interpolate(depth_output, size=depth_labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "#                 depth_output =depth_output_resized\n",
    "\n",
    "\n",
    "#                 # Segmentation Loss\n",
    "#                 seg_loss = nn.CrossEntropyLoss()(seg_output, seg_labels)\n",
    "#                 seg_dice = dice_loss(seg_output, seg_labels)\n",
    "#                 seg_iou = mean_iou(seg_output, seg_labels, num_classes)\n",
    "#                 seg_loss_total = 0.6 * seg_loss  + 0.4 * seg_dice\n",
    "                \n",
    "#                 depth_sidl = scale_invariant_depth_loss(depth_output, depth_labels)\n",
    "#                 depth_inv_huber = inv_huber_loss(depth_output, depth_labels)\n",
    "#                 depth_smooth = depth_smoothness_loss(depth_output, inputs)\n",
    "#                 depth_loss_total = depth_sidl + depth_inv_huber + depth_smooth\n",
    "\n",
    "#                 pix2pix_loss = seg_loss_total + depth_loss_total\n",
    "\n",
    "#                 # Combined Validation Loss\n",
    "#                 combined_loss = pix2pix_loss\n",
    "\n",
    "#                 # Accumulate Validation Metrics\n",
    "#                 epoch_valid[\"seg\"] += seg_loss.item()\n",
    "#                 epoch_valid[\"depth\"] += (depth_sidl + depth_smooth).item()\n",
    "#                 epoch_valid[\"combined\"] += combined_loss.item()\n",
    "#                 epoch_valid[\"iou\"] += seg_iou.item()\n",
    "#                 epoch_valid[\"depth_sidl\"] += depth_sidl.item()\n",
    "#                 epoch_valid[\"depth_smooth\"] += depth_smooth.item()\n",
    "                \n",
    "#                 num_valid_batches += 1\n",
    "                \n",
    "#                 # epoch, inputs, seg_output, depth_output, seg_labels, depth_labels, gif_frames\n",
    "#             frame = save_training_visualization_as_gif2(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels)\n",
    "#             gif_frames.append(frame)\n",
    "                \n",
    "                \n",
    "#         # Calculate epoch averages\n",
    "#         # Average Validation Losses\n",
    "#         for key in epoch_valid.keys():\n",
    "#             valid_losses[key].append(epoch_valid[key] / num_valid_batches)\n",
    "\n",
    "        \n",
    "        \n",
    "# # train_losses = { \"depth_sidl\": [], \"depth_inv_huber\": [], \"depth_contrastive\": [], \"depth_smooth\": []}\n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Valid Seg Loss: {epoch_valid['seg']:.4f}, \"\n",
    "#             f\"Valid Depth Loss: {epoch_valid['depth']:.4f}, Valid Combined Loss: {epoch_valid['combined']:.4f}, \"\n",
    "#             f\"Valid mIOU: {epoch_valid['iou']:.4f}, Valid sidl Loss: {epoch_valid['depth_sidl']:.4f}, \"\n",
    "#             f\"Valid depth smooth: {epoch_valid['depth_smooth']:.4f}\"\n",
    "#         )\n",
    "\n",
    "#         # Write the losses to CSV\n",
    "#         with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerow([\n",
    "#                 epoch + 1,\n",
    "#                 train_losses[\"seg\"], train_losses[\"depth\"], train_losses[\"combined\"],\n",
    "#                 train_losses[\"depth_sidl\"], 0,0,\n",
    "#                 # avg_train_depth_inv_huber, avg_train_depth_contrastive,\n",
    "#                 train_losses[\"depth_smooth\"],\n",
    "#                 valid_losses[\"seg\"], valid_losses[\"depth\"], valid_losses['combined'],\n",
    "#                 valid_losses[\"depth_sidl\"],0,0,\n",
    "#                 # avg_valid_depth_inv_huber, avg_valid_depth_contrastive, \n",
    "#                 valid_losses[\"depth_smooth\"]\n",
    "#             ])\n",
    "\n",
    "#         # Save GIF visualization frames\n",
    "#         # save_training_visualization_as_gif(epoch, inputs, seg_output, depth_output, seg_labels, depth_labels, gif_frames)\n",
    "\n",
    "#         # Save best model\n",
    "#         if valid_losses[\"combined\"][-1] < best_combined_loss:\n",
    "#             best_combined_loss = valid_losses[\"combined\"][-1]\n",
    "#             torch.save(model, os.path.join(save_dir, \"best_model_resnetBackbone.pth\"))\n",
    "#             print(f\"Best model saved at epoch {epoch+1} with combined loss {best_combined_loss:.4f}\")\n",
    "            \n",
    "#         if epoch%10==0:\n",
    "#             gif_path2 =os.path.join(save_dir,f\"viz_epoch_{epoch}.gif\")\n",
    "#             gif_frames[0].save(gif_path2, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "#     # Save GIF\n",
    "#     # gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "    \n",
    "#     plot_loss(train_losses, valid_losses, save_dir)\n",
    "#     gif_frames[0].save(gif_path, save_all=True, append_images=gif_frames[1:], duration=500, loop=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return train_losses,valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f553ec-993f-4642-a88f-f519abbeac8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa457c-8d5c-48cf-a9eb-d3ad1b361e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
